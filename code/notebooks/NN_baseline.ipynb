{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from typing import Optional, List, Set, FrozenSet, Tuple, Dict, Union, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from edn_format import ImmutableDict\n",
    "from memoization import cached\n",
    "from progress.bar import Bar\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl = 60 * 60  # 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statement:\n",
    "    \"\"\"Represents a weighted argumentation graph, based on the definition by Brenneis et al. (2020)\"\"\"\n",
    "\n",
    "    def __init__(self, premises=None, conclusion: Optional[\"Statement\"] = None,\n",
    "                 rating: float = 0.5, weight: float = 0, sid: str = \"\", username=None):\n",
    "        if premises is None:\n",
    "            premises = []\n",
    "        self.premises: Optional[List[\"Statement\"]] = premises\n",
    "        self.conclusion: Optional[\"Statement\"] = conclusion\n",
    "        self.sid: str = str(sid)\n",
    "        self.rating: float = 0.5 if np.isnan(rating) else rating\n",
    "        assert 0 <= self.rating <= 1, f\"rating should be in [0,1], but was {self.rating}\"\n",
    "        self.weight: float = 0 if np.isnan(weight) else weight\n",
    "        self.unnormalized_weight = self.weight  # this is the raw weight in [1,7] (taken from data set +1), needed for predictions\n",
    "        assert 0 <= self.weight, f\"weight should be positive, but was {self.weight}\"\n",
    "        self.username = username\n",
    "\n",
    "    def __hash__(self):\n",
    "        if self.username is not None:\n",
    "            return hash(self.username)\n",
    "        return id(self)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.username is not None:\n",
    "            return self.username == other.username\n",
    "        return id(self) == id(other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<{self.sid}, r={self.rating}, w={self.weight} ({self.unnormalized_weight})>\"\n",
    "\n",
    "    @cached(ttl=ttl)\n",
    "    def depth(self) -> int:\n",
    "        if self.conclusion is None:\n",
    "            return 0\n",
    "        return 1 + self.conclusion.depth()\n",
    "\n",
    "    @cached(ttl=ttl)\n",
    "    def by_name(self, name) -> Optional[\"Statement\"]:\n",
    "        \"\"\"\n",
    "        Find a node by statement id\n",
    "\n",
    "        using breadth first search as we most often need nodes close to the root\n",
    "        \"\"\"\n",
    "        if self.sid == str(name):\n",
    "            return self\n",
    "        queue = self.premises.copy()\n",
    "        while len(queue) > 0:\n",
    "            element = queue.pop()\n",
    "            if element.sid == name:\n",
    "                return element\n",
    "            queue.extend(element.premises)\n",
    "        return None\n",
    "\n",
    "    def path_to_root(self) -> [\"Statement\"]:\n",
    "        if self.conclusion is None:\n",
    "            return [self]\n",
    "        return [self] + self.conclusion.path_to_root()\n",
    "\n",
    "    @cached(ttl=ttl)\n",
    "    def product_weight_to_root(self) -> float:\n",
    "        result = 1\n",
    "        for statement in self.path_from_root():\n",
    "            result *= statement.weight\n",
    "        return result\n",
    "\n",
    "    def path_from_root(self) -> [\"Statement\"]:\n",
    "        return self.path_to_root()[::-1]\n",
    "\n",
    "    def normalize_weights_to_conclusions(self) -> \"Statement\":\n",
    "        weight_to_premises_sum = sum(p.weight for p in self.premises)\n",
    "        for p in self.premises:\n",
    "            if weight_to_premises_sum > 0:\n",
    "                p.weight /= weight_to_premises_sum\n",
    "            # else: set it to 0, but it is already 0\n",
    "            p.normalize_weights_to_conclusions()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    \"\"\"efficient representation of premise-conclusion relations\"\"\"\n",
    "    def __init__(self, arguments: pd.DataFrame):\n",
    "        self.arguments = arguments.set_index(\"statement_id\").fillna(\"\").to_dict(\"index\")\n",
    "        for statement_id in self.arguments:\n",
    "            if self.arguments[statement_id][\"conclusions\"] == \"\":\n",
    "                self.arguments[statement_id][\"conclusions\"] = set()\n",
    "            else:\n",
    "                self.arguments[statement_id][\"conclusions\"] = {conclusion.split(\"(\")[0] for conclusion in str(self.arguments[statement_id][\"conclusions\"]).split(\";\")}\n",
    "\n",
    "    @cached(ttl=ttl)\n",
    "    def get_positions(self) -> Set[str]:\n",
    "        return {str(x) for x, v in self.arguments.items() if len(v[\"conclusions\"]) == 0}\n",
    "\n",
    "    @cached(ttl=ttl)\n",
    "    def get_statement_ids_with_conclusion(self, conclusion_ids: Set[str]) -> Set[str]:\n",
    "        \"\"\"get the ids of all statements which have a conclusion which is in conclusion_ids\"\"\"\n",
    "        return {str(x) for x, v in self.arguments.items() if len(v[\"conclusions\"].intersection(conclusion_ids)) != 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar(Bar):\n",
    "    check_tty = False  # for PyCharm, see https://github.com/verigak/progress/issues/50\n",
    "    suffix = '%(index)d/%(max)d - %(elapsed)ds - ETA: %(eta)ds'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def hyperparameter_combinations(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        returns a list of all hyperparameter combinations to be evaluated for this predictor\n",
    "        \"\"\"\n",
    "        return [{\"no_hyperparameters\": None}]\n",
    "\n",
    "    def set_hyperparameters(self, hyperparameters: Dict[str, Any]):\n",
    "        for name, value in hyperparameters.items():\n",
    "            self.__setattr__(name, value)\n",
    "\n",
    "    def fit(self, arguments: pd.DataFrame, training_profiles: pd.DataFrame):\n",
    "        ...\n",
    "\n",
    "    def predict_conviction(self, statement_id: str, test_username: str) -> int:\n",
    "        \"\"\"\n",
    "        Predict whether an argument is agreed to (1) or not (0)\n",
    "\n",
    "        1 is returned iff the conviction degree is greater than 0.5.\n",
    "        Only the username is provided as the second argument to prevent leakage of the test set into the prediction\n",
    "        \"\"\"\n",
    "        return round(self.predict_conviction_degree(statement_id, test_username))\n",
    "\n",
    "    def predict_conviction_degree(self, statement_id: str, test_username: str) -> float:\n",
    "        \"\"\"\n",
    "        Predict the degree to which an argument is agreed to (1) or not (0), via a value in [0, 1]\n",
    "\n",
    "        Only the username is provided as the second argument to prevent leakage of the test set into the prediction\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def predict_strength(self, statement_id: str, test_username: str) -> float:\n",
    "        \"\"\"\n",
    "        Predict how strong an argument is considered (values in [0,6])\n",
    "\n",
    "        Only the username is provided as the second argument to prevent leakage of the test set into the prediction\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeliberatePredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Predictor which uses the collaborative filtering algorithms found in deliberate using the argumentation distance\n",
    "    metric by Brenneis et al.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_n: int = 100, alpha: float = 0.5, weighting_strategy: Optional[str] = \"raw\", max_depth: int = 2):\n",
    "        \"\"\"\n",
    "        @param top_n: the top_n closest, relevant user profiles are considered (a profile is relevant if it has the value to be predicted)\n",
    "        @param alpha: Î± parameter of the metric, a lower value emphasized positions\n",
    "        @param weighting_strategy: raw, normalized or None: Whether the weighting uses distances normalized by the max. possible distance (raw/normalized), or do not use weighting at all (None)\n",
    "        @param max_depth: maximum node depths to consider when calculating the distance between user profiles (1 only considers positions, 2 considers positions and all top-level arguments)\n",
    "        \"\"\"\n",
    "        self.users: Dict[str, Statement] = {}\n",
    "        self.arguments: Arguments = None\n",
    "        self.top_n = top_n\n",
    "        self.alpha = alpha\n",
    "        assert weighting_strategy in {\"raw\", \"normalized\", None}\n",
    "        self.weighting_strategy = weighting_strategy\n",
    "        assert max_depth > 0\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def hyperparameter_combinations(self, task:str) -> List[Dict[str, Any]]: \n",
    "        with open(\"config.json\", \"r\") as c:\n",
    "            set_task = \"conviction\" if task == \"conviction\" else \"weight\"\n",
    "            hyperparams = [json.loads(c.read())[\"hyperparameters\"][\"NN_Baseline\"][\"T1_T2\"][set_task.capitalize()][\"model_parameters\"]]\n",
    "                \n",
    "        return hyperparams\n",
    "                \n",
    "    def predict_conviction_degree(self, statement_id: str, test_username: str) -> float:\n",
    "        def distance_to_weight(distance: float) -> float:\n",
    "            if self.weighting_strategy == \"normalized\":\n",
    "                w = 1 - distance / (self.alpha * (1 - self.alpha ** self.max_depth))\n",
    "                assert -0.000001 < w < 1.0000001\n",
    "                return w\n",
    "            elif self.weighting_strategy == \"raw\":\n",
    "                return 1 - distance\n",
    "            elif self.weighting_strategy is None:\n",
    "                return 1\n",
    "            raise ValueError(f\"unknown weighting strategy {self.weighting_strategy}\")\n",
    "\n",
    "        this_user_graph = self.users[test_username]\n",
    "        users_by_d = users_by_distance(this_user_graph, self.users, self.alpha, self.max_depth)\n",
    "        relevant_users_by_d = [(graph, distance)\n",
    "                               for graph, distance in users_by_d\n",
    "                               if graph.by_name(statement_id) is not None and graph.by_name(statement_id).rating != 0.5][:self.top_n]\n",
    "        weighted_sum = sum(distance_to_weight(distance) * graph.by_name(statement_id).rating for\n",
    "                           graph, distance in relevant_users_by_d)\n",
    "        normalization = sum(distance_to_weight(distance) for _, distance in relevant_users_by_d)\n",
    "        if normalization == 0:\n",
    "            return 0.5\n",
    "        assert 0 <= weighted_sum / normalization <= 1\n",
    "        return weighted_sum / normalization\n",
    "\n",
    "    def fit(self, arguments: pd.DataFrame, training_profiles: pd.DataFrame):\n",
    "        bar = ProgressBar(\"fitting\", max=len(training_profiles))\n",
    "        self.arguments = Arguments(arguments)\n",
    "        self.users = ImmutableDict({profile[\"username\"]: bar.next() or user_to_graph(self.arguments, profile) for _i, profile in training_profiles.iterrows()})\n",
    "        bar.finish()\n",
    "\n",
    "    def predict_strength(self, statement_id: str, test_username: str) -> float:\n",
    "        this_user_graph = self.users[test_username]\n",
    "        users_by_d = users_by_distance(this_user_graph, self.users, self.alpha, self.max_depth)[:self.top_n]\n",
    "        relevant_users_by_d = [(graph, distance)\n",
    "                               for graph, distance in users_by_d\n",
    "                               if graph.by_name(statement_id) is not None and graph.by_name(statement_id).weight > 0]\n",
    "        weighted_sum = sum((1 - distance) * (graph.by_name(statement_id).unnormalized_weight - 1)\n",
    "                           for graph, distance in relevant_users_by_d)\n",
    "        normalization = sum((1 - distance) for _, distance in relevant_users_by_d)\n",
    "        if normalization == 0:\n",
    "            return 0\n",
    "        predicted_weight = weighted_sum / normalization\n",
    "        assert 0 <= predicted_weight <= 6.000000001, f\"predicted weight {predicted_weight} out of expected range\"\n",
    "        return predicted_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_to_graph(arguments: Arguments, user_data: pd.Series) -> Statement:\n",
    "    \"\"\"\n",
    "    Transform user db entry to a weighted argumentation graph, only considering two levels of arguments (more is effectively not collected by deliberate)\n",
    "    \"\"\"\n",
    "    def get_position_rating(pid: str):\n",
    "        raw_rating = user_data.get(f\"position_rating_after_{pid}\", 0)\n",
    "        raw_opinion = user_data.get(f\"statement_attitude_{pid}\", None)\n",
    "        if raw_opinion is None and raw_rating != 0:\n",
    "            raise ValueError(f\"Got no opinion, but an rating {raw_rating} for {pid}, {user_data['username']}\")\n",
    "        # return np.random.randint(0, 2)\n",
    "        if raw_opinion == 0:\n",
    "            return 0.5 - (raw_rating+1)/7/2\n",
    "        elif raw_opinion == 1:\n",
    "            return (raw_rating+1)/7/2 + 0.5\n",
    "        return 0.5\n",
    "\n",
    "    I = Statement(premises=[], sid=\"I\", weight=1, username=user_data[\"username\"])\n",
    "    position_ids = arguments.get_positions()\n",
    "    assert len(position_ids) == 3, f\"expected 3 positions, got {position_ids}\"\n",
    "    positions = [Statement(premises=[],\n",
    "                           conclusion=I,\n",
    "                           rating=get_position_rating(pid),\n",
    "                           weight=0 if user_data.get(f\"position_rating_after_{pid}\") is None else 1,\n",
    "                           sid=pid) for pid in position_ids]\n",
    "    I.premises = positions\n",
    "    for position in positions:\n",
    "        premises = [Statement(premises=[],\n",
    "                              conclusion=position,\n",
    "                              rating=user_data.get(f\"statement_attitude_{sid}\", 0.5),\n",
    "                              weight=user_data.get(f\"argument_rating_{sid}\", -1) + 1,\n",
    "                              sid=sid)\n",
    "                    for sid in arguments.get_statement_ids_with_conclusion({position.sid})]\n",
    "        position.premises = premises\n",
    "    for first_level_statement_id in arguments.get_statement_ids_with_conclusion(position_ids):\n",
    "        first_level_statement = I.by_name(first_level_statement_id)\n",
    "        premises = [Statement(premises=[],\n",
    "                              conclusion=first_level_statement,\n",
    "                              rating=user_data.get(f\"statement_attitude_{sid}\", 0.5),\n",
    "                              weight=user_data.get(f\"argument_rating_{sid}\", -1) + 1,\n",
    "                              sid=f\"{sid}_{first_level_statement_id}\")  # our weight normalization would run crazy if sids/nodes are repeated\n",
    "                    for sid in arguments.get_statement_ids_with_conclusion({first_level_statement.sid})]\n",
    "        first_level_statement.premises = premises\n",
    "    I.normalize_weights_to_conclusions()\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cached(ttl=ttl)\n",
    "def users_by_distance(this_user_graph: Statement, user_graphs: Dict[str, Statement],\n",
    "                      alpha: float, max_depth: int) -> List[Tuple[Statement, float]]:\n",
    "    assert this_user_graph.username != \"\"\n",
    "    return sorted(((other_graph, d_brenneis(frozenset({this_user_graph, other_graph}), alpha=alpha, max_depth=max_depth))\n",
    "                   for other_graph in user_graphs.values()\n",
    "                   if other_graph.username != this_user_graph.username),\n",
    "                  # for deterministic order (for reproducible results):\n",
    "                  # * include nickname as tiebreaker\n",
    "                  # * round float to prevent random order for \"equal\" values\n",
    "                  key=lambda x: (round(x[1], 10), x[0].username))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cached(ttl=ttl)\n",
    "def d_brenneis(graphs: Union[Set[Statement], FrozenSet[Statement]], alpha: float, max_depth: int=2) -> float:\n",
    "    \"\"\"\n",
    "    metric by Brenneis et al.\n",
    "\n",
    "    @param graphs: exactly two graphs to be compared (put in set to make caching order-independent)\n",
    "    @param alpha: a lower alpha emphasized opinions on arguments closer to the root (cf. PageRank)\n",
    "    \"\"\"\n",
    "    def brenneis(g1: Statement, g2: Statement, depths_left: int) -> float:  # no need to cache recursive calls\n",
    "        if (g1.weight == 0 and g2.weight == 0) or (depths_left == 0):\n",
    "            return 0\n",
    "\n",
    "        g1_weight_to_root = g1.product_weight_to_root()\n",
    "        g2_weight_to_root = g2.product_weight_to_root()\n",
    "        if g1_weight_to_root == 0 and g2_weight_to_root == 0:\n",
    "            return 0\n",
    "\n",
    "        first_summand = (1 - alpha) * abs((g1.rating - .5) * g1_weight_to_root - (g2.rating - .5) * g2_weight_to_root)\n",
    "        second_summand = 0\n",
    "        premises_ids = {p.sid for p in g1.premises + g2.premises}\n",
    "        premises_1 = {p.sid: p for p in g1.premises}\n",
    "        premises_2 = {p.sid: p for p in g2.premises}\n",
    "        for a in premises_ids:\n",
    "            a1 = premises_1.get(a)\n",
    "            a2 = premises_2.get(a)\n",
    "            # we used to test this, but we are quite confident that this is assured (otherwise, the construction algorithm would be broken), and it saved us 50% runtime\n",
    "            # if a1 is not None and a2 is not None:\n",
    "            #     assert a1.depth() == a2.depth(), \"depth of nodes different, is the universe graph the same?\"\n",
    "            if a1 is None:  # do not use default value for get: argument construction work is bad\n",
    "                a1 = Statement(sid=a)\n",
    "            if a2 is None:\n",
    "                a2 = Statement(sid=a)\n",
    "            second_summand += brenneis(a1, a2, depths_left - 1)\n",
    "        second_summand *= alpha\n",
    "        return first_summand + second_summand\n",
    "\n",
    "    assert 0 < alpha < 1\n",
    "    graph1, graph2 = list(graphs)\n",
    "    return brenneis(graph1, graph2, max_depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_conviction(predictor: Predictor, test_profiles: pd.DataFrame, target_id_start: int, target_id_end: int) -> float:\n",
    "    \"\"\"\n",
    "    calculates the macro accuracy for predicting argument conviction (0 or 1)\n",
    "\n",
    "    only arguments with ground truths and ids falling in the given ranges are evaluated on\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    trues, preds = [], []\n",
    "    for i, test_profile in ProgressBar(\"conviction\", max=len(test_profiles)).iter(test_profiles.iterrows()):\n",
    "        evaluation_statements = [(k.split(\"_\")[-1], v) for k, v in test_profile.dropna().iteritems()\n",
    "                                 if \"statement_attitude_\" in k and target_id_start <= int(k.split(\"_\")[-1]) <= target_id_end]\n",
    "        assert len(evaluation_statements) >= 1, f\"not enough evaluation statements for user {test_profile['username']}, is your argument id range and your dataset okay?\"\n",
    "        correct_prediction = 0\n",
    "        for statement_id, ground_truth in evaluation_statements:\n",
    "            prediction = predictor.predict_conviction(statement_id, test_profile[\"username\"])\n",
    "            trues.append(ground_truth)\n",
    "            preds.append(prediction)\n",
    "            if prediction == ground_truth:\n",
    "                correct_prediction += 1\n",
    "        accuracy = correct_prediction / len(evaluation_statements)\n",
    "        accuracies.append(accuracy)\n",
    "    print(f\"Accuracy: {float(np.mean(accuracies))}\")\n",
    "    return np.array(trues), np.array(preds)\n",
    "\n",
    "\n",
    "def evaluate_strength(predictor: Predictor, test_profiles: pd.DataFrame, target_id_start: int, target_id_end: int) -> float:\n",
    "    \"\"\"\n",
    "    calculates the average mean squared error for predicting an argument's strength (value in [0, 6])\n",
    "\n",
    "    only arguments with ground truths and ids falling in the given ranges are evaluated on\n",
    "    \"\"\"\n",
    "    rmses = []\n",
    "    trues, preds = [], []\n",
    "    for i, test_profile in ProgressBar(\"strength\", max=len(test_profiles)).iter(test_profiles.iterrows()):\n",
    "        evaluation_statements = [(k.split(\"_\")[-1], v) for k, v in test_profile.dropna().iteritems()\n",
    "                                 if \"argument_rating_\" in k and target_id_start <= int(k.split(\"_\")[-1]) <= target_id_end]\n",
    "        assert len(evaluation_statements) >= 1, f\"not enough evaluation statements for user {test_profile['username']}, is your argument id range and your dataset okay?\"\n",
    "        squared_errors = 0\n",
    "        for statement_id, ground_truth in evaluation_statements:\n",
    "            prediction = predictor.predict_strength(statement_id, test_profile[\"username\"])\n",
    "            trues.append(ground_truth)\n",
    "            preds.append(round(prediction))\n",
    "            squared_errors += (prediction - ground_truth) ** 2\n",
    "        rmse = np.sqrt(squared_errors / len(evaluation_statements))\n",
    "        rmses.append(rmse)\n",
    "    print(f\"RMSE: {float(np.mean(rmses))}\")\n",
    "    return np.array(trues), np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**data_parameters):\n",
    "    arguments = pd.read_csv(data_parameters[\"arguments_filename\"])\n",
    "    training_profiles = pd.read_csv(data_parameters[\"training_profiles_filename\"])\n",
    "    test_profiles = pd.read_csv(data_parameters[\"test_profiles_filename\"])\n",
    "\n",
    "    if (algorithm:=data_parameters[\"algorithm\"]) == \"deliberate\":\n",
    "        predictor = DeliberatePredictor()\n",
    "    else:\n",
    "        raise ValueError(f\"algorithm {algorithm} is unknown\")\n",
    "\n",
    "    predictor.fit(arguments, training_profiles)\n",
    "\n",
    "    for hyperparameters in predictor.hyperparameter_combinations(data_parameters[\"task\"]):\n",
    "        predictor.set_hyperparameters(hyperparameters)\n",
    "        print(predictor.__class__.__name__, hyperparameters)\n",
    "\n",
    "        target_id_start = data_parameters[\"target_id_start\"]\n",
    "        target_id_end = data_parameters[\"target_id_end\"]\n",
    "        t = data_parameters[\"task\"]\n",
    "        \n",
    "        if t == \"conviction\":\n",
    "            trues, preds = evaluate_conviction(predictor, test_profiles, target_id_start, target_id_end)\n",
    "            return trues, preds\n",
    "        elif t == \"strength\":\n",
    "            trues, preds = evaluate_strength(predictor, test_profiles, target_id_start, target_id_end)\n",
    "            return trues, preds\n",
    "        else:\n",
    "            raise ValueError(f\"task {t} is unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues, preds = main(**data_parameters)\n",
    "%run MetricHelper.ipynb\n",
    "print(mh.compute_average_metrics())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
