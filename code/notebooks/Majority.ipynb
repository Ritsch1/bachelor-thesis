{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export notebook as python script to the ../python-code folder\n",
    "subprocess.run(\"jupyter nbconvert --output-dir='../python-code' --to python Majority.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MajorityVoter():\n",
    "    \n",
    "    def __init__(self, rmh, task:str=\"Conviction\") -> None:\n",
    "        self.rmh_ = rmh\n",
    "        self.task_ = task\n",
    "        \n",
    "    def calculate_predictions(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the mode for each item if it is the Conviction task, else calculate the mean for each item.\n",
    "        \"\"\"\n",
    "        if self.task_ == \"Conviction\":\n",
    "            # Drop username column and get columns corresponding to the task\n",
    "            rating_matrix = self.rmh_.train_rating_matrix.drop(\"username\", axis=1).values[:,1::2]\n",
    "            num_cols = rating_matrix.shape[1]\n",
    "            self.item_means = {}\n",
    "            for i in range(num_cols):\n",
    "                values, counts = np.unique(rating_matrix[:,i][~np.isnan(rating_matrix[:,i])], return_counts=True)\n",
    "                if len(counts) == 0:\n",
    "                    self.item_means[i] = -1\n",
    "                else:\n",
    "                    self.item_means[i] = values[np.argmax(counts)]\n",
    "        else:\n",
    "            # Drop username column and get columns corresponding to the task\n",
    "            rating_matrix = self.rmh_.train_rating_matrix.drop(\"username\", axis=1).values[:,::2]\n",
    "            num_cols = rating_matrix.shape[1]\n",
    "            self.item_means = {i: np.nanmean(rating_matrix[:,i]) for i in range(num_cols)}\n",
    "            \n",
    "    def evaluate(self, *metrics:str) -> float:\n",
    "        \n",
    "        # Values for Metric Helper class\n",
    "        trues, preds = [],[]\n",
    "        task = self.task_\n",
    "        \n",
    "        if self.task_ == \"Conviction\":\n",
    "            # Get odd-indexed arguments that correspond to conviction arguments in the range [0,1]        \n",
    "            test_eval_indices_copy = {user:items[items % 2 == 1] for user,items in self.rmh_.test_eval_indices.items()}\n",
    "            # To match the indices of the training, integer divide all odd indices by 2 to map them to the correct index\n",
    "            for key, value in test_eval_indices_copy.items():\n",
    "                test_eval_indices_copy[key] = value // 2 \n",
    "            # Get rid of the username column in the test-rating -matrix for converting only numerical values into a pytorch tensor\n",
    "            test_rating_matrix_copy = self.rmh_.test_rating_matrix.drop([\"username\"], axis=1)\n",
    "            # Trim the original test_rating_matrix to the conviction columns only\n",
    "            trimmed_test_rating_matrix = torch.index_select(torch.from_numpy(test_rating_matrix_copy.values).to(torch.float16), 1, torch.arange(1, test_rating_matrix_copy.shape[1], 2))\n",
    "            # Calculate the mean-accuracy for the Prediction of Conviction (PoC) - task \n",
    "            mean_acc = 0.0\n",
    "            # Variable for counting the correct 0/1 prediction\n",
    "            count_equality = 0\n",
    "            for username, test_samples in test_eval_indices_copy.items():\n",
    "                # The actual username of the user\n",
    "                username_str = username[0]\n",
    "                # The row-index in the test set of that user\n",
    "                user_idx_test = username[1]\n",
    "                for arg_idx in test_samples:\n",
    "                    # Look up the true value\n",
    "                    true_value = trimmed_test_rating_matrix[user_idx_test][arg_idx]\n",
    "                    prediction = round(self.item_means[arg_idx])\n",
    "                    trues.append(true_value)\n",
    "                    preds.append(prediction)\n",
    "                    # If the prediction is correct, increment the counter\n",
    "                    if  true_value == prediction:\n",
    "                        count_equality += 1\n",
    "                # Normalize by the number of test samples for this user\n",
    "                mean_acc += count_equality / len(test_samples)\n",
    "                # Set the count equality to 0 for the next user\n",
    "                count_equality = 0\n",
    "            # Normalize the error by the number of users in the test-set\n",
    "            mean_acc /= len(test_eval_indices_copy)\n",
    "            print(f\"Accuracy: {mean_acc:.3f}\")\n",
    "\n",
    "            trues = np.array(trues)\n",
    "            preds = np.array(preds)\n",
    "            return trues, preds\n",
    "            \n",
    "        else:\n",
    "            #Get even-indexed arguments that correspond to weight arguments in the range [0,6]  \n",
    "            test_eval_indices_copy = {user:items[items % 2 == 0] for user,items in self.rmh_.test_eval_indices.items()}\n",
    "            # To match the indices of the training, integer divide all odd indices by 2 to map them to the correct index\n",
    "            for key, value in test_eval_indices_copy.items():\n",
    "                test_eval_indices_copy[key] = value // 2\n",
    "            # Get rid of the username column in the test-rating -matrix for proper indexing\n",
    "            test_rating_matrix_copy = self.rmh_.test_rating_matrix.drop([\"username\"], axis=1) \n",
    "            # Trim the original test_rating_matrix to the weight columns only\n",
    "            trimmed_test_rating_matrix = torch.index_select(torch.from_numpy(test_rating_matrix_copy.values).to(torch.float16), 1, torch.arange(0, test_rating_matrix_copy.shape[1], 2))\n",
    "            # Calculate the averaged root mean squared error for the Prediction of Weight (PoW) - task\n",
    "            rmse_error = 0.0\n",
    "            # Variable for measuring the distance of the true value and the prediction\n",
    "            prediction_distance = 0.0\n",
    "            for username, test_samples in test_eval_indices_copy.items():\n",
    "                # The actual username of the user\n",
    "                username_str = username[0]\n",
    "                # The row-index in the test set of that user\n",
    "                user_idx_test = username[1]\n",
    "                for arg_idx in test_samples:\n",
    "                    # Look up the true value\n",
    "                    true_value = trimmed_test_rating_matrix[user_idx_test][arg_idx]\n",
    "                    prediction = round(self.item_means[arg_idx])\n",
    "                    trues.append(int(true_value))\n",
    "                    preds.append(prediction)\n",
    "                    prediction_distance += (true_value - prediction)**2\n",
    "                # Normalize by the number of test samples for this user     \n",
    "                rmse_error += (prediction_distance / len(test_samples))\n",
    "                # Set the prediction distance to 0 for the next user\n",
    "                prediction_distance = 0\n",
    "            # Normalize the prediction_distance by the number of users in the test-set\n",
    "            rmse_error /= len(test_eval_indices_copy)\n",
    "            print(f\"RMSE: {rmse_error:.3f}\")\n",
    "            trues = np.array(trues)\n",
    "            preds = np.array(preds)\n",
    "            return trues,preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for executing the Rating-Matrix-Handler notebook\n",
    "train_path = f\"../../data/T1_T2/train.csv\"\n",
    "test_path  = f\"../../data/T1_T2/test.csv\"\n",
    "validation_path = f\"../../data/T1_T2/validation.csv\"\n",
    "%run Rating_Matrix_Handler.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = MajorityVoter(rmh, task=\"Conviction\")\n",
    "mv.calculate_predictions()\n",
    "trues, preds = mv.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run MetricHelper.ipynb\n",
    "print(mh.compute_average_metrics())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
