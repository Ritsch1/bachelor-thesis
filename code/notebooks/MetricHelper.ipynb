{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricHelper():\n",
    "    \"\"\"\n",
    "    Class that takes care of the calculation of different metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, trues:np.array, preds:np.array, task:str=\"Conviction\", *metrics:str):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            trues (np.array): True values\n",
    "            preds (np.array): predictions.\n",
    "            task (str, optional): The task for which the metrics per class need to be computed. Defaults to \"Conviction\".\n",
    "            *metrics: Metrics to compute. Can be one of the following: 'precision', 'recall', 'f1', 'gmean'.\n",
    "        \"\"\"\n",
    "        self.trues_ = trues\n",
    "        self.preds_ = preds\n",
    "        self.task_ = task\n",
    "        self.metrics_ = metrics\n",
    "        self.classes_ = (0, 1) if task == \"Conviction\" else (0,1,2,3,4,5,6)\n",
    "        \n",
    "    def compute_average_metrics(self) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the metrics' average over all classes\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the metric name as key and its averaged value as value.\n",
    "        \"\"\"\n",
    "        metrics_per_class = self.compute_metrics_per_class()\n",
    "        # metrics averages over all classes with equal weight per class\n",
    "        metrics_averaged = {}\n",
    "        sum = 0.0\n",
    "        for m in self.metrics_:\n",
    "            for c in self.classes_:\n",
    "                sum += metrics_per_class[c][m]\n",
    "            sum /= len(self.classes_)\n",
    "            metrics_averaged[m] = sum\n",
    "            sum = 0.0\n",
    "        \n",
    "        return metrics_averaged\n",
    "    \n",
    "    def compute_metrics_per_class(self) -> dict:\n",
    "        \"\"\"\n",
    "        Compute the metrics Recall, Precision, F1-Score, G-Mean per class. \n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing a class label as key and a dictionary with the metric name as key and its value as value.\n",
    "        \"\"\"\n",
    "        # Compute tuples of (metric_name, metric_value)\n",
    "        metric_values = [(m, getattr(self, f\"compute_{m}\")(self.trues_, self.preds_)) for m in self.metrics_]\n",
    "        # Compute dictionary with metric values per class\n",
    "        metrics_per_class = {c : {m[0] : m[1][c] for m in metric_values} for c in self.classes_}\n",
    "        \n",
    "        return metrics_per_class\n",
    "     \n",
    "    def compute_accuracy(self, trues:np.array, preds:np.array) -> dict:\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            trues (np.array): true values.\n",
    "            preds (np.array): predicitons corresponding to true values. \n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the accuracy value for this class as value.\n",
    "        \"\"\"\n",
    "        \n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        accuracy = {c:None for c in self.classes_}\n",
    "        for c in accuracy:\n",
    "            filtered = np.logical_and(combined[:,0] == c, combined[:,1] == c)\n",
    "            acc = np.sum(filtered)\n",
    "            acc /= len(filtered)\n",
    "            accuracy[c] = acc\n",
    "            \n",
    "        return accuracy\n",
    "    \n",
    "    def compute_precision(self, trues, preds) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the precision metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the precision value for this class as value. \n",
    "        \"\"\"\n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        precision = {c:None for c in self.classes_}\n",
    "        for c in precision:\n",
    "            tp = np.sum(np.logical_and(combined[:,0] == c, combined[:,1] == c))\n",
    "            fp = np.sum(np.logical_and(combined[:,0] != c, combined[:,1] == c))\n",
    "            result_label =  tp / (tp + fp)\n",
    "            if np.isnan(result_label):\n",
    "                precision[c] = 0\n",
    "                continue\n",
    "            else:\n",
    "                precision[c] = result_label\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    \n",
    "    def compute_recall(self, trues, preds) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the recall metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the recall value for this class as value. \n",
    "        \"\"\"\n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        recall = {c:None for c in self.classes_}\n",
    "        for c in recall:\n",
    "            tp = np.sum(np.logical_and(combined[:,0] == c, combined[:,1] == c))\n",
    "            fn = np.sum(np.logical_and(combined[:,0] == c, combined[:,1] != c)) \n",
    "            result_label =  tp / (tp + fn)\n",
    "            if np.isnan(result_label):\n",
    "                recall[c] = 0\n",
    "                continue\n",
    "            else:\n",
    "                recall[c] = result_label \n",
    "        \n",
    "        return recall\n",
    "\n",
    "    def compute_f1(self, trues:np.array, preds:np.array, beta:float=1.0) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the precision metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the f1 value for this class as value. \n",
    "        \"\"\"\n",
    "        assert beta >= 0, \"beta needs to be non-negative.\"\n",
    "        recall = self.compute_recall(trues, preds)\n",
    "        precision = self.compute_precision(trues, preds)\n",
    "        f1 = {c:None for c in self.classes_}\n",
    "        for c in f1:\n",
    "            score = (2 * recall[c] * precision[c]) / (precision[c] + recall[c])\n",
    "            if np.isnan(score):\n",
    "                f1[c] = 0\n",
    "                continue\n",
    "            else:\n",
    "                f1[c] = score\n",
    "        return f1\n",
    "    \n",
    "    def compute_gmean(self,trues, preds) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the precision metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the gmean value for this class as value. \n",
    "        \"\"\"\n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        gmean = {c:None for c in self.classes_}\n",
    "        for c in gmean:\n",
    "            tp = np.sum(np.logical_and(combined[:,0] == c, combined[:,1] == c))\n",
    "            fn = np.sum(np.logical_and(combined[:,0] == c, combined[:,1] != c)) \n",
    "            fp = np.sum(np.logical_and(combined[:,0] != c, combined[:,1] == c)) \n",
    "            tn = np.sum(np.logical_and(combined[:,0] != c, combined[:,1] != c))\n",
    "            result_label = np.sqrt((tp / (tp + fn)) * (tn/(tn+fp)))\n",
    "            if np.isnan(result_label):\n",
    "                gmean[c] = 0\n",
    "                continue\n",
    "            gmean[c] = np.sqrt((tp / (tp + fn)) * (tn/(tn+fp)))\n",
    "            \n",
    "        return gmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MetricHelper(trues, preds, task, *metrics)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
