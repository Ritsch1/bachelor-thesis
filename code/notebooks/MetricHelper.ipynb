{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricHelper():\n",
    "    \n",
    "    def __init__(self, trues:np.array, preds:np.array, task:str=\"Conviction\", *metrics:str):\n",
    "        self.trues_ = trues\n",
    "        self.preds_ = preds\n",
    "        self.task_ = task\n",
    "        self.metrics = metrics\n",
    "        self.classes = (0, 1) if task == \"Conviction\" else (0,1,2,3,4,5,6)\n",
    "        \n",
    "    def compute_average_metrics(self) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the metrics' average over all classes\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the metric name as key and its averaged value as value.\n",
    "        \"\"\"\n",
    "        metrics_per_class = self.compute_metrics_per_class()\n",
    "        # metrics averages over all classes with equal weight per class\n",
    "        metrics_averaged = {}\n",
    "        sum = 0.0\n",
    "        for m in self.metrics:\n",
    "            for c in self.classes:\n",
    "                sum += metrics_per_class[c][m]\n",
    "                sum /= len(self.classes)\n",
    "                metrics_averaged[m] = sum\n",
    "                sum = 0.0\n",
    "        \n",
    "        return metrics_averaged\n",
    "    \n",
    "    def compute_metrics_per_class(self) -> dict:\n",
    "        \"\"\"\n",
    "        Compute the metrics Recall, Precision, F1-Score, G-Mean per class. \n",
    "\n",
    "        Params:\n",
    "            trues (np.array): True values\n",
    "            preds (np.array): predictions.\n",
    "            task (str, optional): The task for which the metrics per class need to be computed. Defaults to \"Conviction\".\n",
    "            *metrics: Metrics to compute. Can be one of the following: 'precision', 'recall', 'f1', 'gmean'.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing a class label as key and a dictionary with the metric name as key and its value as value.\n",
    "        \"\"\"\n",
    "        # Compute tuples of (metric_name, metric_value)\n",
    "        metric_values = [(m, getattr(self, f\"compute_{m}\")(self.trues, self.preds)) for m in self.metrics]\n",
    "        # Compute dictionary with metric values per class\n",
    "        metrics_per_class = {c : {m[0] : m[1][c] for m in metric_values} for c in self.classes}\n",
    "        \n",
    "        return metrics_per_class\n",
    "     \n",
    "    def compute_accuracy(self, trues, preds) -> dict:\n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        accuracy = {c:None for c in self.classes}\n",
    "        for c in accuracy:\n",
    "            acc = np.sum(filtered:= ((combined[i][0] == c) and (combined[i][1] == c) for i in range(len(combined))))\n",
    "            acc /= len(filtered)\n",
    "            accuracy[c] = acc\n",
    "    \n",
    "    def compute_precision(self, trues, preds) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the precision metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the precision value for this class as value. \n",
    "        \"\"\"\n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        precision = {c:None for c in self.classes}\n",
    "        for c in precision:\n",
    "            tp = np.sum((combined[i][0] == c) and (combined[i][1] == c) for i in range(len(combined)))\n",
    "            fp = np.sum((combined[i][0] != c) and (combined[i][1] == c) for i in range(len(combined)))\n",
    "            precision[c] = tp / (tp + fp)\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    \n",
    "    def compute_recall(self, trues, preds) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the recall metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the recall value for this class as value. \n",
    "        \"\"\"\n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        recall = {c:None for c in self.classes}\n",
    "        for c in recall:\n",
    "            tp = np.sum((combined[i][0] == c) and (combined[i][1] == c) for i in range(len(combined)))\n",
    "            fn = np.sum((combined[i][0] == c) and (combined[i][1] != c) for i in range(len(combined)))\n",
    "            recall[c] = tp / (tp + fn)\n",
    "        \n",
    "        return recall\n",
    "\n",
    "    def compute_f1(self, trues:np.array, preds:np.array, beta:float=1.0) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the precision metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the f1 value for this class as value. \n",
    "        \"\"\"\n",
    "        assert beta >= 0, \"beta needs to be non-negative.\"\n",
    "        recall = self.compute_recall(trues, preds)\n",
    "        precision = self.compute_precision(trues, preds)\n",
    "        f1 = {c:None for c in self.classes}\n",
    "        for c in f1:\n",
    "            score = ((1 + beta)**2 * recall * precision) / (beta**2 * recall + precision)\n",
    "            f1[c] = score\n",
    "        \n",
    "        return f1\n",
    "    \n",
    "    def compute_gmean(self,trues, preds) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the precision metric for the given true and prediction values.\n",
    "\n",
    "        Params:\n",
    "            trues (np.array): [description]\n",
    "            preds (np.array): [description]\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the classes as key and the gmean value for this class as value. \n",
    "        \"\"\"\n",
    "        combined = np.array(list(zip(trues, preds)))\n",
    "        gmean = {c:None for c in self.classes}\n",
    "        for c in gmean:\n",
    "            tp = np.sum((combined[i][0] == c) and (combined[i][1] == c) for i in range(len(combined)))\n",
    "            fn = np.sum((combined[i][0] == c) and (combined[i][1] != c) for i in range(len(combined))) \n",
    "            fp = np.sum((combined[i][0] != c) and (combined[i][1] == c) for i in range(len(combined))) \n",
    "            tn = np.sum((combined[i][0] != c) and (combined[i][1] != c) for i in range(len(combined)))\n",
    "            gmean[c] = np.sqrt((tp / (tp + fn)) * (tn/(tn+fp)))\n",
    "            \n",
    "        return gmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MetricHelper(trues, preds, task, metrics)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ba_thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
