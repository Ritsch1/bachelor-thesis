{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# imports\r\n",
    "import pandas as pd\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from IPython.core.debugger import set_trace\r\n",
    "import torch\r\n",
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export notebook as python script to the ../python-code - folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "!jupyter nbconvert --output-dir=\"../python-code\" --to python WTMF.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[NbConvertApp] Converting notebook WTMF.ipynb to python\n",
      "[NbConvertApp] Writing 6741 bytes to ..\\python-code\\WTMF.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Read in argument-data\r\n",
    "args = pd.read_csv(\"../../data/arguments.csv\", sep=\",\", usecols=[\"statement_id\", \"text_en\"])\r\n",
    "# Only filter for relevant arguments\r\n",
    "relevant_args = set([i for i in range(324, 400)])\r\n",
    "args = args[args.statement_id.isin(relevant_args)]\r\n",
    "# Convert to list of tuples for processing it further\r\n",
    "args = list(zip(args[\"text_en\"], args[\"statement_id\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WTMF algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class WTMF():\r\n",
    "    \"\"\"\r\n",
    "    A class that represents the Weighted Textual Matrix Factorization.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, args:list):\r\n",
    "        \"\"\"\r\n",
    "        Params:\r\n",
    "            args (list): A list of (argument, id) - tuples. \r\n",
    "        \"\"\"\r\n",
    "        self.args = [t[0] for t in args]\r\n",
    "        self.args_ids = [t[1] for t in args]\r\n",
    "        # Initialize GPU for computation if available            \r\n",
    "        machine = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "        self.device = torch.device(machine)\r\n",
    "    \r\n",
    "    def create_tfidf_matrix(self, exclude_stopwords:bool=True) -> None:\r\n",
    "        \"\"\"\r\n",
    "        Create a tfidf - matrix out of the arguments where the rows are words and the columns are sentences.\r\n",
    "        \r\n",
    "        Params:\r\n",
    "            exclude_stopwords (bool): A boolean flag that indicates whether stopwords are kept in the vocabulary or not. Default value is True.\r\n",
    "        \"\"\"\r\n",
    "        # Convert all words to lowercase\r\n",
    "        self.args = list(map(lambda s : s.lower(), self.args))        \r\n",
    "        # Lemmatize the sentences\r\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "        self.args = list(map(lambda s : \" \".join(token.lemma_ for token in nlp(s)), self.args))\r\n",
    "        # Exclude stop words while vectorizing the sentences\r\n",
    "        if exclude_stopwords:\r\n",
    "            vectorizer = TfidfVectorizer(stop_words=\"english\")\r\n",
    "        else:\r\n",
    "            vectorizer = TfidfVectorizer()\r\n",
    "        self.X = vectorizer.fit_transform(self.args)\r\n",
    "        set_trace()\r\n",
    "        # Transform the sparse matrix into a dense matrix and transpose the matrix to represent the words as rows and sentences as columns\r\n",
    "        self.X = torch.from_numpy(self.X.toarray().transpose()).float().to(self.device)\r\n",
    "        \r\n",
    "    def train(self, k:int=20, gamma:float=0.05, weight:float=0.05, training_iterations:int=100, random_seed:int=1, print_frequency:int=1) -> None:\r\n",
    "        \"\"\"\r\n",
    "        Use stochastic gradient descent to find the two latent factor matrices A (words), B (sentences) \r\n",
    "        that minimize the error of the objective function. \r\n",
    "\r\n",
    "        Params:\r\n",
    "            vector_dimension(int, optional): Dimension of the latent vector space the users and items are mapped to. Defaults to 20.\r\n",
    "            gamma (float, optional): Regularization factor to control the overfitting. Defaults to 0.05.\r\n",
    "            weight (float, optional): Weight to control the influence of non-present words in a sentence. Defaults to 0.05.\r\n",
    "            training_iterations (int, optional): Number of training iterations to take. Defaults to 100.\r\n",
    "            random_seed (int, optional): Random seed that is used to intialize the latent factor matrices. Defaults to 1.\r\n",
    "            print_frequency (int, optional): The epoch-frequency with which the error is printed to the console. Default to 1.\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        # Set random seed for reproducability\r\n",
    "        torch.manual_seed(random_seed)\r\n",
    "        # Randomly initialize the latent factor matrices\r\n",
    "        self.A = torch.rand([k, self.X.shape[0]]).to(self.device)\r\n",
    "        self.B = torch.rand([k, self.X.shape[1]]).to(self.device)\r\n",
    "\r\n",
    "        # Identity matrix\r\n",
    "        I = torch.eye(k).to(self.device)\r\n",
    "        \r\n",
    "        # Create the weight matrix. Set value to one if value of X is != 0, else set it to the weights' value\r\n",
    "        W = torch.ones_like(self.X).to(self.device)\r\n",
    "        W[self.X == 0] = weight\r\n",
    "        \r\n",
    "        # Matrix for updating the latent matrices in optimization\r\n",
    "        I_scaled = (gamma * I).to(self.device)\r\n",
    "        gamma_half = torch.tensor(gamma / 2).to(self.device)\r\n",
    "        \r\n",
    "        # Error - variable keep track of it for later visualization \r\n",
    "        error = []\r\n",
    "        error_cur = 0.0\r\n",
    "        frobenius_norm = torch.linalg.matrix_norm\r\n",
    "        inverse = torch.inverse\r\n",
    "        for iteration in range(training_iterations):\r\n",
    "            \r\n",
    "            # Iterate over all words\r\n",
    "            for i in range(self.X.shape[0]):\r\n",
    "                # Iterate over all sentences\r\n",
    "                for j in range(self.X.shape[1]):\r\n",
    "                    # Compute error\r\n",
    "                    A_T = torch.transpose(self.A, 0, 1).to(self.device)\r\n",
    "                    error_cur += ((W[i][j] * ((torch.matmul(A_T[i], self.B[:,j]) - self.X[i][j])**2)) + (gamma_half * ((frobenius_norm(self.A)) + frobenius_norm(self.B))))\r\n",
    "                    # Update latent factor matrices\r\n",
    "                    W_diag_i = torch.diag(W[i]).to(self.device)\r\n",
    "                    W_diag_j = torch.diag(W[:,j]).to(self.device)\r\n",
    "                    temp_mat1 = torch.matmul(self.B, W_diag_i).to(self.device)\r\n",
    "                    temp_mat2 = torch.matmul(self.A, W_diag_j).to(self.device)\r\n",
    "                    # Update latent word vector\r\n",
    "                    self.A[:,i] = torch.matmul(inverse(torch.mm(temp_mat1, torch.transpose(self.B, 0, 1)) + (I_scaled)) , torch.matmul(temp_mat1, torch.transpose(self.X[i], 0, 0))).to(self.device)            \r\n",
    "                    # Update latent sentence vector\r\n",
    "                    self.B[:,j] = torch.matmul(inverse(torch.mm(temp_mat2, A_T) + (I_scaled)) , torch.matmul(temp_mat2, torch.transpose(self.X[:,j], 0, 0))).to(self.device)\r\n",
    "                    \r\n",
    "            error.append(error_cur)\r\n",
    "            error_cur = 0\r\n",
    "            # Print out error w.r.t print-frequency\r\n",
    "            if iteration % print_frequency == 0:\r\n",
    "                print(f\"Error:{error[iteration]:.2f}\\tCurrent Iteration: {iteration+1}\\\\{training_iterations}\")\r\n",
    "    \r\n",
    "    def compute_argument_similarity_matrix(self) -> None:\r\n",
    "        \"\"\"\r\n",
    "        Compute the semantic argument similarity between the latent argument - vectors that were optimized within the argument(sentence) matrix B in the WTMF algorithm.\r\n",
    "        \"\"\"\r\n",
    "        # Normalize all column - vectors in matrix B, so we can use the dot-product on normalized vectors which is equivalent to the cosine-similarity\r\n",
    "        self.B /= torch.norm(self.B, dim=0).to(self.device)\r\n",
    "        # Compute pairwise dot-product of all column vectors\r\n",
    "        self.similarity_matrix = torch.matmul(self.B.T, self.B).to(self.device)\r\n",
    "        # The diagonal will keep the value zero, as the similarity of the argument with itself should not be taken into account as it will always be 1.\r\n",
    "        self.similarity_matrix = self.similarity_matrix.fill_diagonal_(0).to(self.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "wtmf = WTMF(args)\r\n",
    "wtmf.create_tfidf_matrix()\r\n",
    "wtmf.train()\r\n",
    "wtmf.compute_argument_similarity_matrix()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> \u001b[1;32mc:\\users\\rico\\appdata\\local\\temp\\ipykernel_4160\\3910879909.py\u001b[0m(36)\u001b[0;36mcreate_tfidf_matrix\u001b[1;34m()\u001b[0m\n",
      "\n",
      "TfidfVectorizer(stop_words='english')\n",
      "{'cultivation': 56, 'genetically': 111, 'modify': 172, 'plant': 193, 'production': 209, 'food': 102, 'allow': 5, 'prohibit': 211, 'germany': 114, 'genetic': 110, 'engineering': 82, 'likely': 155, 'lead': 151, 'animal': 7, 'specie': 254, 'extinct': 90, 'speak': 253, 'modification': 171, 'long': 158, 'reverse': 235, 'interfere': 142, 'nature': 175, 'render': 229, 'soil': 251, 'grow': 119, 'sterile': 257, 'fact': 91, 'large': 150, 'seed': 241, 'manufacturer': 164, 'powerful': 201, 'drive': 74, 'price': 205, 'prevent': 204, 'spread': 255, 'uncontrolled': 278, 'manner': 162, 'gene': 109, 'harmful': 122, 'health': 124, 'live': 157, 'thing': 267, 'scientific': 239, 'study': 258, 'fight': 100, 'hunger': 134, 'world': 289, 'need': 176, 'fairer': 92, 'distribution': 71, 'help': 127, 'farmer': 94, 'dependent': 63, 'corporation': 49, 'far': 93, 'expensive': 88, 'catch': 29, 'small': 249, 'bankrupt': 18, 'en': 77, 'masse': 165, 'contaminate': 46, 'genome': 112, 'normal': 180, 'bee': 20, 'collect': 36, 'pollen': 196, 'point': 195, 'honey': 131, 'dire': 67, 'consequence': 42, 'uncontrollably': 277, 'lot': 160, 'toxic': 273, 'pesticide': 192, 'use': 284, 'continue': 47, 'consumer': 44, 'chance': 30, 'buy': 25, 'argument': 11, 'poor': 199, 'country': 50, 'rich': 236, 'appropriate': 9, 'technology': 264, 'thank': 266, 'probably': 206, 'soon': 252, 'uniformity': 279, 'supermarket': 260, 'alternative': 6, 'shop': 244, 'crossing': 54, 'non': 179, 'related': 226, 'negative': 177, 'real': 219, 'risk': 237, 'know': 147, 'company': 38, 'carry': 28, 'test': 265, 'cut': 58, 'flesh': 101, 'prevail': 203, 'destroy': 64, 'job': 144, 'conventional': 48, 'organic': 185, 'agriculture': 4, 'important': 138, 'lose': 159, 'connection': 41, 'future': 106, 'crucial': 55, 'importance': 137, 'produce': 208, 'people': 190, 'eat': 76, 'favor': 96, 'enable': 78, 'great': 116, 'diversity': 72, 'prove': 213, 'actually': 0, 'human': 133, 'advantage': 1, 'feed': 97, 'high': 128, 'quality': 215, 'energy': 81, 'resistant': 232, 'pest': 191, 'disease': 69, 'good': 115, 'require': 231, 'ensure': 83, 'base': 19, 'raw': 218, 'material': 166, 'switch': 261, 'quickly': 216, 'coal': 35, 'oil': 183, 'renewable': 230, 'create': 51, 'taste': 262, 'healthy': 125, 'problem': 207, 'intensively': 141, 'approval': 10, 'arid': 12, 'region': 224, 'crop': 53, 'make': 161, 'sense': 242, 'smallholder': 250, 'harvest': 123, 'regional': 225, 'quite': 217, 'natural': 174, 'change': 31, 'simply': 248, 'little': 156, 'fast': 95, 'improve': 139, 'just': 145, 'like': 154, 'traditional': 274, 'breeding': 22, 'meet': 168, 'increase': 140, 'demand': 61, 'canada': 26, 'experience': 89, 'technological': 263, 'progress': 210, 'hinder': 129, 'remain': 228, 'competitive': 39, 'worldwide': 290, 'key': 146, 'plastic': 194, 'packaging': 187, 'fresh': 103, 'fruit': 105, 'vegetable': 286, 'microplastic': 169, 'ocean': 181, 'environmental': 85, 'today': 272, 'discard': 68, 'bad': 14, 'manufacture': 163, 'substance': 259, 'chlorine': 32, 'dangerous': 59, 'die': 65, 'agony': 3, 'waste': 287, 'environment': 84, 'time': 271, 'degrade': 60, 'burn': 24, 'difficult': 66, 'recycle': 221, 'harm': 121, 'river': 238, 'drink': 73, 'water': 288, 'sick': 247, 'consume': 43, 'capita': 27, 'europe': 87, 'climate': 34, 'greenhouse': 117, 'gas': 108, 'release': 227, 'break': 21, 'impact': 136, 'hormonal': 132, 'balance': 16, 'pass': 189, 'bring': 23, 'pollute': 197, 'unsold': 283, 'shred': 246, 'compost': 40, 'fertilizer': 98, 'end': 79, 'field': 99, 'habitat': 120, 'contain': 45, 'unnecessary': 280, 'paper': 188, 'bag': 15, 'environmentally': 86, 'friendly': 104, 'ban': 17, 'useless': 285, 'unclear': 276, 'really': 220, 'responsible': 234, 'pollution': 198, 'sea': 240, 'punishment': 214, 'german': 113, 'mistake': 170, 'come': 37, 'garbage': 107, 'disposal': 70, 'introduce': 143, 'standard': 256, 'unsanitary': 282, 'pack': 186, 'depend': 62, 'endanger': 80, 'resource': 233, 'appetizing': 8, 'clean': 33, 'grocery': 118, 'home': 130, 'wrap': 291, 'practical': 202, 'unpackaged': 281, 'think': 268, 'easy': 75, 'identify': 135, 'label': 148, 'protect': 212, 'moisture': 173, 'heat': 126, 'shelf': 243, 'life': 152, 'reduce': 223, 'currently': 57, 'lack': 149, 'affordable': 2, 'portion': 200, 'offer': 182, 'mean': 167, 'throw': 270, 'away': 13, 'nice': 178, 'light': 153, 'old': 184, 'shopping': 245, 'type': 275, 'recycling': 222, 'threat': 269, 'creation': 52}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "BdbQuit",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4160/1508354713.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwtmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWTMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_tfidf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_argument_similarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4160/3910879909.py\u001b[0m in \u001b[0;36mcreate_tfidf_matrix\u001b[1;34m(self, exclude_stopwords)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Transform the sparse matrix into a dense matrix and transpose the matrix to represent the words as rows and sentences as columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_iterations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_frequency\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4160/3910879909.py\u001b[0m in \u001b[0;36mcreate_tfidf_matrix\u001b[1;34m(self, exclude_stopwords)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Transform the sparse matrix into a dense matrix and transpose the matrix to represent the words as rows and sentences as columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_iterations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_frequency\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ba_thesis\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ba_thesis\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ba_thesis': conda)"
  },
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}