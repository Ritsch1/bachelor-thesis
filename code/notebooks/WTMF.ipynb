{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\r\n",
    "# imports\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from IPython.core.debugger import set_trace\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export notebook as python script to the ../python-code - folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "!jupyter nbconvert --output-dir=\"../python-code\" --to python WTMF.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[NbConvertApp] Converting notebook WTMF.ipynb to python\n",
      "[NbConvertApp] Writing 7790 bytes to ..\\python-code\\WTMF.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read in argument-data\r\n",
    "args = pd.read_csv(\"../../data/arguments.csv\", sep=\",\", usecols=[\"statement_id\", \"text_en\"])\r\n",
    "# Convert to list of tuples for processing it further\r\n",
    "args = list(zip(args[\"text_en\"], args[\"statement_id\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WTMF algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "\r\n",
    "class WTMF():\r\n",
    "    \"\"\"\r\n",
    "    A class that represents the Weighted Textual Matrix Factorization.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, args:list):\r\n",
    "        \"\"\"\r\n",
    "        Params:\r\n",
    "            args (list): A list of (argument, id) - tuples. \r\n",
    "        \"\"\"\r\n",
    "        self.args = [t[0] for t in args]\r\n",
    "        self.args_ids = [t[1] for t in args]\r\n",
    "        # Initialize GPU for computation if available            \r\n",
    "        machine = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "        self.device = torch.device(machine)\r\n",
    "    \r\n",
    "    def create_tfidf_matrix(self, exclude_stopwords:bool=True):\r\n",
    "        \"\"\"\r\n",
    "        Create a tfidf - matrix out of the arguments where the rows are words and the columns are sentences.\r\n",
    "        \r\n",
    "        Params:\r\n",
    "            exclude_stopwords (bool): A boolean flag that indicates whether stopwords are kept in the vocabulary or not. Default value is True.\r\n",
    "        \"\"\"\r\n",
    "        # Exclude stop words while vectorizing the sentences\r\n",
    "        if exclude_stopwords:\r\n",
    "            vectorizer = TfidfVectorizer(stop_words=\"english\")\r\n",
    "        else:\r\n",
    "            vectorizer = TfidfVectorizer()\r\n",
    "        self.X = vectorizer.fit_transform(self.args)\r\n",
    "        # Transform the sparse matrix into a dense matrix and transpose the matrix to represent the words as rows and sentences as columns\r\n",
    "        self.X = torch.from_numpy(self.X.toarray().transpose()).float().to(self.device)\r\n",
    "        \r\n",
    "    def train(self, k:int=10, gamma:float=0.05, weight:float=0.05, training_iterations:int=0, random_seed:int=1, print_frequency:int=1):\r\n",
    "        \"\"\"\r\n",
    "        Use stochastic gradient descent to find the two latent factor matrices A (words), B (sentences) \r\n",
    "        that minimize the error of the objective function. \r\n",
    "\r\n",
    "        Params:\r\n",
    "            vector_dimension(int, optional): Dimension of the latent vector space the users and items are mapped to. Defaults to 10.\r\n",
    "            gamma (float, optional): Regularization factor to control the overfitting. Defaults to 0.05.\r\n",
    "            weight (float, optional): Weight to control the influence of non-present words in a sentence. Defaults to 0.05.\r\n",
    "            training_iterations (int, optional): Number of training iterations to take. Defaults to 20.\r\n",
    "            random_seed (int, optional): Random seed that is used to intialize the latent factor matrices. Defaults to 1.\r\n",
    "            print_frequency (int, optional): The epoch-frequency with which the error is printed to the console. Default to 1.\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        # Set random seed for reproducability\r\n",
    "        np.random.seed = random_seed\r\n",
    "        # Randomly initialize the latent factor matrices\r\n",
    "        self.A = torch.rand([k, self.X.shape[0]]).to(self.device)\r\n",
    "        self.B = torch.rand([k, self.X.shape[1]]).to(self.device)\r\n",
    "        self.X = self.X.to(self.device)\r\n",
    "\r\n",
    "        # Identity matrix\r\n",
    "        I = torch.eye(k).to(self.device)\r\n",
    "        \r\n",
    "        # Create the weight matrix. Set value to one if value of X is != 0, else set it to the weights' value\r\n",
    "        W = torch.ones_like(self.X).to(self.device)\r\n",
    "        W[self.X == 0] = weight\r\n",
    "        \r\n",
    "        # Matrix for updating the latent matrices in optimization\r\n",
    "        I_scaled = (gamma * I).to(self.device)\r\n",
    "        gamma_half = torch.tensor(gamma / 2).to(self.device)\r\n",
    "        \r\n",
    "        # Error - variable keep track of it for later visualization \r\n",
    "        error = []\r\n",
    "        error_cur = 0.0\r\n",
    "        frobenius_norm = torch.linalg.matrix_norm\r\n",
    "        inverse = torch.inverse\r\n",
    "        \r\n",
    "        for iteration in range(training_iterations):\r\n",
    "            \r\n",
    "            # Iterate over all words\r\n",
    "            for i in range(self.X.shape[0]):\r\n",
    "                print(f\"Row:{i}\\{self.X.shape[0]}\")\r\n",
    "                # Iterate over all sentences\r\n",
    "                for j in range(self.X.shape[1]):\r\n",
    "                    # Compute error\r\n",
    "                    A_T = torch.transpose(self.A, 0, 1).to(self.device)\r\n",
    "                    error_cur += ((W[i][j] * ((torch.matmul(A_T[i], self.B[:,j]) - self.X[i][j])**2)) + (gamma_half * ((frobenius_norm(self.A)) + frobenius_norm(self.B))))\r\n",
    "                    # Update latent factor matrices\r\n",
    "                    W_diag_i = torch.diag(W[i]).to(self.device)\r\n",
    "                    W_diag_j = torch.diag(W[:,j]).to(self.device)\r\n",
    "                    temp_mat1 = torch.matmul(self.B, W_diag_i).to(self.device)\r\n",
    "                    temp_mat2 = torch.matmul(self.A, W_diag_j).to(self.device)\r\n",
    "                    self.A[:,i] = torch.matmul(inverse(torch.mm(temp_mat1, torch.transpose(self.B, 0, 1)) + (I_scaled)) , torch.matmul(temp_mat1, torch.transpose(self.X[i], 0, 0))).to(self.device)            \r\n",
    "                    self.B[:,j] = torch.matmul(inverse(torch.mm(temp_mat2, A_T) + (I_scaled)) , torch.matmul(temp_mat2, torch.transpose(self.X[:,j], 0, 0))).to(self.device)\r\n",
    "                    \r\n",
    "            error.append(error_cur)\r\n",
    "            # Print out error w.r.t print-frequency\r\n",
    "            if iteration % print_frequency == 0:\r\n",
    "                print(f\"Error:{error[iteration]:.2f}\\tCurrent Iteration{iteration}\\\\{training_iterations}\")\r\n",
    "    \r\n",
    "    def compute_argument_similarity_matrix(self):\r\n",
    "        \"\"\"\r\n",
    "        Compute the semantic argument similarity between the latent argument - vectors that were optimized within the matrix B in the WTMF algorithm.\r\n",
    "        \"\"\"\r\n",
    "        # Normalize all column - vectors in matrix B, so we can use the dot-product on normalized vectors which is equivalent to the cosine-similarity\r\n",
    "        self.B /= torch.norm(self.B, dim=0).to(self.device)\r\n",
    "        # Compute pairwise dot-product of all column vectors\r\n",
    "        self.similarity_matrix = torch.matmul(self.B.T, self.B).to(self.device)\r\n",
    "        # The diagonal will keep the value zero, as the similarity of the argument with itself should not be taken into account as it will always be 1.\r\n",
    "        self.similarity_matrix = self.similarity_matrix.fill_diagonal_(0).to(self.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "wtmf = WTMF(args)\r\n",
    "wtmf.create_tfidf_matrix()\r\n",
    "wtmf.train()\r\n",
    "wtmf.compute_argument_similarity_matrix()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ba_thesis': conda)"
  },
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}