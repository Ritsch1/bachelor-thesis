{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# imports\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "import scipy\r\n",
    "from IPython.core.debugger import set_trace\r\n",
    "import multiprocessing as mp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export notebook as python script to the ../python-code - folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "!jupyter nbconvert --output-dir=\"../python-code\" --to python WTMF.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[NbConvertApp] Converting notebook WTMF.ipynb to python\n",
      "[NbConvertApp] Writing 7358 bytes to ..\\python-code\\WTMF.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Read in argument-data\r\n",
    "args = pd.read_csv(\"../../data/arguments.csv\", sep=\",\", usecols=[\"statement_id\", \"text_en\"])\r\n",
    "# Convert to list of tuples for processing it further\r\n",
    "args = list(zip(args[\"text_en\"], args[\"statement_id\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WTMF algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class WTMF():\r\n",
    "    \"\"\"\r\n",
    "    A class that represents the Weighted Textual Matrix Factorization.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    def __init__(self, args:list):\r\n",
    "        \"\"\"\r\n",
    "        Params:\r\n",
    "            args (list): A list of (argument, id) - tuples. \r\n",
    "        \"\"\"\r\n",
    "        self.args = [t[0] for t in args]\r\n",
    "        self.args_ids = [t[1] for t in args]\r\n",
    "    \r\n",
    "    def create_tfidf_matrix(self, exclude_stopwords:bool=True):\r\n",
    "        \"\"\"\r\n",
    "        Create a tfidf - matrix out of the arguments where the rows are words and the columns are sentences.\r\n",
    "        \r\n",
    "        Params:\r\n",
    "            exclude_stopwords (bool): A boolean flag that indicates whether stopwords are kept in the vocabulary or not. Default value is True.\r\n",
    "        \"\"\"\r\n",
    "        # Exclude stop words while vectorizing the sentences\r\n",
    "        if exclude_stopwords:\r\n",
    "            vectorizer = TfidfVectorizer(stop_words=\"english\")\r\n",
    "        else:\r\n",
    "            vectorizer = TfidfVectorizer()\r\n",
    "        self.X = vectorizer.fit_transform(self.args)\r\n",
    "        # Transform the sparse matrix into a dense matrix and transpose the matrix to represent the words as rows and sentences as columns\r\n",
    "        self.X = self.X.toarray().transpose()\r\n",
    "    \r\n",
    "    def show_training_process(self, error:float, cur_iteration_num:int, num_all_iterations:int):\r\n",
    "        \"\"\"\r\n",
    "        Visualize the training process.\r\n",
    "\r\n",
    "        Params:\r\n",
    "            error (float): The current error of the optimization process.\r\n",
    "            cur_iteration_num (int): The number of the current training iteration.\r\n",
    "            num_all_iterations (int): The number of the planned training iterations.\r\n",
    "        \"\"\"\r\n",
    "        print(f\"Error:{error:.2f}\\tCurrent Iteration{cur_iteration_num}\\\\{num_all_iterations}\")\r\n",
    "    \r\n",
    "    @staticmethod\r\n",
    "    def parallel_computation(W:np.array, C:np.array, I_scaled:np.array, X:np.array, i:int, j:int, is_first_calculation:bool):\r\n",
    "        \"\"\"\r\n",
    "        Worker function to parallelize the computations across multiple CPUS.\r\n",
    "        \r\n",
    "        Params:\r\n",
    "            W (np.array): Weight Matrix for controlling the influence of non-existent words.\r\n",
    "            C (np.array): One of two latent factor matrices, depends on the value of is_first_calculation.\r\n",
    "            I_scaled (np.array): An Identity - matrix of shape k X k (embedding dimension), scaled by the regularization factor.\r\n",
    "            X (np.array): The tfidf - matrix.\r\n",
    "            i (int): The index of the i-th word in the training iteration \r\n",
    "            j (int): The index of the j-th sentence in the training iteration\r\n",
    "            is_first_calculation (bool): The first calculation deals with the word-latent matrix A, the second calculation with the sentence-latent-matrix B.\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            tuple: A tuple of (diagonalized weight matrix, the dot product of the latent vector and the diagonal weight matrix, and the updated latent vector of word i /sentence j)\r\n",
    "        \"\"\"\r\n",
    "        inverse = np.linalg.inv\r\n",
    "        if is_first_calculation:\r\n",
    "            W_diag_i = np.diag(W[i])\r\n",
    "            temp_mat = np.dot(C, W_diag_i)\r\n",
    "            temp_vec = np.dot(inverse(np.dot(temp_mat, C.transpose()) + (I_scaled)) , np.dot(temp_mat, X[i].transpose()))\r\n",
    "        else:\r\n",
    "            W_diag_j = np.diag(W[:,j])\r\n",
    "            temp_mat = np.dot(C, W_diag_j)\r\n",
    "            temp_vec = np.dot(inverse(np.dot(temp_mat, C.transpose()) + (I_scaled)) , np.dot(temp_mat, X[:,j].transpose()))\r\n",
    "            \r\n",
    "        return (W_diag_i, temp_mat, temp_vec)\r\n",
    "    \r\n",
    "    def train(self, k:int=10, gamma:float=0.05, weight:float=0.05, training_iterations:int=20, random_seed:int=1, print_frequency:int=1):\r\n",
    "        \"\"\"\r\n",
    "        Use stochastic gradient descent to find the two latent factor matrices A (words), B (sentences) \r\n",
    "        that minimize the error of the objective function. \r\n",
    "\r\n",
    "        Params:\r\n",
    "            vector_dimension(int, optional): Dimension of the latent vector space the users and items are mapped to. Defaults to 10.\r\n",
    "            gamma (float, optional): Regularization factor to control the overfitting. Defaults to 0.05.\r\n",
    "            weight (float, optional): Weight to control the influence of non-present words in a sentence. Defaults to 0.05.\r\n",
    "            training_iterations (int, optional): Number of training iterations to take. Defaults to 20.\r\n",
    "            random_seed (int, optional): Random seed that is used to intialize the latent factor matrices. Defaults to 1.\r\n",
    "            print_frequency (int, optional): The epoch-frequency with which the error is printed to the console. Default to 1.\r\n",
    "        \"\"\"\r\n",
    "        # Set random seed for reproducability\r\n",
    "        np.random.seed = random_seed\r\n",
    "        # Randomly initialize the latent factor matrices\r\n",
    "        self.A = np.random.rand(k, self.X.shape[0])\r\n",
    "        self.B = np.random.rand(k, self.X.shape[1])\r\n",
    "\r\n",
    "        # Identity matrix\r\n",
    "        I = np.identity(k)\r\n",
    "        \r\n",
    "        # Create the weight matrix. Set value to one if value of X is != 0, else set it to the weights' value\r\n",
    "        W = np.ones_like(self.X)\r\n",
    "        W[self.X == 0] = weight\r\n",
    "        \r\n",
    "        # Matrix for updating the latent matrices in optimization\r\n",
    "        I_scaled = gamma * I\r\n",
    "        \r\n",
    "        # Error - variable keep track of it for later visualization \r\n",
    "        error = []\r\n",
    "        error_cur = 0.0\r\n",
    "        frobenius_norm = np.linalg.norm\r\n",
    "        inverse = np.linalg.inv\r\n",
    "        \r\n",
    "        for iteration in range(training_iterations):\r\n",
    "            \r\n",
    "            # Iterate over all words\r\n",
    "            for i in range(self.X.shape[0]):\r\n",
    "                print(f\"Row:{i}\\{self.X.shape[0]}\")\r\n",
    "                # Iterate over all sentences\r\n",
    "                for j in range(self.X.shape[1]):\r\n",
    "                    # Compute error\r\n",
    "                    error_cur += ((W[i][j] * ((np.dot(self.A.transpose()[i], self.B[:,j]) - self.X[i][j])**2)) + ((gamma/2) * (frobenius_norm(self.A.reshape(-1)) + frobenius_norm(self.B.reshape(-1)))))\r\n",
    "                    # Update latent factor matrices\r\n",
    "                    W_diag_i = np.diag(W[i])\r\n",
    "                    W_diag_j = np.diag(W[:,j])\r\n",
    "                    temp_mat1 = np.dot(self.B, W_diag_i)\r\n",
    "                    temp_mat2 = np.dot(self.A, W_diag_j)\r\n",
    "                    \r\n",
    "                    self.A[:,i] = np.dot(inverse(np.dot(temp_mat1, self.B.transpose()) + (I_scaled)) , np.dot(temp_mat1, self.X[i].transpose()))            \r\n",
    "                    self.B[:,j] = np.dot(inverse(np.dot(temp_mat2, self.A.transpose()) + (I_scaled)) , np.dot(temp_mat2, self.X[:,j].transpose()))\r\n",
    "                    \r\n",
    "            error.append(error_cur)\r\n",
    "            # Print out error w.r.t print-frequency\r\n",
    "            if iteration % print_frequency == 0:\r\n",
    "                print(f\"Error:{error[iteration]:.2f}\\tCurrent Iteration{iteration}\\\\{training_iterations}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "wtmf = WTMF(args)\r\n",
    "wtmf.create_tfidf_matrix()\r\n",
    "wtmf.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Row:0\\1689\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9944/628813461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwtmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWTMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_tfidf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9944/641052306.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, k, gamma, weight, training_iterations, random_seed, print_frequency)\u001b[0m\n\u001b[0;32m    118\u001b[0m                     \u001b[0mW_diag_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                     \u001b[0mtemp_mat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_diag_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                     \u001b[0mtemp_mat2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_diag_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_mat1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mI_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_mat1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ba_thesis': conda)"
  },
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}