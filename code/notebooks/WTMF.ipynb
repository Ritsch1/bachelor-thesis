{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec39ad63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:16:34.344032Z",
     "iopub.status.busy": "2021-09-03T14:16:34.344032Z",
     "iopub.status.idle": "2021-09-03T14:16:37.814613Z",
     "shell.execute_reply": "2021-09-03T14:16:37.813614Z"
    },
    "papermill": {
     "duration": 3.498687,
     "end_time": "2021-09-03T14:16:37.814613",
     "exception": false,
     "start_time": "2021-09-03T14:16:34.315926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f53a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:16:41.317146Z",
     "iopub.status.busy": "2021-09-03T14:16:41.316146Z",
     "iopub.status.idle": "2021-09-03T14:16:41.339463Z",
     "shell.execute_reply": "2021-09-03T14:16:41.338482Z"
    },
    "papermill": {
     "duration": 0.040314,
     "end_time": "2021-09-03T14:16:41.340462",
     "exception": false,
     "start_time": "2021-09-03T14:16:41.300148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in argument-data\n",
    "args = pd.read_csv(\"../../data/arguments.csv\", sep=\",\", usecols=[\"statement_id\", \"text_en\"])\n",
    "# Only filter for relevant arguments\n",
    "relevant_args = set([i for i in range(324, 400)])\n",
    "args = args[args.statement_id.isin(relevant_args)]\n",
    "# Convert to list of tuples for processing it further\n",
    "args = list(zip(args[\"text_en\"], args[\"statement_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b056e",
   "metadata": {
    "papermill": {
     "duration": 0.008985,
     "end_time": "2021-09-03T14:16:41.357465",
     "exception": false,
     "start_time": "2021-09-03T14:16:41.348480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# WTMF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4d3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:16:41.404463Z",
     "iopub.status.busy": "2021-09-03T14:16:41.379481Z",
     "iopub.status.idle": "2021-09-03T14:16:41.417223Z",
     "shell.execute_reply": "2021-09-03T14:16:41.416228Z"
    },
    "papermill": {
     "duration": 0.050741,
     "end_time": "2021-09-03T14:16:41.417223",
     "exception": false,
     "start_time": "2021-09-03T14:16:41.366482",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "class WTMF():\n",
    "    \"\"\"\n",
    "    A class that represents the Weighted Textual Matrix Factorization.\n",
    "    \"\"\"\n",
    "    def __init__(self, args:list):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            args (list): A list of (argument, id) - tuples. \n",
    "        \"\"\"\n",
    "        self.args = [t[0] for t in args]\n",
    "        self.args_ids = [t[1] for t in args]\n",
    "        # Initialize GPU for computation if available            \n",
    "        machine = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(machine)\n",
    "    \n",
    "    def create_tfidf_matrix(self, exclude_stopwords:bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Create a tfidf - matrix out of the arguments where the rows are words and the columns are sentences.\n",
    "        \n",
    "        Params:\n",
    "            exclude_stopwords (bool): A boolean flag that indicates whether stopwords are kept in the vocabulary or not. Default value is True.\n",
    "        \"\"\"\n",
    "        # Convert all words to lowercase\n",
    "        self.args = list(map(lambda s : s.lower(), self.args))        \n",
    "        # Lemmatize the sentences\n",
    "        nlp = en_core_web_sm.load()\n",
    "        self.args = list(map(lambda s : \" \".join(token.lemma_ for token in nlp(s)), self.args))\n",
    "        # Filter out the \"-PRON-\" - insertion from spacy \n",
    "        self.args = list(map(lambda s: s.replace(\"-PRON-\",\"\"), self.args))\n",
    "        # Exclude stop words while vectorizing the sentences\n",
    "        if exclude_stopwords:\n",
    "            vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "        self.X = vectorizer.fit_transform(self.args)\n",
    "        # Transform the sparse matrix into a dense matrix and transpose the matrix to represent the words as rows and sentences as columns\n",
    "        self.X = torch.from_numpy(self.X.toarray().transpose()).float().to(self.device)\n",
    "        \n",
    "    def train(self, k:int=50, gamma:float=0.05, weight:float=0.05, training_iterations:int=50, random_seed:int=1, print_frequency:int=1) -> [float]:\n",
    "        \"\"\"\n",
    "        Use stochastic gradient descent to find the two latent factor matrices A (words), B (sentences) \n",
    "        that minimize the error of the objective function. \n",
    "\n",
    "        Params:\n",
    "            vector_dimension(int, optional): Dimension of the latent vector space the users and items are mapped to. Defaults to 20.\n",
    "            gamma (float, optional): Regularization factor to control the overfitting. Defaults to 0.05.\n",
    "            weight (float, optional): Weight to control the influence of non-present words in a sentence. Defaults to 0.05.\n",
    "            training_iterations (int, optional): Number of training iterations to take. Defaults to 50.\n",
    "            random_seed (int, optional): Random seed that is used to intialize the latent factor matrices. Defaults to 1.\n",
    "            print_frequency (int, optional): The epoch-frequency with which the error is printed to the console. Default to 1.\n",
    "        \n",
    "        Returns:\n",
    "            [float]: A list containing the error values for every iteration.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set random seed for reproducability\n",
    "        torch.manual_seed(random_seed)\n",
    "        # Randomly initialize the latent factor matrices\n",
    "        self.A = torch.rand([k, self.X.shape[0]]).to(self.device)\n",
    "        self.B = torch.rand([k, self.X.shape[1]]).to(self.device)\n",
    "        # Identity matrix\n",
    "        I = torch.eye(k).to(self.device)\n",
    "        \n",
    "        # Create the weight matrix. Set value to one if value of X is != 0, else set it to the weights' value\n",
    "        W = torch.ones_like(self.X).to(self.device)\n",
    "        W[self.X == 0] = weight\n",
    "        \n",
    "        # Matrix for updating the latent matrices in optimization\n",
    "        I_scaled = (gamma * I).to(self.device)\n",
    "        gamma_half = torch.tensor(gamma / 2).to(self.device)\n",
    "        \n",
    "        # Error - variable keep track of it for later visualization \n",
    "        error = []\n",
    "        error_cur = 0.0\n",
    "        frobenius_norm = torch.linalg.matrix_norm\n",
    "        inverse = torch.inverse\n",
    "        for iteration in range(training_iterations):\n",
    "            \n",
    "            # Iterate over all words\n",
    "            for i in range(self.X.shape[0]):\n",
    "                # Iterate over all sentences\n",
    "                for j in range(self.X.shape[1]):\n",
    "                    # Compute error\n",
    "                    A_T = torch.transpose(self.A, 0, 1).to(self.device)\n",
    "                    error_cur += ((W[i][j] * ((torch.matmul(A_T[i], self.B[:,j]) - self.X[i][j])**2)) + (gamma_half * ((frobenius_norm(self.A)) + frobenius_norm(self.B))))\n",
    "                    # Update latent factor matrices\n",
    "                    W_diag_i = torch.diag(W[i]).to(self.device)\n",
    "                    W_diag_j = torch.diag(W[:,j]).to(self.device)\n",
    "                    temp_mat1 = torch.matmul(self.B, W_diag_i).to(self.device)\n",
    "                    temp_mat2 = torch.matmul(self.A, W_diag_j).to(self.device)\n",
    "                    # Update latent word vector\n",
    "                    self.A[:,i] = torch.matmul(inverse(torch.mm(temp_mat1, torch.transpose(self.B, 0, 1)) + (I_scaled)) , torch.matmul(temp_mat1, torch.transpose(self.X[i], 0, 0))).to(self.device)            \n",
    "                    # Update latent sentence vector\n",
    "                    self.B[:,j] = torch.matmul(inverse(torch.mm(temp_mat2, A_T) + (I_scaled)) , torch.matmul(temp_mat2, torch.transpose(self.X[:,j], 0, 0))).to(self.device)\n",
    "                    \n",
    "            error.append(error_cur)\n",
    "            error_cur = 0\n",
    "            # Print out error w.r.t print-frequency\n",
    "            if iteration % print_frequency == 0:\n",
    "                print(f\"Error:{error[iteration]:.2f}\\tCurrent Iteration: {iteration+1}\\\\{training_iterations}\")\n",
    "\n",
    "        return error\n",
    "    \n",
    "    def compute_argument_similarity_matrix(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute the semantic argument similarity between the latent argument - vectors that were optimized within the argument(sentence) matrix B in the WTMF algorithm.\n",
    "        \"\"\"\n",
    "        # Normalize all column - vectors in matrix B, so we can use the dot-product on normalized vectors which is equivalent to the cosine-similarity\n",
    "        self.B /= torch.norm(self.B, dim=0).to(self.device)\n",
    "        # Compute pairwise dot-product of all column vectors\n",
    "        self.similarity_matrix = self.B.T.matmul(self.B).to(self.device)\n",
    "        # Perform min-max scaling to map the dot-product results from the range [-1,1] to [0,1]\n",
    "        min_value = torch.min(self.similarity_matrix)\n",
    "        max_value = torch.max(self.similarity_matrix)\n",
    "        self.similarity_matrix -= min_value\n",
    "        self.similarity_matrix /= (max_value - min_value)\n",
    "        # The diagonal will have the value zero, as the similarity of the argument with itself should not be taken into account as it will always be 1.\n",
    "        self.similarity_matrix = self.similarity_matrix.fill_diagonal_(0).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55919fec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:16:41.478221Z",
     "iopub.status.busy": "2021-09-03T14:16:41.477223Z",
     "iopub.status.idle": "2021-09-03T14:16:52.642348Z",
     "shell.execute_reply": "2021-09-03T14:16:52.642348Z"
    },
    "papermill": {
     "duration": 10.088229,
     "end_time": "2021-09-03T14:16:51.546454",
     "exception": false,
     "start_time": "2021-09-03T14:16:41.458225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wtmf = WTMF(args)\n",
    "wtmf.create_tfidf_matrix()\n",
    "results = wtmf.train(**wtmf_config)\n",
    "wtmf.compute_argument_similarity_matrix()\n",
    "graphics.plot_training_error(error=results, title=\"WTMF Objective function error\", xlabel=\"Iterations\", ylabel=\"Error\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.533909,
   "end_time": "2021-09-03T14:16:53.699588",
   "environment_variables": {},
   "exception": null,
   "input_path": "WTMF.ipynb",
   "output_path": "WTMF.ipynb",
   "parameters": {
    "gamma": 0.05,
    "k": 50,
    "print_frequency": 1,
    "random_seed": 1,
    "training_iterations": 50,
    "weight": 0.05
   },
   "start_time": "2021-09-03T14:16:31.165679",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
