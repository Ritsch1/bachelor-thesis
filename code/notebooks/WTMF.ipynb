{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# imports\r\n",
    "import pandas as pd\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from IPython.core.debugger import set_trace\r\n",
    "import torch\r\n",
    "import spacy\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export notebook as python script to the ../python-code - folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!jupyter nbconvert --output-dir=\"../python-code\" --to python WTMF.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[NbConvertApp] Converting notebook WTMF.ipynb to python\n",
      "[NbConvertApp] Writing 8383 bytes to ..\\python-code\\WTMF.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Read in argument-data\r\n",
    "args = pd.read_csv(\"../../data/arguments.csv\", sep=\",\", usecols=[\"statement_id\", \"text_en\"])\r\n",
    "# Only filter for relevant arguments\r\n",
    "relevant_args = set([i for i in range(325, 400)])\r\n",
    "args = args[args.statement_id.isin(relevant_args)]\r\n",
    "# Convert to list of tuples for processing it further\r\n",
    "args = list(zip(args[\"text_en\"], args[\"statement_id\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WTMF algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class WTMF():\r\n",
    "    \"\"\"\r\n",
    "    A class that represents the Weighted Textual Matrix Factorization.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, args:list):\r\n",
    "        \"\"\"\r\n",
    "        Params:\r\n",
    "            args (list): A list of (argument, id) - tuples. \r\n",
    "        \"\"\"\r\n",
    "        self.args = [t[0] for t in args]\r\n",
    "        self.args_ids = [t[1] for t in args]\r\n",
    "        # Initialize GPU for computation if available            \r\n",
    "        machine = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "        self.device = torch.device(machine)\r\n",
    "    \r\n",
    "    def create_tfidf_matrix(self, exclude_stopwords:bool=True) -> None:\r\n",
    "        \"\"\"\r\n",
    "        Create a tfidf - matrix out of the arguments where the rows are words and the columns are sentences.\r\n",
    "        \r\n",
    "        Params:\r\n",
    "            exclude_stopwords (bool): A boolean flag that indicates whether stopwords are kept in the vocabulary or not. Default value is True.\r\n",
    "        \"\"\"\r\n",
    "        # Convert all words to lowercase\r\n",
    "        self.args = list(map(lambda s : s.lower(), self.args))        \r\n",
    "        # Lemmatize the sentences\r\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "        self.args = list(map(lambda s : \" \".join(token.lemma_ for token in nlp(s)), self.args))\r\n",
    "        # Filter out the \"-PRON-\" - insertion from spacy \r\n",
    "        self.args = list(map(lambda s: s.replace(\"-PRON-\",\"\"), self.args))\r\n",
    "        # Exclude stop words while vectorizing the sentences\r\n",
    "        if exclude_stopwords:\r\n",
    "            vectorizer = TfidfVectorizer(stop_words=\"english\")\r\n",
    "        else:\r\n",
    "            vectorizer = TfidfVectorizer()\r\n",
    "        self.X = vectorizer.fit_transform(self.args)\r\n",
    "        # Transform the sparse matrix into a dense matrix and transpose the matrix to represent the words as rows and sentences as columns\r\n",
    "        self.X = torch.from_numpy(self.X.toarray().transpose()).float().to(self.device)\r\n",
    "        \r\n",
    "    def train(self, k:int=50, gamma:float=0.05, weight:float=0.05, training_iterations:int=50, random_seed:int=1, print_frequency:int=1) -> [float]:\r\n",
    "        \"\"\"\r\n",
    "        Use stochastic gradient descent to find the two latent factor matrices A (words), B (sentences) \r\n",
    "        that minimize the error of the objective function. \r\n",
    "\r\n",
    "        Params:\r\n",
    "            vector_dimension(int, optional): Dimension of the latent vector space the users and items are mapped to. Defaults to 20.\r\n",
    "            gamma (float, optional): Regularization factor to control the overfitting. Defaults to 0.05.\r\n",
    "            weight (float, optional): Weight to control the influence of non-present words in a sentence. Defaults to 0.05.\r\n",
    "            training_iterations (int, optional): Number of training iterations to take. Defaults to 50.\r\n",
    "            random_seed (int, optional): Random seed that is used to intialize the latent factor matrices. Defaults to 1.\r\n",
    "            print_frequency (int, optional): The epoch-frequency with which the error is printed to the console. Default to 1.\r\n",
    "        \r\n",
    "        Returns:\r\n",
    "            [float]: A list containing the error values for every iteration.\r\n",
    "        \"\"\"\r\n",
    "        \r\n",
    "        # Set random seed for reproducability\r\n",
    "        torch.manual_seed(random_seed)\r\n",
    "        # Randomly initialize the latent factor matrices\r\n",
    "        self.A = torch.rand([k, self.X.shape[0]]).to(self.device)\r\n",
    "        self.B = torch.rand([k, self.X.shape[1]]).to(self.device)\r\n",
    "        # Identity matrix\r\n",
    "        I = torch.eye(k).to(self.device)\r\n",
    "        \r\n",
    "        # Create the weight matrix. Set value to one if value of X is != 0, else set it to the weights' value\r\n",
    "        W = torch.ones_like(self.X).to(self.device)\r\n",
    "        W[self.X == 0] = weight\r\n",
    "        \r\n",
    "        # Matrix for updating the latent matrices in optimization\r\n",
    "        I_scaled = (gamma * I).to(self.device)\r\n",
    "        gamma_half = torch.tensor(gamma / 2).to(self.device)\r\n",
    "        \r\n",
    "        # Error - variable keep track of it for later visualization \r\n",
    "        error = []\r\n",
    "        error_cur = 0.0\r\n",
    "        frobenius_norm = torch.linalg.matrix_norm\r\n",
    "        inverse = torch.inverse\r\n",
    "        for iteration in range(training_iterations):\r\n",
    "            \r\n",
    "            # Iterate over all words\r\n",
    "            for i in range(self.X.shape[0]):\r\n",
    "                # Iterate over all sentences\r\n",
    "                for j in range(self.X.shape[1]):\r\n",
    "                    # Compute error\r\n",
    "                    A_T = torch.transpose(self.A, 0, 1).to(self.device)\r\n",
    "                    error_cur += ((W[i][j] * ((torch.matmul(A_T[i], self.B[:,j]) - self.X[i][j])**2)) + (gamma_half * ((frobenius_norm(self.A)) + frobenius_norm(self.B))))\r\n",
    "                    # Update latent factor matrices\r\n",
    "                    W_diag_i = torch.diag(W[i]).to(self.device)\r\n",
    "                    W_diag_j = torch.diag(W[:,j]).to(self.device)\r\n",
    "                    temp_mat1 = torch.matmul(self.B, W_diag_i).to(self.device)\r\n",
    "                    temp_mat2 = torch.matmul(self.A, W_diag_j).to(self.device)\r\n",
    "                    # Update latent word vector\r\n",
    "                    self.A[:,i] = torch.matmul(inverse(torch.mm(temp_mat1, torch.transpose(self.B, 0, 1)) + (I_scaled)) , torch.matmul(temp_mat1, torch.transpose(self.X[i], 0, 0))).to(self.device)            \r\n",
    "                    # Update latent sentence vector\r\n",
    "                    self.B[:,j] = torch.matmul(inverse(torch.mm(temp_mat2, A_T) + (I_scaled)) , torch.matmul(temp_mat2, torch.transpose(self.X[:,j], 0, 0))).to(self.device)\r\n",
    "                    \r\n",
    "            error.append(error_cur)\r\n",
    "            error_cur = 0\r\n",
    "            # Print out error w.r.t print-frequency\r\n",
    "            if iteration % print_frequency == 0:\r\n",
    "                print(f\"Error:{error[iteration]:.2f}\\tCurrent Iteration: {iteration+1}\\\\{training_iterations}\")\r\n",
    "\r\n",
    "        return error\r\n",
    "    \r\n",
    "    def compute_argument_similarity_matrix(self) -> None:\r\n",
    "        \"\"\"\r\n",
    "        Compute the semantic argument similarity between the latent argument - vectors that were optimized within the argument(sentence) matrix B in the WTMF algorithm.\r\n",
    "        \"\"\"\r\n",
    "        # Normalize all column - vectors in matrix B, so we can use the dot-product on normalized vectors which is equivalent to the cosine-similarity\r\n",
    "        self.B /= torch.norm(self.B, dim=0).to(self.device)\r\n",
    "        # Compute pairwise dot-product of all column vectors\r\n",
    "        self.similarity_matrix = self.B.T.matmul(self.B).to(self.device)\r\n",
    "        # Perform min-max scaling to map the dot-product results from the range [-1,1] to [0,1]\r\n",
    "        min_value = torch.min(self.similarity_matrix)\r\n",
    "        max_value = torch.max(self.similarity_matrix)\r\n",
    "        self.similarity_matrix -= min_value\r\n",
    "        self.similarity_matrix /= (max_value - min_value)\r\n",
    "        # The diagonal will have the value zero, as the similarity of the argument with itself should not be taken into account as it will always be 1.\r\n",
    "        self.similarity_matrix = self.similarity_matrix.fill_diagonal_(0).to(self.device)\r\n",
    "    \r\n",
    "    def plot_training_error(self, error:[float], **kwargs) -> None:\r\n",
    "        \"\"\"\r\n",
    "        Plots the training error for every training iteration.\r\n",
    "        \r\n",
    "        Params:\r\n",
    "            error (list): A list of error - values that correspond to each training iteration of the WTMF - algorithm.    \r\n",
    "            **kwargs: Arbitrary many keyword arguments to customize the plot. E.g. color, linewidth or title.\r\n",
    "        \"\"\"        \r\n",
    "        plt.plot([i for i in range(1, len(error)+1)], error)\r\n",
    "        for k in kwargs.keys():\r\n",
    "            # Invoke the function k of the plt - module to customize the plot\r\n",
    "            getattr(plt, k) (kwargs[k])\r\n",
    "        \r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "wtmf = WTMF(args)\r\n",
    "wtmf.create_tfidf_matrix()\r\n",
    "error = wtmf.train(k=1, training_iterations=1)\r\n",
    "wtmf.compute_argument_similarity_matrix()\r\n",
    "wtmf.plot_training_error(error, title=\"WTMF Objective function error\", xlabel=\"Iterations\", ylabel=\"Error\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> \u001b[1;32mc:\\users\\rico\\appdata\\local\\temp\\ipykernel_17340\\4204474289.py\u001b[0m(61)\u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n",
      "\n",
      "tensor([0.7816, 0.7802, 0.5252, 0.2159, 0.2875, 0.5069, 0.2143, 0.2210, 0.4254,\n",
      "        0.3560, 0.6227, 0.3686, 0.4337, 0.3092, 0.0431, 0.9361, 0.5954, 0.9471,\n",
      "        0.4194, 0.3743, 0.0299, 0.3102, 0.8279, 0.0103, 0.3251, 0.8046, 0.7882,\n",
      "        0.6846, 0.4010, 0.4621, 0.4050, 0.3157, 0.7087, 0.3297, 0.2191, 0.6134,\n",
      "        0.1386, 0.3933, 0.7555, 0.6869, 0.9257, 0.1590, 0.5043, 0.3523, 0.2203,\n",
      "        0.2187, 0.6260, 0.5523, 0.2448, 0.1476, 0.0857, 0.9900, 0.4148, 0.3844,\n",
      "        0.3161, 0.2499, 0.0427, 0.2065, 0.8056, 0.6096, 0.3236, 0.3110, 0.8194,\n",
      "        0.8604, 0.9817, 0.6459, 0.7416, 0.4598, 0.2077, 0.0102, 0.8045, 0.4273,\n",
      "        0.7054, 0.5246, 0.0333])\n",
      "tensor([4.5963])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n",
      "tensor([4.5963])\n",
      "tensor([0.7816, 0.7802, 0.5252, 0.2159, 0.2875, 0.5069, 0.2143, 0.2210, 0.4254,\n",
      "        0.3560, 0.6227, 0.3686, 0.4337, 0.3092, 0.0431, 0.9361, 0.5954, 0.9471,\n",
      "        0.4194, 0.3743, 0.0299, 0.3102, 0.8279, 0.0103, 0.3251, 0.8046, 0.7882,\n",
      "        0.6846, 0.4010, 0.4621, 0.4050, 0.3157, 0.7087, 0.3297, 0.2191, 0.6134,\n",
      "        0.1386, 0.3933, 0.7555, 0.6869, 0.9257, 0.1590, 0.5043, 0.3523, 0.2203,\n",
      "        0.2187, 0.6260, 0.5523, 0.2448, 0.1476, 0.0857, 0.9900, 0.4148, 0.3844,\n",
      "        0.3161, 0.2499, 0.0427, 0.2065, 0.8056, 0.6096, 0.3236, 0.3110, 0.8194,\n",
      "        0.8604, 0.9817, 0.6459, 0.7416, 0.4598, 0.2077, 0.0102, 0.8045, 0.4273,\n",
      "        0.7054, 0.5246, 0.0333])\n",
      "tensor([[0.7816, 0.7802, 0.5252, 0.2159, 0.2875, 0.5069, 0.2143, 0.2210, 0.4254,\n",
      "         0.3560, 0.6227, 0.3686, 0.4337, 0.3092, 0.0431, 0.9361, 0.5954, 0.9471,\n",
      "         0.4194, 0.3743, 0.0299, 0.3102, 0.8279, 0.0103, 0.3251, 0.8046, 0.7882,\n",
      "         0.6846, 0.4010, 0.4621, 0.4050, 0.3157, 0.7087, 0.3297, 0.2191, 0.6134,\n",
      "         0.1386, 0.3933, 0.7555, 0.6869, 0.9257, 0.1590, 0.5043, 0.3523, 0.2203,\n",
      "         0.2187, 0.6260, 0.5523, 0.2448, 0.1476, 0.0857, 0.9900, 0.4148, 0.3844,\n",
      "         0.3161, 0.2499, 0.0427, 0.2065, 0.8056, 0.6096, 0.3236, 0.3110, 0.8194,\n",
      "         0.8604, 0.9817, 0.6459, 0.7416, 0.4598, 0.2077, 0.0102, 0.8045, 0.4273,\n",
      "         0.7054, 0.5246, 0.0333]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "BdbQuit",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17340/1363390767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwtmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWTMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_tfidf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_argument_similarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwtmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_training_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17340/4204474289.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, k, gamma, weight, training_iterations, random_seed, print_frequency)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Identity matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# Create the weight matrix. Set value to one if value of X is != 0, else set it to the weights' value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17340/4204474289.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, k, gamma, weight, training_iterations, random_seed, print_frequency)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Identity matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# Create the weight matrix. Set value to one if value of X is != 0, else set it to the weights' value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ba_thesis\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ba_thesis\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ba_thesis': conda)"
  },
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}