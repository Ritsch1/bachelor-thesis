{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export notebook as python script to the ../python-code - folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=\"jupyter nbcbonvert --output-dir='../python-code' --to python User_Based_Neighborhood.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True\", returncode=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\"jupyter nbcbonvert --output-dir='../python-code' --to python User_Based_Neighborhood.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neighborhood_Model(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all neighborhood based models. The 'predict', and the 'compute_similarity' - functions need to be implemented by inheriting classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rmh, task:str=\"Conviction\"):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            rmh (Rating_Matrix_Handler): A Rating_Matrix_Handler object that provides the relevant rating matrices as well as test indices.\n",
    "            task (str): The task that the model should predict on. Defaults to \"Conviction\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rmh_ = rmh\n",
    "        self.task_ = task\n",
    "        if self.task_ == \"Conviction\":\n",
    "            self.task_train_rating_matrix = self.rmh_.final_rating_matrix_w_usernames[[\"username\"] + [f\"statement_attitude_{i}\" for i in range(324,400)]]\n",
    "            self.task_test_rating_matrix = self.rmh_.test_rating_matrix[[\"username\"] + [f\"statement_attitude_{i}\" for i in range(324, 400)]]\n",
    "            self.test_eval_indices = {user:items[items % 2 == 1] for user,items in self.rmh_.test_eval_indices.items()}\n",
    "        else:\n",
    "            self.task_rating_matrix = self.rmh_.final_rating_matrix_w_usernames[[\"username\"] + [f\"argument_rating_{i}\" for i in range(324,400)]]\n",
    "            self.task_test_rating_matrix = self.rmh_.test_rating_matrix[[\"username\"] + [f\"argument_rating_{i}\" for i in range(324, 400)]]\n",
    "            self.test_eval_indices = {user:items[items % 2 == 0] for user,items in self.rmh_.test_eval_indices.items()}\n",
    "    \n",
    "    def build_lookups(self) -> None:\n",
    "        \"\"\"\n",
    "        Map users and items to numerical values for further indexing.\n",
    "        \"\"\"\n",
    "        self.userid_lookup_ = {username: i for i, username in enumerate(self.task_rating_matrix[\"username\"])}\n",
    "        self.itemid_lookup_ = {item: i for i, item in enumerate(list(self.task_rating_matrix.columns))}\n",
    "        # Reverse the two calculated mappings for bidirectional lookup\n",
    "        self.username_lookup = {user_id: username for username, user_id in self.userid_lookup_.items()}\n",
    "        self.itemname_lookup = {item_id: itemname for itemname, item_id in self.itemid_lookup_.items()}\n",
    "        \n",
    "    def calculate_items_rated_by_user(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculate a dictionary containing usernames as keys and a numpy array of rated items as key.\n",
    "        \"\"\"\n",
    "        self.items_rated_by_user = {}\n",
    "        users = set(self.userid_lookup_.keys())\n",
    "        for u in users:\n",
    "            # Calculate the item-indices that are non-na for each user\n",
    "            self.items_rated_by_user[self.userid_lookup_[u]] = np.argwhere(~pd.isna(self.task_rating_matrix[self.task_rating_matrix[\"username\"] == u]).values)[:,1][1:]\n",
    "        for rated_items in self.items_rated_by_user.values():\n",
    "            # Delete the first entry, as its the username which will not be used for similarity computation\n",
    "            rated_items = rated_items[1:]\n",
    "    \n",
    "    def compute_mean_ratings(self, for_users:bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Compute the mean rating for users/items depending on the for_users - flag as a dictionary with users/items as key and the average rating as value.\n",
    "\n",
    "        Params:\n",
    "            for_users (bool, optional): If set to True, calculate user-rating means. If set to False, calculate item-rating means. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.mean_ratings = {}\n",
    "        \n",
    "        if for_users:\n",
    "            for username, user_id in self.userid_lookup_.items():\n",
    "                self.mean_ratings[user_id] = np.nanmean(np.array(self.task_rating_matrix[self.task_rating_matrix[\"username\"] == username].values[0][1:],dtype=float))\n",
    "        else:\n",
    "            # Exclude the username column\n",
    "            for item in self.train_rating_matrix.columns[1:]:\n",
    "                self.mean_ratings[self.itemid_lookup_[item]] = np.nanmean(self.task_rating_matrix[item].values)\n",
    "    \n",
    "    def compute_mutual_objects(self, iterable1:Iterable, iterable2:Iterable) -> set:\n",
    "        \"\"\"\n",
    "        Computes the mutual objects of two iterables.\n",
    "\n",
    "        Args:\n",
    "            iterable1 (Iterable): First iterable object.\n",
    "            iterable2 (Iterable): Second iterable object.\n",
    "\n",
    "        Returns:\n",
    "            set: The mutual objects of the first and second iterable object.\n",
    "        \"\"\"\n",
    "        return set(iterable1).intersection(set(iterable2))\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, user:str, item:str) -> int:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute_similarity(self, object1, object2) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between the ratings of both objects. As the similarity is only computed over mutual ratings of both objects, the dimension of both rating vectors must be equal.\n",
    "\n",
    "        Args:\n",
    "            ratings1 (np.array): First object.\n",
    "            ratings2 (np.array): Second object.\n",
    "\n",
    "        Returns:\n",
    "            float: Similarity score of both rating vectors.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute_similarity_matrix(self) -> np.array:\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, k:int, similarity_threshold:float) -> float:\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User_Neighborhood_Pearson_Centered(Neighborhood_Model):\n",
    "    \"\"\"\n",
    "    A user - based neighborhood model that takes into account rating bias by centering the raw data for each user and applying the Pearson Correlation Coefficient for predicting the similarity of user-pairs. \n",
    "    \"\"\"\n",
    "    def __init__(self, rmh, task='Conviction'):\n",
    "        super().__init__(rmh, task=task)\n",
    "        if self.task_ == \"Conviction\":\n",
    "            # Subtract the row - mean from all values in that row\n",
    "            self.mean_centered_train_rating_matrix = self.task_train_rating_matrix.drop(\"username\", axis=1).sub(self.task_train_rating_matrix.drop(\"username\", axis=1).mean(axis=1), axis=0).values\n",
    "        else:\n",
    "            # Subtract the row mean from all the values in that row\n",
    "            self.mean_centered_train_rating_matrix = self.task_train_rating_matrix.values[1:].sub(self.task_train_rating_matrix.drop(\"username\", axis=1).mean(axis=1), axis=0).values\n",
    "    \n",
    "    def compute_similarity(self, user1:int, user2:int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the Pearson Correlation Coefficient for the two rating vectors.\n",
    "\n",
    "        Args:\n",
    "            user1 (int): First user.\n",
    "            user2 (int): Second user.\n",
    "\n",
    "        Returns:\n",
    "            float: Pearson Correlation Coefficient of both rating vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get rated items of both users\n",
    "        rated_items1 = self.items_rated_by_user[user1]\n",
    "        rated_items2 = self.items_rated_by_user[user2]\n",
    "     \n",
    "        # Get mutual rated items\n",
    "        mutual_rated_items = self.compute_mutual_objects(rated_items1, rated_items2)\n",
    "        # If there are no mutual rated items, return 0 for the Pearson Correlation Coefficient\n",
    "        if len(mutual_rated_items) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get mean rating of both users\n",
    "        mean_rating1 = self.mean_ratings[user1]\n",
    "        mean_rating2 = self.mean_ratings[user2]\n",
    "        \n",
    "        # Variable holding the difference between actual rating and mean rating for both users, as this value needs to be calculated multiple times\n",
    "        diffs = []\n",
    "        for i, item in enumerate(mutual_rated_items):\n",
    "           r_u1 = self.rmh.final_rating_matrix_w_usernames[self.task_train_rating_matrix[\"username\"] == user1][item]\n",
    "           r_u2 = self.rmh.final_rating_matrix_w_usernames[self.task_train_rating_matrix[\"username\"] == user2][item]\n",
    "           diffs.append(tuple(r_u1 - mean_rating1, r_u2 - mean_rating2))\n",
    "        \n",
    "        # Calculate the nominator and denominator of the Pearson Correlation Coefficient\n",
    "        # Transform the list into numpy-array for indexing\n",
    "        diffs = np.array(diffs)\n",
    "        nominator = np.sum(diffs[:,0] * diffs[:,1])\n",
    "        denominator = np.sqrt(np.sum(diffs[:,0]**2) * np.sum(diffs[:,1]**2))\n",
    "        \n",
    "        # Catch division by zero\n",
    "        try:\n",
    "            return nominator / denominator\n",
    "        except:\n",
    "            return 0.0\n",
    "        \n",
    "    def compute_similarity_matrix(self):\n",
    "        \"\"\"\n",
    "        Compute a dictionary containing the target-user name as key and a dictionary {user_id:similarity_value} as value for the corresponding similarity value with other users.\n",
    "        \"\"\"\n",
    "        super().compute_similarity_matrix()\n",
    "        similarity_values = []\n",
    "        users = self.userid_lookup_.values()\n",
    "        counter = 1\n",
    "        for target_user in users:\n",
    "            print(f\"Computation started for user {counter}\")\n",
    "            similarity_values_target_user = []\n",
    "            for user in users:\n",
    "                # Set the similarity value of target user with himself to -1 to ensure that his ratings are not used for himself in the prediction\n",
    "                if target_user == user:\n",
    "                    similarity_values_target_user.append( (user, -1) )\n",
    "                else:\n",
    "                    similarity_values_target_user.append( (user, self.compute_similarity(target_user, user)) ) \n",
    "        \n",
    "            similarity_values.append(similarity_values_target_user)\n",
    "            counter += 1\n",
    "            \n",
    "        # Convert the list of lists to a 2d - numpy array for later processing    \n",
    "        self.pearson_correlation_matrix = np.array(similarity_values)\n",
    "\n",
    "    def calculate_k_closest_users(self, item:int, target_user:int, k:int, similarity_threshold:float) -> np.array:\n",
    "        \"\"\"\n",
    "        Calculate the k-closest users to the target-user that rated the same item and whose similarity value is above the similarity threshold.\n",
    "        \n",
    "        Params:\n",
    "            item (int): The item-id for which the k closest users have to be found w.r.t. the target-user.\n",
    "            target_user (int): The target user-id for which the k-closest users have to be found.\n",
    "            k (int): The upper bound of the number of users that should be included in the final set. \n",
    "            similarity_threshold (float): A similarity threshold to set the minimum degree of similarity that a user has to have in order to be included in the final set.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: A numpy array containing tuples [user_id, similarity_value].\n",
    "        \"\"\"\n",
    "        # Get array of similarities for target user\n",
    "        user_similarities = self.pearson_correlation_matrix[target_user]\n",
    "        # Get all users with their rated items\n",
    "        users_rated_items = tuple(self.items_rated_by_user.items())\n",
    "        # Filter for all users that have rated the same item\n",
    "        users_that_rated = np.array([user_id for user_id, rated_items in users_rated_items if item in rated_items])\n",
    "        # Get all indices of users where pearson similarity >= threshold\n",
    "        users_sim_bigger_thresh = np.nonzero(user_similarities[:,1] >= similarity_threshold)\n",
    "        # Build intersection of users who rated the item and whose similarity value >= threshold\n",
    "        user_set = np.intersect1d(users_that_rated, users_sim_bigger_thresh)\n",
    "        # Filter the similarities for the calculated user_ids\n",
    "        user_similarities = user_similarities[user_set]\n",
    "        # Sort depending on the similarity value\n",
    "        user_similarities = user_similarities[user_similarities[:, 1].argsort()]\n",
    "        # Depending on the size of the set, return the final [user-ids, similarity-value] tuples\n",
    "        if len(user_similarities >= k):\n",
    "            return user_similarities[-k:]\n",
    "        else:\n",
    "            return user_similarities\n",
    "    \n",
    "    def predict(self, target_user:int, item:int, k:int, similarity_threshold:float) -> int:\n",
    "        super().predict(target_user, item)\n",
    "        # Get the k -closest [user_id, similarity_value] - tuples for the target-user and item \n",
    "        k_closest_users = self.calculate_k_closest_users(item, target_user, k, similarity_threshold)\n",
    "        # Calculate the mean-centered prediction\n",
    "        nominator = []\n",
    "        denominator = []\n",
    "        for user in k_closest_users[:,0]:\n",
    "            user_int = int(user)\n",
    "            pearson_correlation = self.pearson_correlation_matrix[target_user][user_int][1]\n",
    "            denominator.append(abs(pearson_correlation))\n",
    "            nominator.append(pearson_correlation * self.mean_centered_train_rating_matrix[user_int][item])\n",
    "        \n",
    "        nominator = sum(nominator)\n",
    "        denominator = sum(denominator)\n",
    "        prediction = self.mean_ratings[target_user] + (nominator / denominator)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def evaluate(self, k, similarity_threshold):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the model on the test dataset for a specific task.\n",
    "\n",
    "        Args:\n",
    "            task (str, optional): The task that the model should be evaluated on. Can either be \"Conviction\" (columns with ratings 0 & 1) or \"Weight\" (columns with rating [0-6]). Defaults to \"Conviction\".\n",
    "\n",
    "        Returns:\n",
    "            float: RMSE if task is \"Weight\" and mean accuracy if task is \"Conviction\".\n",
    "        \"\"\"\n",
    "        # Filter the evaluation indices based on the task\n",
    "        if self.task_ == \"Conviction\":\n",
    "            # Calculate the mean-accuracy for the Prediction of Conviction (PoC) - task \n",
    "            mean_acc = 0.0\n",
    "            # Variable for counting the correct 0/1 prediction\n",
    "            count_equality = 0\n",
    "            for username, test_samples in self.test_eval_indices.items():\n",
    "                # Get the target-user id\n",
    "                target_user_id = self.userid_lookup_[username[0]]\n",
    "                for item_id in test_samples:\n",
    "                # Look up the true value\n",
    "                    true_value = self.task_test_rating_matrix[self.task_test_rating_matrix[\"username\"] == username[0]][self.itemname_lookup_[item_id+1]]\n",
    "                    prediction = round(self.predict(target_user_id, item_id, k, similarity_threshold))\n",
    "                    # If the prediction is correct, increment the counter\n",
    "                    if  true_value == prediction:\n",
    "                        count_equality += 1\n",
    "                # Normalize by the number of test samples for this user\n",
    "                mean_acc += count_equality / len(test_samples)\n",
    "                # Set the count equality to 0 for the next user\n",
    "                count_equality = 0\n",
    "            # Normalize the error by the number of users in the test-set\n",
    "            mean_acc /= len(self.test_eval_indices)\n",
    "        \n",
    "            return mean_acc\n",
    "        else:\n",
    "            # Get even-indexed arguments that correspond to weight arguments in the range [0,6]  \n",
    "            test_eval_indices_copy = {user:items[items % 2 == 0] for user,items in self.rmh.test_eval_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for executing the Rating-Matrix-Handler notebook\n",
    "timepoint = \"T1_T2\"\n",
    "train_path = f\"../../data/{timepoint}/train.csv\"\n",
    "test_path  = f\"../../data/{timepoint}/test.csv\"\n",
    "%run Rating_Matrix_Handler.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Rating_Matrix_Handler' object has no attribute 'final_rating_matrix_w_usernames'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8916/3694603301.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munpc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUser_Neighborhood_Pearson_Centered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_lookups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0munpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_items_rated_by_user\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0munpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mean_ratings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0munpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_similarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8916/842271609.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rmh, task)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Conviction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Conviction\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;31m# Subtract the row - mean from all values in that row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8916/3948753127.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rmh, task)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Conviction\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_train_rating_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmh_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_rating_matrix_w_usernames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"username\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf\"statement_attitude_{i}\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m324\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_test_rating_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmh_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_rating_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"username\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf\"statement_attitude_{i}\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m324\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_eval_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmh_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_eval_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Rating_Matrix_Handler' object has no attribute 'final_rating_matrix_w_usernames'"
     ]
    }
   ],
   "source": [
    "unpc = User_Neighborhood_Pearson_Centered(rmh)\n",
    "unpc.build_lookups()\n",
    "unpc.calculate_items_rated_by_user()\n",
    "unpc.compute_mean_ratings()\n",
    "unpc.compute_similarity_matrix()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ba_thesis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
