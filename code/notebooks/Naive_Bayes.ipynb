{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes_CF():\n",
    "    \"\"\"\n",
    "    Class representing a Naive - Bayes classifier implementation for the collaborative filterting setting of recommender systems.\n",
    "    \"\"\"\n",
    "    def __init__(self, rmh, is_task_conviction:bool=True, laplacian_smoothing:float=0.0):\n",
    "        self.rmh_ = rmh\n",
    "        self.is_task_conviction = is_task_conviction\n",
    "        self.laplacian_smoothing_ = laplacian_smoothing\n",
    "        arg_range = [x for x in range(324,400) if x != 397]\n",
    "        if is_task_conviction:\n",
    "            self.possibles_classes = set([0,1])\n",
    "            self.test_eval_indices = {user:items[items % 2 == 1] for user,items in self.rmh_.test_eval_indices.items()}\n",
    "            self.task_test_rating_matrix = self.rmh_.test_rating_matrix[[\"username\"] + [f\"statement_attitude_{i}\" for i in arg_range]]\n",
    "            self.task_train_rating_matrix = self.rmh_.final_rating_matrix_w_usernames[[\"username\"] + [f\"statement_attitude_{i}\" for i in arg_range]]\n",
    "        else:\n",
    "            self.possibles_classes = set([i for i in range(7)])\n",
    "            self.test_eval_indices = {user:items[items % 2 == 0] for user,items in self.rmh_.test_eval_indices.items()}\n",
    "            self.task_test_rating_matrix = self.rmh_.test_rating_matrix[[\"username\"] + [f\"argument_rating_{i}\" for i in arg_range]]\n",
    "            self.task_train_rating_matrix = self.rmh_.final_rating_matrix_w_usernames[[\"username\"] + [f\"argument_rating_{i}\" for i in arg_range]]\n",
    "            \n",
    "        self.numerical_rating_matrix = self.rmh_.final_rating_matrix_w_usernames.drop(\"username\", axis=1).values\n",
    "    \n",
    "    def build_lookups(self) -> None:\n",
    "        \"\"\"\n",
    "        Map users and items to numerical values for further indexing.\n",
    "        \"\"\"\n",
    "        self.userid_lookup_ = {username: i for i, username in enumerate(self.rmh_.final_rating_matrix_w_usernames[\"username\"])}\n",
    "        self.itemid_lookup_ = {item: i-1 for i, item in enumerate(list(self.rmh_.final_rating_matrix_w_usernames.columns))}\n",
    "        # Reverse the two calculated mappings for bidirectional lookup\n",
    "        self.username_lookup_ = {user_id: username for username, user_id in self.userid_lookup_.items()}\n",
    "        self.itemname_lookup_ = {item_id: itemname for itemname, item_id in self.itemid_lookup_.items()}\n",
    "    \n",
    "    def compute_bias(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute item as well as user - bias as the deviation from the global average rating as a dictionary.\n",
    "        \"\"\"\n",
    "        if self.is_task_conviction:\n",
    "            global_average_rating = np.nanmean(self.numerical_rating_matrix[:, 1::2])\n",
    "            self.user_bias = {user_id: np.nanmean(np.array(self.task_train_rating_matrix[self.rmh_.final_rating_matrix_w_usernames[\"username\"] == username].values[0][2::2],dtype=float)) - global_average_rating for username, user_id in self.userid_lookup_.items()}\n",
    "        else:\n",
    "            global_average_rating = np.nanmean(self.numerical_rating_matrix[:, 0::2])\n",
    "\n",
    "            self.user_bias = {user_id: np.nanmean(np.array(self.task_train_rating_matrix[self.rmh_.final_rating_matrix_w_usernames[\"username\"] == username].values[0][1::2],dtype=float)) - global_average_rating for username, user_id in self.userid_lookup_.items()}\n",
    "        self.item_bias = {item_id: np.nanmean(self.rmh_.final_rating_matrix_w_usernames[self.itemname_lookup_[item_id]].values) - global_average_rating for item_name, item_id in self.itemid_lookup_.items() if item_name != \"username\"}\n",
    "    \n",
    "    def compute_prior_prob(self) -> None:   \n",
    "        \"\"\"\n",
    "        Compute the prior probability for every item/rating combination and save it into a class dictionary.\n",
    "        \"\"\"\n",
    "        items_without_username = set([self.itemid_lookup_[item] for item in self.itemid_lookup_.keys() if item != \"username\"])\n",
    "        # Build dictionary to hold the prior probabilities for all item/class combinations\n",
    "        self.prior_prob_for_item = {item_id: {} for item_id in items_without_username}\n",
    "        for item_id in items_without_username:\n",
    "            for c in self.possibles_classes:\n",
    "                # Calculate the number of users that rated the item with class c\n",
    "                class_count = len(self.numerical_rating_matrix[self.numerical_rating_matrix[:,item_id] == c])\n",
    "                # Calculate all the users that gave a rating for the item\n",
    "                rated_count = np.sum(~np.isnan(self.numerical_rating_matrix[:,item_id]))\n",
    "                self.prior_prob_for_item[item_id][c] = (class_count + self.laplacian_smoothing_) / (rated_count + rated_count * self.laplacian_smoothing_) \n",
    "    \n",
    "    def compute_users_that_rated_item_with_class(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute a lookup dictionary that associates a item-id with all possibles classes. \n",
    "        These classes itself are associated with all users that rated the associated item with the associated class.\n",
    "        \"\"\"\n",
    "        items_without_username = set([self.itemid_lookup_[item] for item in self.itemid_lookup_.keys() if item != \"username\"])\n",
    "        self.users_rated_item_with_class = {i: {c: None} for c in self.possibles_classes for i in items_without_username}\n",
    "        \n",
    "        for item_id in items_without_username:\n",
    "            for c in self.possibles_classes:\n",
    "                # Calculate the users that rated the item with the class\n",
    "                users = np.argwhere(self.numerical_rating_matrix[:,item_id] == c).flatten()\n",
    "                # Set the users in the dictionary\n",
    "                self.users_rated_item_with_class[item_id][c] = users\n",
    "                  \n",
    "    def compute_likelihood(self, _class:int, item:int, user:int, epsilon_shift:float=0.000000001) -> float:\n",
    "        \"\"\"\n",
    "        Compute the likelihood of observed ratings given the class for a provided user-item combination.\n",
    "        \n",
    "        Params:\n",
    "            _class: class label for which the likelihood is computed.\n",
    "            item: The item - id.\n",
    "            user: The user -id.\n",
    "            epsilon_shift: How strongly should the likelihood be shifted if there are no users that gave the same class label to a\n",
    "            item in question.  \n",
    "        \n",
    "        Returns:\n",
    "            float: The computed likelihood.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute items the user has rated\n",
    "        items_rated_by_user = np.argwhere(~np.isnan(self.numerical_rating_matrix[user]))\n",
    "        \n",
    "        # Get users that rated the provided item with the provided class\n",
    "        users_rated_item_with_class = self.users_rated_item_with_class[item][_class]\n",
    "        \n",
    "        likelihood = 1\n",
    "        for k in items_rated_by_user:\n",
    "            # The rating r_u_k given by the provided user to item k\n",
    "            rating_given_by_user = self.numerical_rating_matrix[user][k]\n",
    "            # The number of users that rated the provided item with the provided class and that additionaly rated item k with r_u_k\n",
    "            num_users_identical_rating = np.sum(self.numerical_rating_matrix[[users_rated_item_with_class]][:,k] == rating_given_by_user)\n",
    "            # The number of users that rated the provided item with the provided class and specified a rating for item k\n",
    "            all_users = len(self.numerical_rating_matrix[[users_rated_item_with_class]][:,k])\n",
    "            # Due to assumption of independence of ratings, the likelihood is the result of a product of the single probalities\n",
    "            ratio = ((num_users_identical_rating + self.laplacian_smoothing_) / (all_users + all_users * self.laplacian_smoothing_))\n",
    "            if ratio == 0:\n",
    "                likelihood *= epsilon_shift\n",
    "            else:\n",
    "                likelihood *= ratio\n",
    "            \n",
    "        return likelihood\n",
    "    \n",
    "    def predict(self, user:int, item:int) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the most probable class label given the ratings.\n",
    "\n",
    "        Params:\n",
    "            user (int): The user-id for which the prediction is made.\n",
    "            item (int): The item-id for which the prediction is made.\n",
    "\n",
    "        Returns:\n",
    "            int: The class label that maximizes the posterior probability.\n",
    "        \"\"\"\n",
    "        # Get the prior probability for each possible class - label\n",
    "        prior_probs = {c: self.prior_prob_for_item[item][c] for c in self.possibles_classes}\n",
    "        # Variable for holding the posterior probability associated with each class label\n",
    "        posterior_probs = {c: None for c in self.possibles_classes}\n",
    "        # Calculate the posterior probability for all possible class labels\n",
    "        for c in self.possibles_classes:\n",
    "            posterior_probs[c] = self.compute_likelihood(c, item, user) * prior_probs[c]\n",
    "        # Return the class label that maximizes the posterior probability\n",
    "        max_posterior = max(posterior_probs, key=posterior_probs.get)\n",
    "        \n",
    "        return max_posterior\n",
    "    \n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the performance score of the Naive Bayes model on the test dataset.\n",
    "\n",
    "        Returns:\n",
    "            float: The performance score on the test dataset. In the case of the 'Conviction' task it is the mean - accuracy error. In the case of the 'Weight' task it is the RMSE.\n",
    "        \"\"\"\n",
    "        \n",
    "        trues, preds = [], []\n",
    "        \n",
    "        # Only retrieve the test instances of the conviction task (odd-indexed)\n",
    "        if self.is_task_conviction:\n",
    "            # Calculate the mean-accuracy for the Prediction of Conviction (PoC) - task \n",
    "            mean_acc = 0.0\n",
    "            # Variable for counting the correct 0/1 prediction\n",
    "            count_equality = 0\n",
    "            for username, test_samples in self.test_eval_indices.items():\n",
    "                # Get the target-user id\n",
    "                target_user_id = self.userid_lookup_[username[0]]\n",
    "                for item_id in test_samples:\n",
    "                # Look up the true value \n",
    "                    true_value = self.task_test_rating_matrix[self.task_test_rating_matrix[\"username\"] == username[0]][self.itemname_lookup_[item_id]]\n",
    "                    prediction = round(self.predict(target_user_id, item_id))\n",
    "                    #print(inttrue_value)\n",
    "                    trues.append(int(true_value))\n",
    "                    preds.append(int(prediction))\n",
    "                    # If the prediction is correct, increment the counter\n",
    "                    if  int(true_value) == prediction:\n",
    "                        count_equality += 1\n",
    "                # Normalize by the number of test samples for this user\n",
    "                mean_acc += count_equality / len(test_samples)\n",
    "                # Set the count equality to 0 for the next user\n",
    "                count_equality = 0\n",
    "            # Normalize the error by the number of users in the test-set\n",
    "            mean_acc /= len(self.test_eval_indices)\n",
    "            print(f\"Accuracy: {mean_acc:.3f}\")\n",
    "            return np.array(trues), np.array(preds)\n",
    "        \n",
    "        # Only retrieve the test instances of the weight task (even-indexed)\n",
    "        else:\n",
    "            rmse_error, prediction_distance = 0.0, 0.0\n",
    "            for username, test_samples in self.test_eval_indices.items():\n",
    "                # Get the target-user-id\n",
    "                target_user_id = self.userid_lookup_[username[0]]\n",
    "                for item_id in test_samples:\n",
    "                    # Look up the true value\n",
    "                    true_value = int(self.task_test_rating_matrix[self.task_test_rating_matrix[\"username\"] == username[0]][self.itemname_lookup_[item_id]])\n",
    "                    prediction = int(round(self.predict(target_user_id, item_id)))\n",
    "                    #if prediction == 0:\n",
    "                    #    print(username[0], self.itemname_lookup_[item_id], true_value)\n",
    "                    trues.append(int(true_value))\n",
    "                    preds.append(int(prediction))\n",
    "                    prediction_distance += (int(true_value) - prediction)**2\n",
    "                # Normalize by the number of test samples for this user     \n",
    "                rmse_error += (prediction_distance / len(test_samples))\n",
    "                # Set the prediction distance to 0 for the next user\n",
    "                prediction_distance = 0\n",
    "            # Normalize the prediction_distance by the number of users in the test-set\n",
    "            rmse_error /= len(self.test_eval_indices)\n",
    "            print(f\"RMSE: {rmse_error:.3f}\")\n",
    "            return np.array(trues), np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb = Naive_Bayes_CF(rmh) if task == \"Conviction\" else Naive_Bayes_CF(rmh, is_task_conviction=False)\n",
    "nb.build_lookups()\n",
    "nb.compute_prior_prob()\n",
    "nb.compute_users_that_rated_item_with_class()\n",
    "trues,preds = nb.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run MetricHelper.ipynb\n",
    "print(\"Averaged metrics:\\n\",mh.compute_average_metrics())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c4321509887871942225181aea45e229e5aed2157cb28edcc519edea6ae29dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
