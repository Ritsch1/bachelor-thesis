{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# imports\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!jupyter nbconvert --output-dir=\"../python-code\" --to python Create_Train_Test_Splits.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_input_prompt=True"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[NbConvertApp] Converting notebook Create_Train_Test_Splits.ipynb to python\n",
      "[NbConvertApp] Writing 2186 bytes to ..\\python-code\\Create_Train_Test_Splits.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def merge_datafiles(files:list):\r\n",
    "    \"\"\"\r\n",
    "    Merges the data files together.\r\n",
    "    \r\n",
    "    params:\r\n",
    "        files: The files that contain the data.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    dfs = [pd.read_csv(f, sep=\",\") for f in files]\r\n",
    "    concat_df = pd.concat(dfs, axis=0)\r\n",
    "    return concat_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def normalize_sim_scores(df:pd.DataFrame, max_value:int=5):\r\n",
    "    \"\"\"\r\n",
    "    Normalize continous similarity scorings by dividing through the maximum value.\r\n",
    "    \r\n",
    "    params:\r\n",
    "        df: The dataframe that contains all the data.\r\n",
    "        max_value: The maximum possible value for a dataset.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # normalize by dividing through the maxmimum value, which is 5 for this dataset\r\n",
    "    df[\"regression_label\"] = df[\"regression_label\"].map(lambda l: float(l)/5)\r\n",
    "    df.to_csv(\"Merged_dataset.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def split_data(df_orig:pd.DataFrame, train_ratio=0.8) -> tuple:\r\n",
    "    \"\"\"\r\n",
    "    Split the dataframe with train_ratio, 1-train_ratio split into training and test set.\r\n",
    "    \r\n",
    "    params:\r\n",
    "        df_orig: The original dataframe containing the complete data.\r\n",
    "        train_ratio: The ratio of training data compared to the complete data.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    mask = np.random.rand(len(df_orig)) < train_ratio\r\n",
    "    train = df_orig[mask]\r\n",
    "    test = df_orig[~mask]\r\n",
    "    return (train, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def write_split(data_split:tuple):\r\n",
    "    \"\"\"\r\n",
    "    Creates train and test directory and saves the data splits into them.\r\n",
    "    \r\n",
    "    params:\r\n",
    "        data_split: tuple t where t[0] contains the training data and t[1] contains the test data.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    dirs = [\"TRAIN\", \"TEST\"]\r\n",
    "    for d in dirs:\r\n",
    "        os.makedirs(d) \r\n",
    "\r\n",
    "    data_split[0].to_csv(dirs[0] + \"/\" + dirs[0] + \"_\" + f, index=False)\r\n",
    "    data_split[1].to_csv(dirs[1] + \"/\" + dirs[1] + \"_\" + f, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Argument Facet Similarity dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# Read in relevant data\r\n",
    "files = [f for f in os.listdir(\"../../data/fine_tune/Argument_Facet_Similarity\") if f.endswith(\".csv\")]\r\n",
    "# Merge data\r\n",
    "merged_df = merge_datafiles(files)\r\n",
    "# Normalize the similarity scores to the range [0,1]\r\n",
    "normalize_sim_scores(merged_df)\r\n",
    "# Create data split\r\n",
    "data_split = split_data(merged_df)\r\n",
    "# Write data splits into files\r\n",
    "write_split(data_split)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('assignment3': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "a62e5bbf5970cd9f299de7aca32613cba63d66c85f73b0e8e9b824abe8724139"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}