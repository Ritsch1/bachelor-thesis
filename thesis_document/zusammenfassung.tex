%%% Die folgende Zeile nicht Ã¤ndern!
\section*{\ifthenelse{\equal{\sprache}{deutsch}}{Zusammenfassung}{Abstract}}
%%% Zusammenfassung:

Argumentation is essential to forming opinions and making decisions. Since people tend to make decisions based on their instinct, it is helpful to be confronted with various arguments in order to make a more rational decision \cite{klein2017sources}. An argument recommender system provides arguments about a topic that are relevant for a specific user. In order to do that, it needs to be able to predict the rating of that user for unseen arguments. Thus, argument recommender systems could be a useful tool for decision making. By presenting arguments that run counter to ones own convictions, the widespread problem of confirmation bias, as defined in \cite{nickerson1998confirmation}, could be addressed. 

%Add which models were used
In this bachelor thesis several recommender algorithms are implemented and evaluated on an argument rating data set provided by Heinrich Heine Universities deliberate application \cite{brenneis2020deliberate}. These algorithms are:
\begin{itemize}
    \item Two - level matrix factorization that utilizes linguistic information of arguments
    \item Matrix factorization using linguistic similarity scores produced by the \acrfull{bert} model
    \item Autoencoder neural network
    \item Probabilistic Naive Bayes approach
    \item User - neighborhood model
\end{itemize}
The models are evaluated on two tasks:
\begin{itemize}
    \item Predicting a users conviction by an argument (binary classification) - \acrfull{poc}
    \item Predicting the strength of the conviction for an argument (multiclass classification in the range $[0,6]$) - \acrfull{pow}
\end{itemize}

The goal is to improve upon the performance of two baseline recommender algorithms that are provided along with the dataset.
The provided baseline metrics accuracy and \acrfull{rmse} are discussed in terms of suitability for measuring performance of classification models on imbalanced data sets.\\
The implemented algorithms are trained on the provided training data set and optimized upon the validation data set. Both, the proposed algorithms as well as the baseline algorithms are evaluated on the baseline metrics as well as on four proposed metrics that take into account the class distribution within the rating data set to address the imbalance of the existing classes. These four metrics are the F - Score, G - Mean, Precision \& Recall. 

%Key findings
While using the baseline metrics the baseline algorithms delivered the best performance for some data set / task combinations. This was no longer the case when any of the four proposed metrics were used, indicating that the baseline metrics are not reasonable to use for the provided data set as the ratings within single items are imbalanced. Although it can be observed that the proposed models outperform the baseline models in all evaluations, there is no model that outperforms every other model on the given data set / task combinations. Worth mentioning is that the probabilistic Naive Bayes model achieves the best performance in 43.75\% of the evaluated data set / task combinations. 

%Conclusion and Results
As the best performing model changes regarding the tasks, there is no silver bullet for choosing a model solely regarding its performance. 
Since the Naive Bayes model is the least computational complex model of all the models that are presented, it shows that complexity does not necessarily correlate positively with performance improvements for this task and data set.
The results also show that the thorough assessment of a metric regarding its capability to evaluate the performance of a model is crucial to producing meaningful results.

%Maybe add comment that models that perform good on a dataset solely due to its structure and distribution will tend to perform bad once the distribution changes to a more complex one.