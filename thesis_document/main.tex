\listoffigures
\listoftables
\addcontentsline{toc}{section}{Acronyms}
\printglossary[type=\acronymtype, nonumberlist, style=alttree, nogroupskip=true]

\section{Introduction}
\label{chap:intro}
We are living in a world of ever - increasing information growth. The annual global datasphere was estimated to be 33 Zetabytes in 2018 \cite{rydning2018digitization}. The \acrfull{idc} forecasts the global datasphere to grow to 175 Zetabytes by 2025. Figure \ref{fig:datasphere_growth} shows the forecasted development of the global datasphere size. A large part of this data can be retrieved by users. The problem that this data - growth causes is the following: finding relevant information for yourself in this increasing amount of data \cite{burke2011recommender}.\\Recommender systems help in solving exactly this problem. The goal is to filter the relevant information for a specific user, making the amount of the provided information manageable.
Nowadays, recommender systems are used in a large number of areas, ranging from e - commerce to music \cite{kumar2021recommender}. Because of their widespread use, recommender systems have a major impact on polarization within a society \cite{stray2021designing}.
An important factor that accelerated the widespread use of recommender systems is the ease with which users can give feedback on items in the form of ratings on web - applications \cite{aggarwal2016recommender}.\\
One area in which recommender systems are less well established is online argumentation. Online argumentation is a domain where the users opinion is influenced strongly by the specific arguments he faces. Problems such as filter bubbles, as introduced in \cite{pariser2011filter}, or the provision of irrelevant arguments can be mitigated by using a recommender system
which provides suitable arguments to the user.
In this thesis several recommendation algorithms are implemented: a two - level matrix factorization in two variants, a probabilistic Naive Bayes approach, a neural network autoencoder as well as a user neighborhood approach. The goal is to outperform two baseline algorithms on an argument rating dataset that was provided alongside with the baseline algorithms from Heinrich Heine Universities deliberate application \cite{brenneis2020deliberate}.\\
The first task is to predict a users conviction by an argument, called \acrfull{poc}. The second task is to predict the strength of the conviction for an argument, called \acrfull{pow}.\\
The contributions of this thesis are summarized as follows:
\begin{itemize}
    \item Discuss the use of accuracy and \acrshort{rmse} as metrics for classification models that make use of imbalanced data.
    \item Propose use of metrics that take into account the class distribution.
    \item Implement recommender algorithms including matrix factorization, Naive Bayes, autoencoder and user neighborhood models that outperform the baseline models on all task - / dataset combinations using the proposed metrics.
 \end{itemize}
The thesis is structured as follows: In section \ref{sec:background} terms and concepts that are necessary for understanding the thesis are presented and defined. In section \ref{sec:dataset} the provided data set is presented. In section \ref{sec:problem} the tasks of this thesis are defined. In section \ref{sec:models} the implemented classification models are presented. In the following section \ref{sec:results} the performance of the implemented models as well as the baseline models are presented and discussed. Finally, section \ref{sec:appendix} contains the hyperparameters that were used for parametrized models as well as plots regarding the distribution of ratings within single arguments.  

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/annual_datasphere_growth.jpg}
    \caption{Annual Global Datasphere Size, Estimated by \acrshort{idc}}
    \label{fig:datasphere_growth}
\end{figure}

\section{Related Work}
Various approaches and applications in the field of argument recommender systems have been developed in the past.

Chalaguine and Hunter \cite{chalaguine2021addressing} developed a chatbot in the context of the global COVID - 19 pandemic. Their goal was to persuade people who refuse or are hesitant to take a COVID - 19 vaccination to get vaccinated. This type of system is classified as a \acrfull{aps}. A study was carried out in which participants engaged in a dialogue with the developed chatbot. The chatbot was programmed to be reactive i.e. whenever a participant raised a concern about getting vaccinated, the chatbot provided a counter - argument. In order to be able to do that, the chatbot was able to parse and map the presented concern to one of the concerns in a argument graph that was connected to counter - arguments. However, as the argumentation graph used by the chatbot is identical for every user, a personalized recommender model like the ones presented in this thesis could potentially lead to a more individual persuasion of the user. While this thesis focuses on predicting the numerically encoded conviction of users by arguments, their \acrshort{aps} model is a downstream application of this thesis.

Donadello et al. \cite{donadello2021machine} developed two \acrshort{aps} models that estimate the utility of an argument for the persuader as well as for the persuadee, using decision trees. As a formal basis the \acrfull{bpdt} \cite{hadoux2018biparty} was used to create an argumentation tactic that is optimal with respect to the utility of the persuader as well as for the utility of the persuadee. The utility functions are calculated for clusters of persuadees. Questions needed to be answered by the users prior to the dialog in order to find the initial clusters and to assign new users to an existing cluster. While the proposed models in this paper take into account both the utility of arguments for the persuader as well as for the persuadee, the models implemented in this thesis are solely optimized upon user - centric objective functions. While the attitude of the persuadees towards a topic needs to be obtained by asking questions, the provided data set within this thesis already contains these attitudes. 

Hadoux et al. \cite{hadoux2021strategic} also developed an \acrshort{aps} that incorporates user profiles by modelling their concerns and beliefs. Beliefs of a user were used as an indicator for argument acceptance, while concerns were used to create argument clusters by mapping an argument to a concern. They evaluated the change of attitudes of participants that engaged with their developed \acrshort{aps} and with a baseline \acrshort{aps} which did not create user profiles. The results showed that the developed \acrshort{aps} led to statistically significant changes in the attitude of the participants while the non - personalized baseline \acrshort{aps} did not. Similar to this thesis, user profiles were used to provide personalized arguments to users within a certain concern - cluster. However, no dataset with previous ratings of the users is available. Therefore the authors rely on a purely memory - based \cite{aggarwal2016recommender} approach regarding the arguments, while memory - based approaches as well as model - based \cite{aggarwal2016recommender} approaches are presented in this thesis, which is possible due to the available rating data set.

\section{Background}
\label{sec:background}
In this section the terms and concepts that are necessary for understanding the thesis will be defined. 

\subsection{Recommender System}
\label{subsec:rec_sys}
A recommender system is an information retrieval system that aims to predict the rating of a user for a given item based on historical rating data and / or item attributes \cite{aggarwal2016recommender}.
The historical data can either be implicit i.e. users in the system do not explicitly give ratings to items (e.g. clicks) or the data
can be explicit i.e. users in the system provide explicit ratings for items \cite{beel2016paper}.\\
LÃ¼ et al. \cite{lu2012recommender} make a basic distinction between the following three types of recommender systems :
\begin{enumerate}
    \item Content - based recommender systems 
    \item \acrfull{cf} recommender systems
    \begin{enumerate}
        \item Model - based \acrshort{cf}
        \item Memory - based \acrshort{cf}
    \end{enumerate}
    \item Hybrid recommender systems
\end{enumerate}

\noindent Content - based recommender systems exploit the attributes of items to make their recommendations.
This means that items are interpreted as similar if their attribute - values are similar. 
In content - based recommender systems the item - similarity is combined with the historical user - data to produce recommendations.
For this purpose a similarity - matrix ${A} \in \mathbb{R}^{n \times n}$ (with $n$ being the number of items) is calculalated to find
similar items. The assumption that a content - based recommender system makes is that the ratings of one specific user strongly
correlate for similar items \cite{aggarwal2016recommender}. 

\noindent Collaborative filtering recommender systems exploit the similarity of users to make their recommendations. Only
implicit or explicit feedback provided by the users is used in this case. The goal is to find clusters of similar users i.e.
users whose rating - behaviour is similar. The assumption that a \acrshort{cf} recommender system makes is that
unspecified ratings can be imputed because the ratings of similar users for the same items strongly correlate \cite{aggarwal2016recommender}.
Model - based \acrshort{cf} methods and memory - based \acrshort{cf} methods differ in the way predictions are created. Model - based \acrshort{cf} methods create a model that represents the data and is then utilized to produce a prediction. Memory - based \acrshort{cf} methods, on the other hand, produce predictions by utilizing the data from the data set for each specific prediction \cite{aggarwal2016recommender}. 

\noindent Hybrid recommender systems use both content - based and \acrshort{cf} techniques \cite{lu2012recommender}. 

\subsection{Cosine Similarity}
The cosine similarity, which is defined in \cite{xia2015learning}, is a commonly used metric to determine the similarity of two vector representations. The cosine similarity metric calculates the cosine of the angle between two vectors. Thus, the range of the cosine similarity is $[-1, 1]$.\\
It is defined as the normalized dot - product of two equidimensional vectors $A, B \in \mathbb{R}^{n}$ \cite{xia2015learning}:
\begin{center}
\begin{equation}
    \text{cos}(A, B) = \frac{A.B}{\|A\|\|B\|} = \frac{\sum_{i=1}^{n}A_i \cdot B_i}{\sqrt{\sum_{i=1}^{n}A_i^{2}} \cdot \sqrt{\sum_{i=1}^{n}B_i^{2}}}
\end{equation}
\end{center}
with $\|A\|$ being the Euclidean norm of $A$. For $\|A\| = \|B\| = 1$ the cosine similarity is equal to the dot product. Due to the normalization factor $\|A\|\|B\|$ the cosine similarity is not susceptible to the magnitude of the vectors.

\subsection{Matrix Factorization}
Koren et al. \cite{koren2009matrix} define a matrix factorization as a decomposition of a matrix ${A} \in {\mathbb{R}}^{n \times m}$ into the product ${B} \text{\mathbf{C}}^\top$ of two lower - dimensional matrices $B \in\mathbb{R}^{n \times d}$ and $C \in \mathbb{R}^{m \times d}$:
\begin{center}
\begin{equation}
A \approx BC^\top
\end{equation}
\end{center}
In the context of recommender systems each cell $c_i_j$ of matrix $A$ represents the feedback of user $u_i$ for item $i_j$.
Each row of the lower - dimensional matrix $B$ is the $d$ - dimensional representation of a user. Likewise, each row of the lower - dimensional matrix $C$ is the $d$ - dimensional representation of an item.
The higher the dimension $d$, the more latent features of an item are found that a user can either like or dislike \cite{jannach2010recommender}. As matrix $A$ is likely to be sparse in the context of recommender systems, matrix factorization is used to impute the missing values in $A$ by the approximation $BC^\top$.  
The concept is illustrated in Figure \ref{fig:mf_schema}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/matrix_factorization}
    \caption{Matrix Factorization Schema With Embedding Dimension d = 2}
    \label{fig:mf_schema}
\end{figure}

\subsection{Perceptron}
The perceptron is a mathematical model that performs a dot - product of $n$ inputs $x_1, .., x_n \in \mathbb{R}$, $n$ weights $w_1, .., w_n \in \mathbb{R}$ and a bias term $b \in \mathbb{R}$ \cite{rosenblatt1958perceptron}. The result of the dot - product is then fed into an activation - function whose output decides if this perceptron is activated or not, taking on a binary value. This activation function was originally the Heaviside step function, which is defined as \cite{abramowitz1964handbook}:\\
\begin{equation}
        H(x) = \left\{
	\begin{array}{ll}
		\text{1} & \mbox{if } x \geq 0\\
		\text{0} & \mbox{if } x < 0 \\
	\end{array}
\right.
    \end{equation}
The perceptron model is illustrated in Figure \ref{fig:perceptron}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.12]{images/perceptron.jpg}
    \caption{Perceptron Schema With $n=2$}
    \label{fig:perceptron}
\end{figure}

\subsection{Neural Network}
Wang \cite{wang2003artificial} defines a neural network, also called a multi - layer perceptron, as a universal function approximator.
Neural networks are organized into groups of units called layers. The most common neural network architectures arrange these layers in a chained structure. Within this structure each layer is a function of the preceding layer \cite{zou2008overview}.
As a single perceptron can only model linear functions due to the linear nature of the dot - product, non - linear activation functions are applied to the output of each perceptron in the hidden layers. This way, a neural network is able to approximate any mathematical function, not only linear functions \cite{zou2008overview}. In fully connected neural networks there is a weight specified for each neuron - pair combination of two adjacent layers. All these weights compose the learnable parameters $\theta$, whose optimal values are iteratively updated by the backpropagation algorithm in an unsupervised manner \cite{rumelhart1986learning}. A fully connected neural network is illustrated in Figure \ref{fig:nn}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/nn.jpg}
    \caption{Fully Connected Neural Network With $3$ Layers.}
    \label{fig:nn}
\end{figure}

\subsection{Autoencoder}
\label{chap:autoencoder}
Ng \cite{ng2011sparse} defines an autoencoder as a type of neural network that learns to approximate its inputs by applying the backpropagation algorithm. Instead of just learning the identity function to map the input to the output, the goal is to learn a low - dimensional representation of the input that is used to reconstruct it.
Bank et al. \cite{bank2021autoencoders} formally define the objective function of an autoencoder model as follows:
\begin{equation}
    \min_{A,B}E[\Delta(x, B \circ A(x)]
\end{equation}
with A being the encoder function $A: \mathbb{R}^{n} \to \mathbb{R}^{p}$, $B$ being the decoder function $B: \mathbb{B}^{p} \to \mathbb{R}^{n}$, $E$ being the expectation over the distribution of input $x$ and $\Delta$ being some sort of loss - function that measures the distance between input x and the reconstruction $B \circ A(x)$.

\subsection{Transformer}
The transformer model is composed of an encoder - decoder architecture and fully connected neural networks that are combined with the concept of self - attention \cite{vaswani2017attention}. The introduction of the transformer model led to the development of the transformer - based language model \acrfull{bert} by Google \cite{devlin2018bert}. \acrshort{bert} reached new state - of - the - art performances on a number of \acrfull{nlp} tasks and its performance is often used as a benchmark to evaluate new \acrshort{nlp} models .

\subsection{\acrfull{tfidf}}
In information retrieval the \acrshort{tfidf} - metric is used to compute the importance of a term in a corpus of texts w.r.t. to a search query in order to retrieve the most relevant documents for this query \cite{jones1972statistical}.\\
The \acrshort{tfidf} is defined as
\begin{center}
\begin{equation}
    \text{\acrshort{tfidf}(t, d, D)} = \text{tf(t,d)} \cdot \text{idf(t,D)}
\end{equation}
\end{center}
where tf is the term - frequency and idf the inverse document - frequency.\\
t is a specific term, d is a specific document with $d \in D$ where $D$ is the set of all documents within the corpus.\\
Let 
\begin{equation}
    \sum\limits_{t \in d}f_{t,_d} = \text{total}(d)
\end{equation} 
$f_t,_d$ calculates the number of times the term $t$ occurs in $d$.\\
The term - frequency is defined as 
\begin{center}
\begin{equation}
    \text{tf(t,d)} = \frac{f_t,_d}{\text{total}(d)}
\end{equation}
\end{center}
\noindent The inverse document - frequency is defined as:
\begin{center}
\begin{equation}
    \text{idf}(t,D) = \text{log}(\frac{|D|}{\sum\limits_{d \in D}\mathbbm{1_{d}}(t)}}})
\end{equation}
\end{center}
where $\mathbbm{1_{d}}(t)$ is the \textit{indicator function} of term $t$ being in document $d$. 
\subsection{Classification Metrics for Imbalanced Data Sets}
\label{sec:metrics}
One of the most frequently used metrics when evaluating a binary classification problem is the accuracy metric \cite{tharwat2020classification}. It is defined as the ratio of correctly classified instances to the number of all instances. In binary classifications the two class labels are called majority class and minority class with the majority class being the more frequent class \cite{hossin2015review}. To define accuracy the following concepts are needed
\begin{itemize}
    \item \acrfull{tp}: Minority class is correctly predicted.
    \item \acrfull{fp}: Minority class is wrongly predicted.
    \item \acrfull{tn}: Majority class is correctly predicted.
    \item \acrfull{fn}: Majority class is wrongly predicted.
\end{itemize}
These metrics are well - established to characterize and evaluate a binary classification \cite{tharwat2020classification}. \\
The accuracy is defined as:
\begin{equation}
    \text{Accuracy} = \frac{\acrshort{tp} + \acrshort{tn}}{\acrshort{fp} + \acrshort{fn} + \acrshort{tp} + \acrshort{fn}}
\end{equation}\\
The accuracy metric gives more weight to the more frequently represented labels in the data set and its score can therefore easily be misleading. A majority classifier evaluated on a data set with a label distribution of 5 percent minority class labels and 95 percent majority class labels will for example reach 95 percent accuracy.\\
Thus, in the case of imbalanced data sets, it is not appropriate to use the accuracy metric as it does not provide information about the predictive power of the model in this context.\\
Four metrics that do take the label distribution into account which are mentioned in the literature are the \texit{F - Score}, the \texit{G - Mean}, \texit{Recall} and \texit{Precision} \cite{he2009learning}.\\
In the following the minority class will be the positive class and the majority class will be the negative class.\\ 
Too define the F - Score and the G - Mean, Recall and Precision need to be defined:\\
\begin{equation}
    \text{Recall} = \frac{\acrshort{tp}}{\acrshort{tp} + \acrshort{fn}}
\end{equation}
\begin{equation}
    \text{Precision} = \frac{\acrshort{tp}}{\acrshort{tp} + \acrshort{fp}}
\end{equation}\\
Incorporating Recall and Precision, the F - Score is defined as:\\
\begin{equation}
    \text{F - Score} = \frac{(1 + \beta)^{2} \cdot \text{Recall} \cdot \text{Precision}}{\beta^{2} \cdot \text{Recall} + \text{Precision}}
\end{equation}
where $\beta$ is a parameter that controls the relative importance of precision versus recall. If precision and recall should be taken into account equally, then $\beta = 1$.\\
The G - Mean is defined as:\\
\begin{equation}
    \text{G - Mean} = \sqrt{\frac{\acrshort{tp}}{\acrshort{tp} + \acrshort{fn}} \cdot \frac{\acrshort{tn}}{\acrshort{tn} + \acrshort{fp}}}
\end{equation}
These four metrics are designed for binary classification problems where there are only two labels. In order to map these metrics to classification problems where the number of labels is greater than two, the following two approached can be applied. They will be illustrated for the F - Score metric:
\begin{enumerate}
    \item Taking the average F - Score per class $c$, where $c$ is the minority class and all other classes are the majority class. The set of all classes is denoted as $C$. The advantage is that all labels are treated as equally important:
    \begin{equation}
        \text{F - Score}_A = \frac{1}{|C|}\sum_{c \in C} \text{F - Score}_c
    \end{equation}
    \item Take the weighted average F - Score per class $c$, where $c$ is again the minority class and whose weight $w$ is its share of the total number of ratings:
    \begin{equation}
        \text{F - Score}_W = \sum_{c \in C} w_c \cdot \text{F - Score}_c , \;\;\sum_{c \in C} w_c = 1
    \end{equation}
    The disadvantage is that this gives again more weight to the more frequent labels. Thus, the first approach will be used later in the evaluation process.
\end{enumerate}
The G - Mean, Recall and Precision can be adapted in the same way to the multivariate classification problem with the number of labels $n > 2$. 

\section{Data Set}
\label{sec:dataset}
The data set consists of ratings of arguments on two different topics from German food policy: The prohibition of plastic packaging and the prohibition of genetic engineering \cite{brenneis2021will}. The ratings for these arguments were given by German online participants. The collection of data was conducted at three different time points: August 2020 ($T_1$), October 2020 ($T_2$) and December 2020 ($T_3$). 

\subsection{Subdivision of the Data Set}
\label{chap:subdivison}
The data set is divided into two different time transitions. The first one is defined as $T_1 \xrightarrow[]{} T_2$ and the second one is defined as $T_2 \xrightarrow[]{} T_3$.\\
The \acrshort{poc} and the \acrshort{pow} tasks are defined on both time transitions.
Each of these time transitions is split into train, validation and test data sets with a share of $60\%, 20\%, 20\%$.\\
The data sets can be further divided into the \acrshort{poc} and \acrshort{pow} task. The conviction columns are binary columns in $\{0,1\}$ and the weight columns take values from $\{0,1,2,3,4,5,6\}$. While the conviction columns indicate if a user is convinced by a given argument or not, the corresponding weight columns indicate how strong the users conviction regarding the specific argument is.

\subsection{Properties of the Data Set}
Different properties of the data set regarding the subdivisions described in \autoref{chap:subdivison} are presented in \autoref{tab:my_label}.

\begin{table}[h!]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|c|c|c|c|c|c|c|c}
    \hline
     & \multicolumn{2}{|c|}{Train} & \multicolumn{2}{|c|}{Validation} &
      \multicolumn{2}{|c|}{Test} \\
     \hline
     \text{Properties} &  
     $T_1 \to T_2$ & 
     $T_2 \to T_3$ &
     $T_1 \to T_2$ & 
     $T_2 \to T_3$ & 
     $T_1 \to T_2$ & 
     $T_2 \to T_3$\\
    \hline
    Instances & 672 & 553 & 144 & 116 & 145 & 113 \\
    \hline
    Sparsity & 90.01\% & 84.13\% & 89.48\% & 81.57\% & 89.46\% & 81.57\% \\
    \hline
    $\mu_c$ & 0.62 & 0.59 & 0.59 & 0.57 & 0.60 & 0.55 \\ 
    \hline
    $\mu_w$ & 3.68 & 3.52 & 3.50 & 3.41 & 3.50 & 3.38\\
    \hline
    $H_c$ & 0.96 & 0.97 & 0.97 & 0.98 & 0.96 & 0.99 \\
    \hline
    $H_w$ & 0.96 & 0.97 & 0.93 & 0.95 & 0.94 & 0.96 \\
    \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Properties of the Provided Data Set}
    \label{tab:my_label}
\end{table}

\noindent $\mu_c$ expresses the mean value of the conviction columns and $\mu_w$ expresses the mean value of the weight columns.\\
$H$ denotes the normalized Shannon Entropy and it is used in this case to measure the balance of a data set \cite{shannon1948mathematical}.\\
In this case $H_c$ is the normalized Shannon entropy for the \acrshort{poc} task and $H_w$ is the normalized Shannon Entropy for the \acrshort{pow} task.\\
Given a data set of $N$ instances, $C$ different columns and $M$ possible discrete values that a (user, item) pair can take, the normalized \texit{Shannon Entropy} is defined as follows:
\begin{equation}
    H = \frac{-\sum\limits_{i=1}^{M}\frac{M_i}{N \cdot C} \cdot \text{log} \frac{M_i}{N \cdot c}}{\text{log} M}
\end{equation}

\noindent $M_i$ denotes the number of ratings that were rated with the discrete rating value $i$.\\
\noindent $H=0$ indicates that the data set is completely unbalanced i.e. only one rating is given to each (user,item) pair. $H=1$ indicates that the rating - values are distributed uniformly in terms of occurrences.\\
From \autoref{tab:my_label} it can be observed that the data sets overall are balanced.  Additional to the overall balance of the data set it is of great interest for the modelling as well as for the evaluation to know if the distribution of ratings within a single argument itself is also balanced. Subsection \ref{sec:plots} provides plots as an overview of the rating distribution within single arguments. The ratings for the \acrshort{pow} task are grouped for reasons of clarity.\\
It can be seen from these plots that the distribution of the ratings within single items is highly skewed. As both baseline algorithms rely on the distribution of ratings within specific items, accuracy (\acrshort{poc}) as well as \acrfull{rmse} (\acrshort{pow}) is not a reasonable metric to use in this context. Both metrics do not provide a reliable information about the classifiers performance with regard to the properties of the data set. Therefore, metrics that take into account the performance on each discrete rating class are more sensible in the context of imbalanced data.\\
In the literature, four of such proposed metrics are the \textit{F - Score}, \texit{G - Mean}, \texit{Precision} and \texit{Recall} \cite{he2009learning}. All metrics are defined in subsection \ref{sec:metrics} and will be used for evaluation of the different models on the different task - / data set - combinations additionally to the provided metrics.

\section{Problem Statement}
\label{sec:problem}
Two tasks are defined on the provided data set, the \acrshort{poc} - task and the \acrshort{pow} - task. Within both tasks the challenge is to predict a rating for a user - argument combination in timepoint $T_{a+1}$ with the knowledge of the ratings in timepoint $T_a$.\\ Let $x \in \{0,1,2,3,4,5,6\}^{n}$ denote a partially observed rating - vector of the rating matrix $R \in \mathbb{R}^{m \times n}$, with $m$ being the number of users and $n$ being the number of arguments.
\subsection{Prediction of Conviction (\acrshort{poc})}
The \acrshort{poc} task is defined as follows: given a user, argument pair $(u,i) \in R$, predict the unknown conviction of user $u$ for argument $i$. As this task is a binary classification task, the range of $x$ is limited to $\{0, 1\}$.\\
\subsection{Prediction of Weight (\acrshort{pow})}
The \acrshort{pow} task is defined as follows: given a user, argument pair $(u,i) \in R$, predict the unknown weight of the conviction of user $u$ for argument $i$. This task is a multi classification task, the range of $x$ is $\{0, 1, 2, 3, 4, 5, 6\}$.\\
\subsection{Difficulty of the Tasks}
\label{sec:difficulty}
The difficulty of these tasks is given by the fact that for each user from the test data set in $T_{a+1}$  only two ratings are given in the training data set in $T_{a}$ for each of the respective tasks. Since very little data is available about the relevant users, the available rating information of other users needs to be used to impute the missing values. This setup is defined as a \acrshort{cf} task, as described in \ref{subsec:rec_sys}.\\
\subsection{Generalization of Classification}
Aggarwal \cite{aggarwal2016recommender} interprets CF as a generalization of a classification or regression problem. Traditional classification and regression problems deal with observations $x \in \mathbb{R}^{m}$, where $m - 1$ variables are specified, the independent variables. However, one variable is unspecified, the dependent variable. The value of the dependent variable needs to be predicted. The independent and dependent variables stay the same throughout all observations. In CF, on the other hand, any variable can be the dependent variable. In addition, more than one of the independent variables may be unspecified. Furthermore, no row - wise split into training data set and test data set can be performed in most cases as any row is likely to contain missing values. The relationship between CF and traditional classification modelling is illustrated in Figure \ref{fig:CFvsClassification}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/CFvsClassification2.jpg}
    \caption{Comparison of the Traditional Classification Problem With CF \cite{aggarwal2016recommender}}
    \label{fig:CFvsClassification}
\end{figure}

\section{Classification Models}
\label{sec:models}
The different approaches that were applied to the classification problem within the provided rating matrix data sets are presented in this section. 

\subsection{\acrfull{tlmf}}
In the following the concept as well as implementation of the \acrshort{tlmf} will be presented.
The \acrshort{tlmf} is a two - step latent factor model which applies two sequential matrix factorizations 
to find the two optimal lower - level matrices whose product approximates the original rating - matrix \cite{li2016two}.
The first matrix factorization computes a similarity matrix of text - entities.
In the context of this thesis these text - entities are arguments. This similarity matrix is then utilized in the second
matrix factorization to include the argument - similarity into the rating prediction additionally to the user - rating information.

\subsubsection{\acrfull{wtmf}}
The first step of the \acrshort{tlmf} is the \acrshort{wtmf} \cite{guo2012weiwei}, an unsupervised learning algorithm that has been successfully applied to many natural language processing tasks such as sentence similarity computation \cite{guo2012modeling}. 
The first step in \acrshort{wtmf} is to normalize the arguments, i. e. lemmatizing and lowercasing.
Then a \acrshort{tfidf} - vector is calculated for each normalized argument.
The length of each \acrshort{tfidf} - vector is $|V|$, with $V$ being the vocabulary over all arguments.
In the following, the vocabulary $V$ of a text corpus will be defined as:
    \begin{equation}
        V = \{V_1 \cup V_2 \cup .. \cup V_n\}    
    \end{equation}
with $V_1, V_2, .., V_n$ being the set of words in the texts $T_1, .., T_n$.\\
For every argument in the corpus, the \acrshort{tfidf} - value for every word in $V$ is calculated. 
The resulting matrix $X$ is called a term - document - matrix. It contains \acrshort{tfidf} - values of words in each cell. Each argument is represented by a column vector of \acrshort{tfidf} - values for each word in $V$.\\
In the next step $X$ gets approximated by a matrix factorization in order to find lower - dimensional representations of the arguments. $X$ is decomposed into two lower - dimensional matrices $M \in \mathbb{R}^{n \times k}$ and $N \in \mathbb{R}^{m \times k}$.\\ $k$ is the embedding dimension of the sentences and a parameter of the model.
Since every argument in $X$ is represented in the dimension of $|V|$, many entries in $X$ will be $0$. To account for that fact, a weight - matrix $W  \in \mathbb{R}^{|V| \times |V|}$ 
is introduced to control the influence of cells $x = 0$.\\
The two model parameter matrices $N$ and $M$ are optimized by minimizing the following objective function:
\begin{equation}
\begin{align*}
    L_W = \min_{M,N} \frac{1}{2} W_i_j (X_i_j - M_iN_j^\top) + \frac{\gamma}{2}(\|M\|^2_F + \|N\|^2_F) \\
    W_i_j = \left\{
	\begin{array}{ll}
		1  & \mbox{if } x_i_j \neq 0 \\
		w_m & \mbox{if } x_i_j = 0, w_m \in [0,1]
	\end{array}
\right.
\end{align*}
\end{equation}
In the above objective function $\gamma$ is a free regularization factor that is adjusted to prevent the model from overfitting. $\|\cdot\|^2_F$ is the Frobenius Norm.\\

\noindent In the following the training process of the \acrshort{wtmf} - model will be described.\\
The model is trained by first initializing the matrices $N$ and $M$ randomly and then updating $N_i$ and $M_j$ iteratively by applying the following equations for every entry $x_i_j \in X$:
    \begin{equation}
        N_i = (M\tilde{W}^{(i)}M^\top + \gamma\mathbb{I})^{-1}B\tilde{W}^{(i)}X^\top_i
    \end{equation}
    \begin{equation}
        M_j =(N\tilde{W}^{(j)}N^\top + \gamma\mathbb{I})^{-1}A\tilde{W}^{(j)}X^\top_j 
    \end{equation}
In the above equations:
\begin{itemize}
    \item $\mathbb{I} \in \mathbb{R}^{k \times k}$  is the \textit{Identity Matrix}
    \item $\tilde{W}^{(i)} \in \mathbb{R}^{n \times n} = \text{diag}(W.,i)$ is a diagonal matrix containing the i - th row of $W$.
    \item $\tilde{W}^{(j)} \in \mathbb{R}^{m \times m} = \text{diag}(W.,j)$ is a diagonal matrix containing the j - th column of $W$.
\end{itemize}
After the training process, the final latent factor matrices $N$ and $M$ have been calculated, such that their product approximates $X$.
$M_j$ now represents the $k$ - dimensional embedding of argument $A_j$.\\
The similarity $Sim(A_i, A_j)$ of two arguments $A_i, A_j$ is now defined as:
    \begin{equation}
        \text{Sim}(A_i, A_j) = \text{cos}(A_i, A_j) = \frac{A_i.A_j}{\|A_i\|\|A_j\|} = \frac{\sum_{i=1}^{n}A_i \cdot B_i}{\sqrt{\sum_{i=1}^{n}A_i^{2}} \cdot \sqrt{\sum_{i=1}^{n}B_i^{2}}}
    \end{equation}
The pairwise similarity - matrix $S \in \mathbb{R}^{m \times m}$ is defined as:
    \begin{equation}
        S_i_j = \left\{
	\begin{array}{ll}
		\text{Sim}(A_i, A_j)  & \mbox{if } i \neq j\\
		0 & \mbox{if } i=j \\
	\end{array}
\right.
    \end{equation}
The original similarity value of $1$ for $Sim(A_i, A_j)$ with $i=j$ is replaced with $0$ to force the \acrshort{tlmf} model later on to only take into account arguments that differ from the argument at hand when making predictions.\\
$S$ is symmetric, i.e. $\forall{s_i_j} \in S: s_i_j = s_j_i$. Thus, only the entries of an upper triangular matrix need be computed.

\subsubsection{Combining Ratings \& Similarity Matrix}
\label{subsub:tlmf}
The second level of the \acrshort{tlmf} model attempts to combine the rating matrix with the textual similarity matrix from the first level into a unified learning model that tries to approximate the given user - argument ratings in the provided data set by applying another matrix factorization.\\
The given rating matrix $Z \in \mathbb{R}^{u \times i}$ is decomposed into two low - dimensional matrices
$Q \in \mathbb{R}^{u\times d}$ and $P \in \mathbb{R}^{i \times d}$. $d$ is the embedding size of the model i.e. each user and each item is represented as an $d$ - dimensional vector.\\
To optimize the model parameters $P, Q$ the regularized squared error is minimized:
\begin{equation}
\begin{align*}
    L_T = \min_{P, Q} \frac{1}{2}\sum\limits_{(u,i) \in K}(Z_u_i - P_uQ_i^\top)^2 + \frac{\lambda}{2}(\|Q\|^2_F + \|P\|^2_F) + \\
    \frac{\alpha}{2}\sum\limits_{i}(q_i - \sum\limits_{j}S_i_j q_j)(q_i - \sum\limits_{j}S_i_j q_j)^\top
\end{algin*}
\end{equation}
The second summand in the above equation is the first regularization. The regularization factor $\lambda$ controls the overfitting of the model to the rating - data.\\
The third summand is the semantic regularization factor. It controls that the argument profiles (i.e. the ratings they get) of similar arguments should be similar. The regularization factor $\alpha$ controls the influence that the semantic relations of the arguments have on the prediction of the model.\\
The set $K$ contains all index - tuples $(u,i)$ of the rating matrix whose value is available.\\
For each $z_u_i \in Z$, $P_u$ and $Q_i$ are updated by the following gradient descent approach:
\begin{equation}
    P_u = P_u + \eta((Z_u_i - P_uQ_i^\top)Q_i - \lambda P_u))
\end{equation}
\begin{equation}
\begin{align*}
    Q_i = Q_i + \eta((Z_u_i - P_uQ_i^\top)P_u - \lambda Q_i) - \\
    \alpha(Q_i - \sum\limits_{j \in \mathbb{N}(i)}S_i_jQ_j) + \\
    \alpha\sum\limits_{j}(S_j_i)(Q_j - \sum\limits_{k \in \mathbb{N}(j)}S_j_k Q_k))
\end{align*}
\end{equation}
In the above equation:
\begin{itemize}
    \item $\eta$ is the \textit{learning rate}. 
    \item $N(i)$ is the \textit{Neighborhood} of argument i. The parameter \textit{n} that is passed to the \acrshort{tlmf} training algorithm determines the number of neighbours (similar arguments) to consider.
\end{itemize}
This training procedure is applied until convergence. Afterwards an unknown value $z_u_i$ can efficiently be predicted by calculating the dot - product P_uQ_i^\top.  

\subsection{\acrshort{bert} + Matrix Factorization}
The similarity matrix of arguments was built by the \acrshort{wtmf} model in the \acrshort{tlmf} model. In this model the similarity matrix is computed by applying a pretrained \acrshort{bert} model to the arguments. Specifically, the \acrshort{bert} - Base model with 12 transformer blocks, 768 hidden layers and 12 self - attention heads is used \cite{devlin2018bert}. This \acrshort{bert} model maps an input sequence to a $768$ - dimensional, dense vector. This mapping is done in such a way that semantically similar sequences are close to each other in the embedding space. The pairwise argument similarity is then computed as the cosine similarity of the normalized argument embeddings.\\
The second step of this model is equal to the second step in the \acrshort{tlmf} model i.e. the computed similarity matrix is incorporated into the matrix factorization model proposed in section \ref{subsub:tlmf}. 

\subsection{AutoRec}
Sedhain et al. \cite{sedhain2015autorec} introduced as an autoencoder framework specifically designed for \acrshort{cf} tasks.
The before described matrix factorization model is a linear latent model, i.e. the complexity of the relationships it is able to model is limited to linear complexity. The goal of implementing the AutoRec model is to model possible non - linear relationships within the rating data and to improve upon the limited predicition power of the matrix factorization model.\\
AutoRec follows the basic architecture of an autoencoder, which is described in section \ref{chap:autoencoder}. AutoRec can either be implemented as an \acrfull{iautorec} or as a \acrfull{uautorec}. In the experiments that were conducted in \cite{sedhain2015autorec}, the item - based AutoRec yielded better results compared to the user - based AutoRec. It is assumed that this performance difference is caused by the fact that item - vectors are likely to be less sparse than user - vectors in the recommender system scenario.\\
In the following the implementation of \acrshort{iautorec} will be described.\\
Let $x$ denote a partially observed item - vector. The goal of AutoRec is to compute a mapping of the input $x$ into a low - dimensional space from which $x$ can be approximated. Let the reconstruction of the item - vector $x$ be denoted as $h(x;\theta)$. $\theta$ denotes all the learnable parameters of the AutoRec model. The values for the parameters $\theta$ are learned by using backpropagation. The hidden layer size $k \in \mathbb{N}$ is the non - learnable parameter of the AutoRec model. The objective function that is optimized upon is given by:
\begin{equation}
    \min_{\theta}\sum_{x \in I} ||x - h(x; \theta)||^{2}_{2}
\end{equation}
where $||x||_{2}^{2}$ denotes the squared Euclidean Norm of a vector $x$.\\
The reconstruction function $h(x, \theta)$ is defined as:\\
\begin{equation}
    h(x,\theta) = f(W \cdot g(Vx + \mu) + b)
\end{equation}
where\\
\begin{itemize}
    \item $f$ and $g$ are activation functions
    \item $W$ and $V$ are weight matrices
    \item $x$ is the (masked) item vector
    \item $\mu$ and $b$ are bias terms
\end{itemize}
$f, g, W$ and $V$ are the learnable parameters $\theta$.\\
To avoid overfitting of the model on the training data, a regularisation term for the weighting matrices is added with $\lambda > 0$:\\
\begin{equation}
   \min_{\theta}\sum_{x \in I} ||x - h(x; \theta)||^{2}_{O} + \frac{\lambda}{2} \cdot (||W||^{2}_{2} + ||V||^{2}_{F})
\end{equation}
The $||\cdot||_{O}$ notation means that only the weights of observed ratings are updated in the AutoRec training procedure.\\
Because the input vector to the network needs to be dense, an appropriate encoding for missing values needs to be found. As the AutoRec model only updates the weights of the network that correspond to observed ratings in the input vector, the missing values can be set to an arbitrary value $\tilde{v} \not\in V$, with $V$ denoting the possible range of rating values. Instead of optimizing the squared distance of the original vector $x$ and its reconstruction $h(x; \theta)$, the \acrfull{mmse} will be optimized:\\
\begin{equation}
\begin{align*}
    \text{\acrshort{mmse}}(x, h(x;\theta)) = \sum_{u=0}^{d}\frac{\mathbbm{1_{\texit{V}}}(x_u) \cdot (x_u - h(x;\theta)_u)^{2}}
    {\sum_{u=0}^{d} \mathbbm{1_{\texrm{V}}}(x_u)} + \\ \frac{\lambda}{2} \cdot (||W||^{2}_{2} + ||V||^{2}_{F}) 
\end{align*}
\end{equation}
with $\mathbbm{1_{\texrm{V}}}(x_u)$ being the indicator function defined as:\\
\begin{equation}
    \mathbbm{1_{\texrm{V}}}(x_u) = \left\{
	\begin{array}{ll}
		1  & \mbox{if } x_u \in V\\
		0 & \mbox{if } x_u \not\in V \\
	\end{array}
\right.
\end{equation}
This results in the final objective function:\\
\begin{equation}
    \min_{\theta}\sum_{x \in I} \text{\acrshort{mmse}}(x, h(x;\theta)) + \frac{\lambda}{2} \cdot (||W||^{2}_{2} + ||V||^{2}_{F})
\end{equation}
The \acrshort{mmse} is used because the error of the AutoRec model should only be computed over known values in the input vector.\\
The architecture of the AutoRec model is illustrated in \autoref{fig:autorec_example} by a computational graph for an AutoRec instance with input dimension $d = 2$ and hidden layer parameter of size $k=1$. The weights are denoted by $w_{xy}$ where x indicates the layer in which the weight is located and y denotes to which input the weight belongs.\\
After learning the optimal values for the learnable parameters $\theta$, a prediction for the combination of user $u$ and item $x$ is produced by:\\
\begin{equation}
    R_u_x = h(x; \theta)_u
\end{equation}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/computational_graph.jpg}
    \caption{Computational Graph of AutoRec instance with input dimension $d=2$ and hidden layer size $k=1$}
    \label{fig:autorec_example}
\end{figure}

\subsection{Naive Bayes \acrshort{cf}}
The Naive Bayes classifier is a model family that utilizes \texit{Bayes Theorem} combined with an independence assumption of the features to produce a classification \cite{rish2001empirical}. A way of mapping This model to the \acrshort{cf} scenario is proposed in \cite{valdiviezo2019collaborative}. Bayes Theorem is defined in \cite{berrar2018bayes} as:\\
\begin{equation}
    P(A|B) = \frac{P(A)\cdot P(B|A)}{P(B)}
\end{equation}
In the above equation
\begin{itemize}
    \item $P(A)$ is called the \texit{prior probability}
    \item $P(B|A)$ is called the \texit{likelihood}.
    \item $P(B)$ is called the evidence.
    \item $P(A|B)$ is called the \texit{posterior probability}
\end{itemize}
Let there be $n$ different classes $c_1, c_2,\dots,c_n$ and $m$ features $f_1, f_2,\dots,f_m$.
Given a data vector $\Vec{f} \in \mathbb{R}^{m}$, the goal is to find the class $c$ that maximizes the following expression:\\
\begin{equation}
    \begin{align*}
        c^{*} = \arg\max_{c} P(c|\Vec{f})\\
        = \arg\max_{c} \frac{P(\Vec{f}|c)\cdot P(c)}{P(\Vec{f)}}\\
        \propto \arg\max_{c} P(\Vec{f}|c)\cdot P(c) \label{eq:1}  
    \end{align*}
\end{equation}
As the denominator in the above equation is independent on the class label $c$ for which the value needs to be found that maximizes the expression above, the equation is rewritten in form of a constant of proportionality.\\
Without the independence assumption the likelihood $P(\Vec{f}|c)$ would yield $m!$ factors.
With the independence assumption the joint probability can be written as the product of the single probabilities:
\begin{equation}
    \begin{align*}
        P(\Vec{f}|c) = P(f_1, f_2,\dots, f_m|c)\\
        = P(f_1|c)\cdot P(f_2|c) \dots P(f_m|c)
    \end{align*}
\end{equation}
The parts of the last expression of equation \eqref{eq:1} can be translated into the \acrshort{cf} scenario as follows:
\begin{equation}
    \begin{align*}
        \arg\max_{c} P(\Vec{f}|c)\cdot P(c) \;\widehat{=}\; \arg\max_{c} \prod\limits_{k \in I_u} P(r_{u_k}|r_{u_j}=c) \cdot P(r_{u_j} = c)
    \end{align*}
\end{equation}
The prior probability $P(r_{u_j} = c)$ is computed as the fraction of users who have specified the rating $c$ for item $j$ to all users that have specified any rating for item $j$.\\
The likelihood $\prod\limits_{k \in I_u} P(r_{u_k}|r_{u_j}=c)$ is based on the conditional independence assumption of the ratings.%Refactor this freaking sentence 
It is computed as the product of the fractions of users who have specified the rating $r_u_k$ for item $k$, given that they specified the rating for the $j$th item to $c$ to all users that have specified the rating for the $j$th item to c and have given a rating $c_k$ to item $k$ that is different from $r_{u_k}$ for every item $k \in I_u$. $I_u$ is the set of items the user $u$ has specified any rating for.\\
The final prediction will thus be:
\begin{equation}
    \begin{align*}
        \hat{r}_{u_j} = \arg\max_{c} \prod\limits_{k \in I_u} P(r_{u_k}|r_{u_j}=c) \cdot P(r_{u_j} = c)
    \end{align*}
\end{equation}

\subsection{User - Based Neighborhood Models}
The essence of user - based neighborhood models is to predict a rating for the target user - item pair $(u,i)$ based on a weighted rating of the $k$ most similar users $\tilde{u}_1, \dots ,\tilde{u}_k$ to the target user $u$ which made a prediction for item $i$ \cite{aggarwal2016recommender}. \\ 
A general issue in user - based neighborhood models is the fact that different users differ in their scale of ratings due to rating - biases \cite{wherry1982control}. Some users in general might give higher ratings whereas other users tend to give lower ratings. Approaches to tackle this problem will be proposed in the following.\\
Some definitions that are important for the following user - based neighborhood model before proceeding are presented now.
$I_u$ denotes the set of items rated by user $u$. Given a rating matrix $R \in \mathbb{R}^{m \times n}$ with $m$ users and $n$ items, $r_u_j$ denotes the rating of user $u$ for item $j$. $\mathbbm{1_{u_j}}$ indicates if user $u$ has rated item $j$ or not. The average rating of user $u$ is defined as folows:
    \begin{equation}
    \mu_u = \frac{\sum\limits_{i \in I_u}r_u_i}{|I_u|}
    \end{equation}

\subsubsection{Mean - Centered Pearson Correlation}
The similarity of two users $u_1$ and $u_2$ is computed exclusively on the mutual observed ratings defined as $I_u_1 \cap I_u_2$.\\
The training - data sets that are provided specify $4$ ratings ($2$ for \acrshort{poc}, $2$ for \acrshort{pow}) for users for which predictions need to be made in the test - data set. So in general for these users of interest $U$ it holds that $|I_u| = 4\: \forall{u \in U}$.\\
A measure that can be applied to compute the similarity of two users $u_1, u_2$ is the Pearson Correlation Coefficient \cite{benesty2009pearson}:
\begin{equation}
    \text{Pearson}(u_1, u_2) = \frac{\sum\limits_{i \in I_u_1 \cap I_u_2} (r_{u_1_i} - \mu_u_1) \cdot (r_{u_2_i} - \mu_u_2)}{\sqrt{\sum\limits_{i \in I_u_1 \cap I_u_2} (r_{u_1_i} - \mu_u_1)^2 \cdot \sum\limits_{i \in I_u_1 \cap I_u_2} (r_{u_2_i} - \mu_u_2)^2}}
\end{equation}
The Pearson Correlation Coefficient is computed for all user - pairs. Intuitively, the next step is to find the set $S$ of the $k$ users with the highest Pearson Correlation Coefficient to the target user to calculate a prediction.\\
As $R$ is sparse the number of mutually rated items by the users $u_1,\dots,u_k \in S$ is likely to change over different items. Therefore different sets of $S$ will be calculated for different items for the target user. It is also likely to happen that $|I_{u_1} \cap I_{u_2} \cap \dots \cap I_{u_k}| < I_{u_t}$ with $I_{u_t}$ being the target user and $u_1,\dots,u_k \in S$. In this case, the reduced number of neighbors will be used w.r.t. a similarity threshold $t$ that ensures that only ratings of users $u$ are considered in the prediction for which holds that $Pearson(u, u_t) >= t$.\\
The weighted average of these ratings will be used as the prediction for a specific item. The ratings will be weighted by the Pearson Correlation Coefficient of the owner to the target user.
Before actually producing predictions the problem of rating bias has to be tackled. To deal with rating biases the raw rating data is mean centered for every user:

\begin{equation}
    c_{u_j} = r_{u_j} - \mu_u
\end{equation}
Using the mean centered rating data, we can produce mean centered predictions for user item pairs $(u,j)$. To account for the centering, we add the mean $\mu_u$ of a specific user $u$ back to the prediction to produce a \textit{raw prediction}. Let $P_u(j,t)$ be the set of k - closest users to target user $u$ that specified a rating for item $j$ and for which $Pearson(u, v) >= t\;: \forall{v} \in P_u(j)$ holds.\\ The prediction function is defined as: 
\begin{equation}
    \hat{r}_{u_j} = \mu_u + \frac{\sum\limits_{v \in P_u(j)}\text{Pearson}(u, v) \cdot c_{v_j}}{\sum\limits_{v \in P_u(j)} |\text{Pearson}(u, v)|}
\end{equation}

\subsection{A Note on Item - Based Neighborhood Models}
In Item - Based Neighborhood Models the predictions of a target user - item pair $(u, i)$ is based on the weighted average of the $k$ most similar items $i_1,\dots,i_k$ the user $u$ has provided a rating for. As described in \ref{sec:difficulty}, only two items per task have a rating for the users in the training set for which a prediction needs to be made in the test set.\\
Therefore it is not feasible to apply an Item - Based Neighborhood Model in this scenario as there would only be two other ratings for which the corresponding items don't necessarily need to be similar.

\section{Results \& Discussion}
\label{sec:results}
In the following the performance of the proposed models on all data set -/task - combinations using each of the discussed metrics will be discussed. This will be done in \autoref{chap:eval}. In \autoref{chap:efficiency} the computational efficiency of the proposed models will be discussed. The discussion of the hyperparameters is presented in \autoref{chap:hyperparams}.

\subsection{Evaluation}
\label{chap:eval}
In the following the margins of performance differences are computed as the difference between the best baseline performance and the best non - baseline performance.\\ 

\noindent In Table \ref{tab:results} the performance results of the models using the provided metrics accuracy (\acrshort{poc}) and \acrshort{rmse} (\acrshort{pow}) are presented. It can be observed that for the tasks defined on $T_1 \to T_2$ the \acrfull{nn} - classifier achieves the best performance by a margin of 0.1\% (\acrshort{poc}) and 29.4\% (\acrshort{pow}). For the tasks defined on $T_2 \to T_3$ the \acrshort{iautorec} model achieves the best performance by a margin of 6.8\% (\acrshort{poc}) and 12.8\% (\acrshort{pow}) compared to the best baseline model. As discussed in \ref{chap:subdivison} accuracy as well as \acrshort{rmse} are not reasonable metrics to use on this data set as the distribution of ratings within single items is highly skewed.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & \multicolumn{2}{|c|}{\acrshort{poc} (Accuracy)} & \multicolumn{2}{|c|}{\acrshort{pow} (\acrshort{rmse})} \\
     \hline
    Algorithms & $T_1 \to T_2$ & $T_2 \to T_3$ & $T_1 \to T_2$ & $T_2 \to T_3$\\
    \hline  Majority (baseline) & \textbf{.783} & .737 & 3.26 & 3.52 \\
    \acrshort{nn} (baseline) & \textbf{.783} & .738 & \textbf{2.86} & 2.57\\
    \acrshort{tlmf} & .782 & .788 & 3.70 & 2.40 \\
    \acrshort{tlmf} + Bert & .780 & .785 & 3.70 & 2.41\\
    Naive Bayes & .782 & .802 & 5.96 & 2.99 \\
    I - AutoRec & .730 & \textbf{.806} & 4.13 & \textbf{2.24}\\
    User - Neighborhood (Pearson) & .593 & .617 & 7.53 & 4.71\\
    \hline
    \end{tabular}
    \caption{Results of the Models, Evaluated With the Provided Metrics.}
    \label{tab:results}
\end{table}
\noindent In Table \ref{tab:results_f1_mean} the performance results of the models using the averaged F - score are presented. It can be seen that the \acrshort{tlmf} model performs best on the \acrshort{poc} task for the $T_1 \to T_2$ data set by a margin of 3.2\%. For the respective \acrshort{pow} task the Naive Bayes model performs best by a margin of 15\%. Likewise, the Naive Bayes model shows the best performance for tasks defined on $T_2 \to T_3$. It achieves the best performance by a margin of 7.8\% (\acrshort{poc}) and 43.4\%, respectively.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & \multicolumn{2}{|c|}{\acrshort{poc} (F1 - Score)} & \multicolumn{2}{|c|}{\acrshort{pow} (F1 - Score)} \\
     \hline
    Algorithms & $T_1 \to T_2$ & $T_2 \to T_3$ & $T_1 \to T_2$ & $T_2 \to T_3$\\
    \hline  Majority (baseline) & .411 & .387 & .115 & .108 \\ 
    \acrshort{nn} (baseline) & .746 & .720 & .080 & .104\\
    \acrshort{tlmf} & \textbf{.778} & .784 & .191 & .481\\
    \acrshort{tlmf} + Bert & .777 & .780  & .191 & .481 \\
    Naive Bayes & .770 & \textbf{.798} & \textbf{.265} & \textbf{.561} \\
    I - AutoRec & .722 & .784 & .202 & .542\\
    User - Neighborhood (Pearson) & .542 & .602 & .188 & .108\\
    \hline
    \end{tabular}
    \caption{Results of the Models, Evaluated With the Average F1 â Score.}
    \label{tab:results_f1_mean}
\end{table}
\noindent In Table \ref{tab:results_precision_mean} the performance results of the models using the averaged precision score are presented. For data set $T_1 \to T_2$ the \acrshort{tlmf} model and the Naive Bayes model achieve the highest performance by a margin of 2.9\% (\acrshort{poc}) and 7.7\%, respectively. For data set $T_2 \to T_3$ the Naive Bayes model and the \acrshort{iautorec} model achieve the highest performance by a margin of 18.1\% and 48\%, respectively.
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & \multicolumn{2}{|c|}{\acrshort{poc} (Precision)} & \multicolumn{2}{|c|}{\acrshort{pow} (Precision)} \\
     \hline
    Algorithms & $T_1 \to T_2$ & $T_2 \to T_3$ & $T_1 \to T_2$ & $T_2 \to T_3$\\
    \hline  Majority (baseline) & .404 & .367 & .099 & .085 \\ 
    \acrshort{nn} (baseline) & .759 & .725 & .085 & .103\\
    \acrshort{tlmf} & \textbf{.788} & .778 & .236 & .531 \\
    \acrshort{tlmf} + Bert & .787 & .774 & .236 & .481\\
    Naive Bayes & .775 & \textbf{.802} & \textbf{.266} & .574 \\
    I - AutoRec & .731 & .783 & .260 & \textbf{.583}\\
    User - Neighborhood (Pearson) & .558 & .612 & .181 & .181\\
    \hline
    \end{tabular}
    \caption{Results of the Models, Evaluated With the Average Precision â Score.}
    \label{tab:results_precision_mean}
\end{table}\\
\noindent In Table \ref{tab:results_recall_mean} the performance results of the models using the averaged recall score are presented. For data set $T_1 \to T_2$ the \acrshort{tlmf} model performs best by a margin of 3.6\% (\acrshort{poc}) and 8.1\% (\acrshort{pow}). For data set $T_2 \to T_3$ the naive bayes model performs best by a margin of 6.6\% (\acrshort{poc}) and 40.9\% (\acrshort{pow}).

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & \multicolumn{2}{|c|}{\acrshort{poc} (Recall)} & \multicolumn{2}{|c|}{\acrshort{pow} (Recall)} \\
     \hline
    Algorithms & $T_1 \to T_2$ & $T_2 \to T_3$ & $T_1 \to T_2$ & $T_2 \to T_3$\\
    \hline  Majority (baseline) & .419 & .410 & .202 & .178 \\ 
    \acrshort{nn} (baseline) & .741 & .718 & .145 & .147\\
    \acrshort{tlmf} & \textbf{.777} & \textbf{.799} & .223 & .497 \\
    \acrshort{tlmf} + Bert & .776 & \textbf{.799} & .223 & .497\\
    Naive Bayes & .763 & .796 & \textbf{.268} & \textbf{.556} \\
    I - AutoRec & .714 & .786 & .223 & .552\\
    User - Neighborhood (Pearson) & .550 & .604 & .224 & .153\\
    \hline
    \end{tabular}
    \caption{Results of the Models, Evaluated With the Average Recall â Score.}
    \label{tab:results_recall_mean}
\end{table}\\

\noindent In Table \ref{tab:results_g_mean} the performance results of the models using the averaged G - mean are presented. For data set $T_1 \to T_2$ the \acrshort{tlmf} model achieves the best performance with by a margin of 4.1\% (\acrshort{poc}) and 8.3\% (\acrshort{pow}). For data set $T_2 \to T_3$ the naive bayes model performs best by a margin of 16.4\% (\acrshort{poc}) and 44.7\%. 

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & \multicolumn{2}{|c|}{\acrshort{poc} (G - Mean)} & \multicolumn{2}{|c|}{\acrshort{pow} (G - Mean)} \\
     \hline
    Algorithms & $T_1 \to T_2$ & $T_2 \to T_3$ & $T_1 \to T_2$ & $T_2 \to T_3$\\
    \hline  Majority (baseline) & .300 & .299 & .299 & .270 \\ 
    \acrshort{nn} (baseline) & .731 & .714 & .255 & .285\\
    \acrshort{tlmf} & \textbf{.772} & \textbf{.797} & .408 & .672 \\
    \acrshort{tlmf} + Bert & .771 & .796 & .408 & .672\\
    Naive Bayes & .758 & .794 & \textbf{.463} & \textbf{.717} \\
    \acrshort{iautorec} - AutoRec & .724 & .786 & .431 & .715\\
    User - Neighborhood (Pearson) & .501 & .590 & .342 & .270\\
    \hline
    \end{tabular}
    \caption{Results of the Models, Evaluated With the Average G - Mean.}
    \label{tab:results_g_mean}
\end{table}
\noindent It can be observed that the Naive Bayes model as well as the \acrshort{tlmf} model perform remarkably well. The Naive Bayes model achieves the best performance in 62.5\% cases of all task / data set combinations for the newly introduced metrics. The \acrshort{tlmf} model achieves the best performance in 31.25\% of the cases. It can also be seen that the User Neighborhood model is performing significantly worse than the other proposed models. This is likely caused by the fact that too little rating data is available for most users to compute reasonable user - similarity clusters.
It can also be observed that the argument - similarity matrix computed by the \acrshort{bert} model does not seem to enhance the prediction power of the matrix factorization model in comparison to the similarity matrix computed by the less complex \acrshort{wtmf} model.\\
As predicted, the baseline models are far off from the best performance in every evaluation instance. This supports the assumption that the choice of accuracy and \acrshort{rmse} as the evaluation metrics is not suitable for the data set at hand.\\
In general, the performance difference of the baseline models and the proposed models is greater for the data set $T_2 \to T_3$. One explanation could be the fact that $T_2 \to T_3$ is less sparse and more balanced than $T1 \to T_2$. This can be observed from Table \ref{tab:my_label}.\\
Overall the models perform generally worse in the \acrshort{poc} task than in the \acrshort{pow} task on the same data sets. This is likely due to the fact that the \acrshort{pow} task is more complex.

\subsection{Computational Efficiency}
\label{chap:efficiency}
\subsubsection{\acrshort{tlmf}}
The largest share of the computation costs can be attributed to the evaluation of the objective functions and updating the corresponding latent vectors \cite{li2016two}. 
The computational complexity of the WTMF - step is hence $O(N a K)$ with $N$ being the number of arguments, $K$ being the embedding dimension of the arguments and $a$ the average number of words per argument. The computational cost of updating the latent factor vectors is given by $O(M a K)$ for the word vectors and $O(N a K)$ for the sentence vectors.\\
For the second step the computational complexity is given by $O(m\bar{r}d + m\bar{t}d)$ with $\bar{r}$ being the average rating number per item and $\bar{t}$ being the average neighbor number per item. The computational cost of updating the latent factor vectors is given by $O(n\bar{x}d)$ for the user vector and $O(m\bar{x}d + m\bar{t}^{2}d)$ for the item vector.

\subsubsection{\acrshort{iautorec}}
In the following the dimension of the input layer will be denoted with $n$, the dimension of the hidden layer will be denoted with $h$ and the dimension of the output layer will be denoted with $m$.\\
A perceptron in the hidden layer with $n$ inputs will perform $n + 1$ multiplications (dot - product and bias terms) and $n + 1$ additions (dot - product and bias term). Afterwards one operation has to be done to evaluate the activation function.\\
The computational complexity of one perceptron in the hidden state can be approximated by\\
\begin{equation}
    O((n+1) + (n+1) + 1) = O(n)
\end{equation} 
As this needs to be done by all perceptrons in the hidden layer, the total computational complexity of the hidden layer can be stated as $h \cdot O(n)$. The same procedure is repeated for the output layer. The output layer gets $h$ inputs. Thus, a $h + 1$ dimensional dot product needs to be computed with a succeeding operation for the activation function. Its computational complexity can be stated as\\
\begin{equation}
    O((h +1) + (h + 1) + 1 = O(h)
\end{equation}
Scaled by the number of perceptrons in the output layer, the computational complexity of the output layer can be described as\\
\begin{equation}
    m \cdot O(h)
\end{equation}
Thus, the computational complexity of a forward pass, i.e. the computational cost of making a prediction, is given by\\
\begin{equation}
    m \cdot O(h) + h \cdot O(n) = O((n + m) \cdot h)
\end{equation}
Doing a forward pass with succeeding backpropagation of the error to adjust the weights changes the computational complexity to\\
\begin{equation}
    2 \cdot O((n + m) \cdot h) = O((n + m) \cdot h)
\end{equation}
The computational complexity for training the \acrshort{iautorec} model is therefore\\
\begin{equation}
    k \cdot e \cdot O((n + m) \cdot h)
\end{equation}
where $e$ is the number of epochs the model trains and $k$ is the number of samples in the training set.

\subsubsection{Naive Bayes \acrshort{cf}}
The computational cost for computing the prior - probability is given by\\
\begin{equation}
    O(I \cdot C)
\end{equation}
where I is the number of items and C is the number of classes.
The computational cost for computing the users that rated a specific item with a specific class is given by\\
\begin{equation}
    O(\bar{I} \cdot \bar{U} \cdot C)
\end{equation}
where $\bar{I}$ is the average number of ratings for an argument and $\bar{U}$ is the average number of ratings given by a user.\\
These two calculations are implemented in the training phase. That means the computational complexity of the training phase is given by:\\
\begin{equation}
    O((I \cdot C) + (\bar{I} \cdot \bar{U} \cdot C))\\ = O(C \cdot (I + (\bar{I} \cdot \bar{U})
\end{equation}
The likelihood is computed when making predictions, not in the training phase. Therefore the computational cost for making a prediction is given by:\\
\begin{equation}
    O((C \cdot \bar{I} \cdot \bar{U}))
\end{equation}
If the likelihood computation for all users and items is carried out in the training phase, the computational cost for making a prediction even reduces to $O(1)$ as two hashtable lookups with succeeding multiplication need to be executed. In this case, the term $(C \cdot I \cdot U)$ would be added to the computational cost of the training.

\subsubsection{User Neighborhood}
Usually neighborhood models are divided into two phases: an offline phase and an online phase \cite{aggarwal2016recommender}. In the offline phase the similarity computation is carried out. In the online phase, a prediction is made using the previously calculated similarity values.
Let $u$ denote the maximal number of specified ratings of a user and let $i$ denote the maximal number of specified ratings of an item. Then the computational complexity for calculating the similarity value of one user to the other users in the offline phase is given by $O(u \cdot i)$. 
To determine the calculation time for all users, the previously mentioned expression is multiplied by the maximum number of specified ratings for an item, leading to the computational complexity of $O(i^2 \cdot u)$.\\
Regarding the online phase, the computational complexity of predicting a rating is given by $O(k \cdot i)$ with $k$ being the user - neighborhood size that is used for prediction.
\subsection{Hyperparameters}
\label{chap:hyperparams}
The hyperparameters for the parameterizable models were chosen based on the best performance on the validation data set. It is important to note that for each model the same set of hyperparameter values are used for the different metrics.\\
It is noteworthy that the best performing Naive Bayes model does not need to be tuned as it does not contain any tuneable parameters.\\
It can be observed from Table \ref{tab:tlmf} and Table \ref{tab:autorec} that the embedding dimension that yields the best results for the \acrshort{tlmf} model and the \acrshort{iautorec} model is much smaller for the \acrshort{poc} task than for the \acrshort{pow} task, implying that the \acrshort{poc} task is less complex than the \acrshort{pow} task.\\ 
The hyperparameter values that were used to obtain the claimed performances are presented in \ref{chap:pvo}.

\section{Conclusion}
\label{chap:conclusion}
In this thesis, several recommendation algorithms were implemented and compared to two baseline algorithms. All algorithms were evaluated on the two baseline metrics, accuracy and rmse, as well as four additional metrics, namely G - Mean, F - Score, Precision and Recall. The additional metrics treated every rating label within the data as equally important, forcing the models to not only rely on making correct predictions for the most frequent rating labels.\\
The original evaluation metrics accuracy and rmse were not reasonable to use for assessing the performance of the proposed baseline algorithms as the class distribution within single items was imbalanced. Using those metrics, an implicit assumption of balance within the data was made. Therefore, by using the metrics presented, the suitability of the evaluation of the recommender algorithms becomes independent of the characteristics of the data set.\\
Evaluating on the proposed metrics showed that the baseline algorithms were outperformed on every task -/data set - combination. Especially the Naive Bayes algorithm and the \acrshort{tlmf} algorithm performed well with the Naive Bayes algorithm being the more efficient one of them without the need to tune this algorithm.\\

\subsection{Limitations}
\label{chap:limits}
The tasks and metrics defined in this thesis focus mainly on the performance of the models to predict the ratings in the test - set. However, the quality of the recommendations that the user finally gets to see are not taken into consideration. Since the final recommendations that the user sees depend heavily on the training test split on which the models are trained, the ability to predict a correct rating alone is not a solid measure for the satisfaction of the user with a recommendation. Thus, the metrics that were used in this thesis can only provide information about the ability to predict unseen ratings of arguments, they do not provide any information about the satisfaction or conviction of the user to whom the argument is presented.  

\subsection{Further Research}
It is worth looking at the ways in which this work could be taken forward. Ideas for further research are suggested below. 

As pointed out in section \ref{chap:limits}, it seems to be reasonable to use other metrics to assess the quality of the recommendations. Thus, measuring the quality of the recommendations by metrics that are independent of the training test split could be a next research step.\\
Kunaver et al. \cite{kunaver2017diversity} describe a diversity metric that is defined as the average dissimilarity between all pairs of items in the result set of a recommendation algorithm. In the context of argument recommender algorithms, item similarity could be measured as the semantic similarity of the textual content of the arguments, similar to the first step of the TLMF algorithm. A more diverse result set would include arguments that are different in content, but all arguments would still fit the specific user.\\ 
Another metric, serendipity, is defined by Ge et al. in \cite{ge2010beyond}. 
Serendipity, also referred to as novelty, is defined as the degree to which a user is positively surprised by a recommendation.

Another interesting continuation would be to compare the implemented models in a real application like the \acrshort{aps} described in section \ref{chap:intro}. This would also offer the possibility to investigate the behavior of the models and the reaction of the users when non - performance related metrics like diversity are combined with performance related metrics like accuracy.

\section{Acknowledgements}
I am grateful for the opportunity to write this thesis in the Machine Learning working group. Above all, I was grateful for the opportunity to present an intermediate state of the thesis to the working group and to receive valuable feedback.\\
In particular, I would like to thank Maike Behrendt for always sharing valuable advice and helpful guidance throughout the process. 

\clearpage
\section{Appendix}
\label{sec:appendix}
\subsection{Parameter Values Overview}
\label{chap:pvo}
This section provides information about the parameter values that were used in the different models to obtain the claimed results and to reproduce them using the provided source code.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
        \hline Parameter & Value \\
        \hline  $k$ & 20 \\
        iterations & 30 \\
        $w$ & .01 \\
        $\gamma$ & .01 \\
        random - seed & 8 \\
    \hline    
    \end{tabular}
    \caption{Parameter Values Used in the WTMF, the First Step of the \acrshort{tlmf}.}
    \label{tab:wtmf}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & \multicolumn{4}{|c|}{Values} \\
        \hline & \multicolumn{2}{|c|}{\acrshort{poc}} & \multicolumn{2}{|c|}{\acrshort{pow}} \\
        \hline Parameter & $T_1 \to T_2$  & $T_2 \to T_3$  & $T_1 \to T_2$  & $T_2 \to T_3$ \\
        \hline  $d$ & 1 & 3 & 20 & 20\\
          iterations & 17 & 23 & 201 & 201\\
          r & .041 & .001 & .1 & .1\\
          $\alpha$ & .0001 & .0009 & .0001 & .0001\\
          random - seed & 46 & 514 & 262 & 262\\
          $\eta$ & .031 & .081 & .005 & .005\\
          $n$ & 74 & 18 & 3 & 3\\
        \hline
    \end{tabular}
    \caption{Parameter Values Used in the \acrshort{tlmf} for the Different Task/Data Set Combinations.}
    \label{tab:tlmf}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & \multicolumn{4}{|c|}{Values} \\
        \hline & \multicolumn{2}{|c|}{\acrshort{poc}} & \multicolumn{2}{|c|}{\acrshort{pow}} \\
        \hline Parameter & $T_1 \to T_2$  & $T_2 \to T_3$  & $T_1 \to T_2$  & $T_2 \to T_3$ \\
        \hline  $k$ & 6 & 90 & 200 & 200\\
          iterations & 358 & 29 & 200 & 200\\
          random - seed & 7 & 3 & 7 & 7\\
          $\eta$ & .001 & .001 & .0001 & .0001\\
        \hline
    \end{tabular}
    \caption{Parameter Values Used in the \acrshort{iautorec} Model for the Different Task/Data set Combinations.}
    \label{tab:autorec}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & \multicolumn{4}{|c|}{Values} \\
        \hline & \multicolumn{2}{|c|}{\acrshort{poc}} & \multicolumn{2}{|c|}{\acrshort{pow}} \\
        \hline Parameter & $T_1 \to T_2$  & $T_2 \to T_3$  & $T_1 \to T_2$  & $T_2 \to T_3$ \\
        \hline  $k$ & 36 & 1 & 12 & 38\\
          similarity threshold & .87 & .7 & .4 & .63\\
        \hline
    \end{tabular}
    \caption{Parameter Values Used in the User Neighborhood Model for the Different Task/Data Set Combinations.}
    \label{tab:autorec}
\end{table}
\clearpage

\subsection{Plots}
\label{sec:plots}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T1_T2_train_Conviction.jpg}
    \caption{Distribution of the Train â Ratings Given for Arguments in $T_1 \to T_2$ in Percentage for the Conviction Task.}
    \label{fig:conviction_t1_t2_train}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T1_T2_train_Weight.jpg}
    \caption{Distribution of the Train â Ratings Given for Arguments in $T_1 \to T_2$ in Percentage for the Weight Task, Grouped by Rating Values 0 - 3 and 4 - 6.}
    \label{fig:conviction_t1_t2_train}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T1_T2_test_Conviction.jpg}
    \caption{Distribution of the Test â Ratings Given for Arguments in $T_1 \to T_2$ in Percentage for the Conviction Task.}
    \label{fig:conviction_t1_t2_test}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T1_T2_test_Weight.jpg}
    \caption{Distribution of the Test â Ratings Given for Arguments in $T_1 \to T_2$ in Percentage for the Weight Task.}
    \label{fig:conviction_t2_t3_test}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T2_T3_train_Conviction.jpg}
    \caption{Distribution of the Train â Ratings Given for Arguments in $T_2 \to T_3$ in Percentage for the Conviction Task.}
    \label{fig:conviction_t2_t3_train}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T2_T3_train_Weight.jpg}
    \caption{Distribution of the Train â Ratings Given for Arguments in $T_2 \to T_3$ in Percentage for the Weight Task, Grouped by Rating Values 0 - 3 and 4 - 6.}
    \label{fig:conviction_t1_t2_train}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T2_T3_test_Conviction.jpg}
    \caption{Distribution of the Test â Ratings Given for Arguments in $T_2 \to T_3$ in Percentage for the Conviction Task.}
    \label{fig:conviction_t2_t3_test}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{images/T2_T3_test_Weight.jpg}
    \caption{Distribution of the Test â Ratings Given for Arguments in $T_2 \to T_3$ in Percentage for the Weight Task.}
    \label{fig:conviction_t2_t3_test}
\end{figure}